<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>BSRGAN</title>
      <link href="/2023/08/14/BSRGAN/"/>
      <url>/2023/08/14/BSRGAN/</url>
      
        <content type="html"><![CDATA[<h1 id="BSRGAN"><a href="#BSRGAN" class="headerlink" title="BSRGAN"></a>BSRGAN</h1><p>单图像超分辨率（SISR）旨在从低分辨率（LR）图像y中重建自然而清晰的高分辨率（HR）对应图像x。</p><p>1）设计了一个实用的SISR降质模型，考虑了更复杂的模糊、下采样和噪声，更重要的是采用了一种降质重排策略。</p><p>2）使用我们的降质模型生成的合成训练数据，训练了一个盲SISR模型。在不同的降质情况下，该模型在真实图像上表现良好。</p><p>3）据我们所知，这是第一个采用新的手工设计的降质模型用于通用盲图像超分辨率的工作。</p><p>4）本文强调了准确的降质建模对于基于DNN的SISR方法在实际应用中的重要性。</p><p>1.根据传统的退化模型，影响真实图像退化的主要因素有三个，即模糊、下采样和噪声。</p><p>2.由于LR和HR图像都可能存在噪声和模糊，因此不必像传统的退化模型中那样采用模糊&#x2F;下采样&#x2F;噪声添加流程来生成LR图像。</p><p>3.传统退化模型的模糊核空间应该随着尺度的变化而变化，这使得在实践中对于非常大的尺度因子难以确定。</p><p>4.虽然双三次插值退化很少适用于真实LR图像，但它可用于数据增强，确实是用于清晰锐利图像超分辨率的好选择。</p><h3 id="模糊："><a href="#模糊：" class="headerlink" title="模糊："></a>模糊：</h3><p>模糊是常见的图像退化。我们提出从HR空间和LR空间建模模糊。一方面，在传统的SISR退化模型[28，45]中，HR图像首先通过与模糊核的卷积模糊。这种HR模糊实际上旨在防止混叠现象，并在随后的下采样中保留更多的空间信息。另一方面，真实的LR图像可能是模糊的，因此在LR空间建模这种模糊是可行的。考虑到高斯核足以满足SISR任务的需求，我们进行两次高斯模糊操作，即使用各向同性高斯核的“Biso”和使用各向异性高斯核的“Baniso”[3，43，58]。请注意，HR图像或LR图像可以通过两次模糊操作进行模糊处理（有关详情，请参见第3.4节）。这样做可以大大扩展模糊的退化空间。<br>对于模糊核设置，大小从{7×7，9×9，…，21×21}中均匀采样，各向同性高斯核从区间[0.1, 2.4]中均匀采样内核宽度，而各向异性高斯核则从区间[0, π]中均匀采样旋转角度，对于尺度因子2和4，每个轴的长度分别从[0.5, 6]和[0.5, 8]中均匀采样。采用反射填充以确保模糊输出的空间大小保持不变。由于内核宽度为0.1的各向同性高斯核对应于delta（恒等）内核，因此我们可以始终应用两个模糊操作。</p><p>本文的创新之处在于提出了新的降级模型，并且可以借用现有的网络结构，如ESRGAN [49]来训练深度盲模型。</p><p>与ESRGAN相比，BSRGAN在几个方面进行了修改。首先，我们使用略有不同的HR图像数据集，其中包括DIV2K[2]，Flick2K[27,46]，WED[33]和FFHQ[22]中的2,000张人脸图像来捕捉图像先验信息。原因是，BSRGAN的目标是解决通用盲图像超分辨率问题，除了降级先验外，图像先验也可以促进超分辨器的成功。我们还基于图像的Laplacian方差移除了模糊图像。其次，BSRGAN使用更大的LR补丁大小为72×72。原因是我们的降级模型可以产生严重降质的LR图像，较大的补丁大小可以让深度模型捕捉更多的信息以获得更好的恢复效果。第三，我们通过最小化L1损失，VGG感知损失和基于谱范数的最小二乘PatchGAN损失的加权组合来训练BSRGAN，其权重分别为1，1和0.1。特别地，VGG感知损失在预训练的19层VGG模型的第四个卷积层之前而不是第五个最大池化层上操作，因为这样更稳定且可以防止色偏问题。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CDCN</title>
      <link href="/2023/08/14/CDCN/"/>
      <url>/2023/08/14/CDCN/</url>
      
        <content type="html"><![CDATA[<h1 id="CDCN"><a href="#CDCN" class="headerlink" title="CDCN"></a>CDCN</h1><h1 id="Bridging-Component-Learning-with-Degradation-Modelling-for-Blind-Image-Super-Resolution"><a href="#Bridging-Component-Learning-with-Degradation-Modelling-for-Blind-Image-Super-Resolution" class="headerlink" title="Bridging Component Learning with Degradation                                                                                                                                                                                                                                                                                                                                  Modelling for Blind Image Super-Resolution"></a>Bridging Component Learning with Degradation                                                                                                                                                                                                                                                                                                                                  Modelling for Blind Image Super-Resolution</h1>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DAN_V1</title>
      <link href="/2023/08/14/DAN-V1/"/>
      <url>/2023/08/14/DAN-V1/</url>
      
        <content type="html"><![CDATA[<h1 id="DAN-V1"><a href="#DAN-V1" class="headerlink" title="DAN_V1"></a>DAN_V1</h1><h1 id="Unfolding-the-Alternating-Optimization-for-Blind-Super-Resolution"><a href="#Unfolding-the-Alternating-Optimization-for-Blind-Super-Resolution" class="headerlink" title="Unfolding the Alternating Optimization for Blind Super Resolution"></a>Unfolding the Alternating Optimization for Blind Super Resolution</h1><p>[TOC]</p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;以前的方法将盲超分辨率（SR）问题分解为两个顺序步骤：i）从给定的低分辨率（LR）图像中估计模糊核和ii）基于估计的核恢复SR图像。这两个步骤的解决方案涉及两个独立训练的模型，它们可能不太兼容。第一步的小估计误差可能会导致第二步的性能严重下降。另一方面，第一步只能利用来自LR图像的有限信息，这使得预测高精度模糊核变得困难。针对这些问题，我们采用交替优化算法，而不是分别考虑这两个步骤，它可以在单个模型中估计模糊核并恢复SR图像。具体来说，我们设计了两个卷积神经模块，即Restorer和Estimator。Restorer基于预测的核恢复SR图像，而Estimator在恢复的SR图像的帮助下估计模糊核。我们交替使用这两个模块，并展开这个过程以形成一个端到端可训练的网络。这样，Estimator利用来自LR和SR图像的信息，使模糊核的估计更容易。更重要的是，Restorer是使用Estimator估计的核进行训练的，而不是使用真实的核，因此Restorer对Estimator的估计误差更加容忍。在合成数据集和真实世界图像上进行的大量实验表明，我们的模型可以大大优于最先进的方法，并以更高的速度产生更具视觉效果的结果。源代码可在<a href="https://github.com/greatlog/DAN.git%E4%B8%8A%E8%8E%B7%E5%BE%97%E3%80%82">https://github.com/greatlog/DAN.git上获得。</a></p><h2 id="1-emsp-Introduction"><a href="#1-emsp-Introduction" class="headerlink" title="1&emsp;Introduction"></a>1&emsp;Introduction</h2><p>&emsp;&emsp;单张图像超分辨率（SISR）旨在恢复给定降质低分辨率（LR）图像的高分辨率（HR）版本。它在视频增强、医学成像以及安全和监视成像等方面具有广泛的应用。从数学上讲，降质过程可以表示为：<br>$$\mathbf{y}&#x3D;(\mathbf{x}\otimes\mathbf{k})\downarrow_s\mathbf{+n}\tag{1}$$<br>&emsp;&emsp;其中，$x$是原始的HR图像，$y$是降质的LR图像，$⊗$表示$x$与模糊核$k$的二维卷积，$n$表示加性白噪声（AWGN），而$↓s$表示标准的$s$倍下采样器，即仅保留每个不同的$s×s$块的左上像素[35]。因此，SISR指的是从$y$中恢复$x$的过程。由于这种反向特性，它是一个高度不适定的问题，因此一直是一个具有挑战性的任务。<br>&emsp;&emsp;最近，深度神经网络（DNN）在SISR方面取得了显著的成果。但是，这些方法中的大多数[39、2、40、23、8、21]假设模糊核预定义为双三次插值的核。通过这种方式，可以手动合成大量的训练样本，并进一步用于训练功能强大的DNN。然而，在实际应用中，模糊核要复杂得多，而双三次合成的训练样本与真实图像之间存在域差异。这种域差异会导致这些网络在应用于真实应用时性能严重下降。因此，应该更加关注在未知模糊核的情况下进行SR，即盲SR。<br>&emsp;&emsp;在盲SR中，还有一个未确定的变量，即模糊核$k$，优化也变得更加困难。为了使这个问题更容易解决，先前的方法[37、32、38、35]通常将其分解为两个顺序步骤：i）从LR图像估计模糊核；ii）基于估计的核恢复SR图像。这个两步解决方案涉及到两个独立训练的模型，因此它们可能彼此不兼容。第一步的小估计误差可能会导致后续步骤的性能严重下降[14]。但是另一方面，第一步只能利用来自LR图像的有限信息，这使得难以预测高度精确的模糊核。因此，尽管这两个模型都可以单独表现良好，但当它们结合在一起时，最终结果可能是次优的。<br>&emsp;&emsp;我们采用一种交替优化算法，不是将这两个步骤分别考虑，而是在同一个模型中估计模糊核$k$并恢复SR图像$x$。具体来说，我们设计了两个卷积神经模块，即Restorer和Estimator。Restorer基于Estimator预测的模糊核恢复SR图像，恢复的SR图像进一步用于帮助Estimator更好地估计模糊核。一旦手动初始化了模糊核，这两个模块就可以很好地相互配合形成一个封闭循环，可以反复迭代。迭代过程被展开为端到端可训练的网络，称为深度交替网络（DAN）。通过这种方式，Estimator可以利用LR和SR图像的信息，使得模糊核的估计更加容易。更重要的是，Restorer是使用Estimator估计的模糊核进行训练的，而不是使用真实的模糊核。因此，在测试过程中，Restorer对Estimator的估计误差更加容忍。此外，在迭代过程中，两个模块的结果都可以得到显著的改进，因此我们的交替优化算法可能比直接的两步解决方案获得更好的最终结果。我们将我们的贡献总结为三个要点：<br>&emsp;&emsp;    1. 我们采用一种交替优化算法，通过一个单一的网络（DAN）来估计模糊核并恢复SR图像，这有助于两个模块相互兼容，并且可能比先前的两步解决方案获得更好的最终结果。<br>&emsp;&emsp;    2. 我们设计了两个卷积神经模块，可以交替重复使用，然后展开成端到端可训练的网络，没有任何预处理或后处理。这种方法比以前的两步解决方案更容易训练，速度更快。据我们所知，所提出的方法是盲SR的第一个端到端网络。<br>&emsp;&emsp;    3. 在合成数据集和真实世界图像上进行的广泛实验表明，我们的模型可以大大优于最先进的方法，并以更高的速度产生更具视觉效果的结果。</p><h2 id="2-emsp-Related-Work"><a href="#2-emsp-Related-Work" class="headerlink" title="2&emsp;Related Work"></a>2&emsp;Related Work</h2><h3 id="2-1-emsp-在双三次插值的情况下的超分辨率"><a href="#2-1-emsp-在双三次插值的情况下的超分辨率" class="headerlink" title="2.1&emsp;在双三次插值的情况下的超分辨率"></a>2.1&emsp;在双三次插值的情况下的超分辨率</h3><p>&emsp;&emsp;针对SISR的基于学习的方法通常需要大量成对的HR和LR图像作为训练样本。然而，在现实世界中很难获取这些成对样本。因此，研究人员通过预定义的下采样设置从HR图像手动合成LR图像。最流行的设置是双三次插值，即在公式1中定义k为双三次插值核。从SRCNN [9]的出现开始，基于这种设置提出了各种DNN [21、40、39、10、16、18]。最近，在RCAN [39]和RRDB [30]的提出之后，这些非盲方法在常见的基准数据集上的性能甚至开始饱和。然而，真实图像的模糊核确实更加复杂。在实际应用中，核是未知的，而且各不相同。因此，尽管这些方法在双三次下采样的情况下表现出色，但由于存在域差异，它们仍然不能直接应用于真实图像。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DAN_V1.md/img-20230531111454.png" alt="Img"></p><h3 id="2-2-emsp-多重降级情况下的超分辨率"><a href="#2-2-emsp-多重降级情况下的超分辨率" class="headerlink" title="2.2&emsp;多重降级情况下的超分辨率"></a>2.2&emsp;多重降级情况下的超分辨率</h3><p>&emsp;&emsp;另一种非盲超分辨率方法旨在针对多种降级提出一个单一模型，即盲SR的两步解决方案中的第二步。这些方法将LR图像及其相应的模糊核作为输入。在[13、29]中，使用模糊核对图像进行下采样和合成训练样本，以便为给定的核和LR图像训练特定模型。在[37]中，直接将模糊核和LR图像连接到DNN的第一层。因此，SR结果可以与LR图像和模糊核密切相关。在[38]中，张等人提出了一种基于ADMM算法的方法。他们将这个问题解释为MAP优化，交替解决数据项和先验项。在[14]中，提出了一种空间特征变换（SFT）层，以更好地保留LR图像中的细节，同时将模糊核作为附加输入。然而，正如[14]所指出的，这些方法的SR结果通常对提供的模糊核非常敏感。提供的核与真实模糊核之间的微小偏差会导致这些非盲超分辨率方法的性能严重下降。</p><h3 id="2-3-emsp-盲超分辨率"><a href="#2-3-emsp-盲超分辨率" class="headerlink" title="2.3&emsp; 盲超分辨率"></a>2.3&emsp; 盲超分辨率</h3><p>&emsp;&emsp;以前的盲超分辨率方法通常是核估计方法和非盲超分辨率方法的顺序组合。因此，核估计方法也是盲超分辨率的重要组成部分。在[26]中，Michaeli等人利用内部补丁重复来估计模糊核。在[3]和[5]中，LR图像首先通过生成网络进行下采样，然后使用鉴别器验证下采样图像是否与原始LR图像具有相同的分布。通过这种方式，可以通过生成网络学习到模糊核。在[14]中，Gu等人不仅训练了一个用于核估计的网络，还提出了一个纠正网络来迭代地修正核。尽管估计核的准确性大大提高了，但需要训练两个甚至三个网络，这相当复杂。相比之下，DAN是一个可以进行端到端训练的网络，更容易训练且速度更快。</p><h2 id="3-emsp-端到端盲超分"><a href="#3-emsp-端到端盲超分" class="headerlink" title="3&emsp;端到端盲超分"></a>3&emsp;端到端盲超分</h2><h3 id="3-1-emsp-问题描述"><a href="#3-1-emsp-问题描述" class="headerlink" title="3.1&emsp; 问题描述"></a>3.1&emsp; 问题描述</h3><p>&emsp;&emsp;在盲超分问题中，如公式1所示，需要确定三个变量，即x、k和n。在文献中，我们可以首先应用去噪算法[36, 7, 15]。然后，盲超分算法只需要专注于解决k和x。这可以被数学地表达为一个优化问题：<br>$$\underset{\mathbf{k},\mathbf{x}}{\operatorname{arg}\min}|\mathbf{y}-(\mathbf{x}\otimes\mathbf{k})\downarrow_s|<em>1+\phi(\mathbf{x}) \tag{2}$$<br>&emsp;&emsp;其中前一部分是重建项，$\phi(x)$是HR图像的先验项。先验项通常是未知的，很难通过解析方式表达。因此，直接解决这个问题非常困难。先前的方法将这个问题分解成两个连续的步骤：<br>$$\begin{cases}\mathbf{k}&#x3D;M(\mathbf{y})\ \mathbf{x}&#x3D; \underset{\mathbf{k},\mathbf{x}} {\operatorname{arg}\min}|\mathbf{y}-(\mathbf{x}\otimes\mathbf{k})\downarrow</em>{s}||_1+\phi(\mathbf{x})\ \end{cases}\tag{3}$$</p><p>&emsp;&emsp;其中，$M(·)$表示从$y$估计$k$的函数，第二步通常通过第2.2节中描述的非盲超分方法来解决。这种两步解决方案有三个缺点。首先，该算法通常需要训练两个甚至更多的模型，这相当复杂。其次，$M(·)$只能利用$y$中的信息，将$k$视为$y$的一种先验。但实际上，没有来自$x$的信息，$k$就不能被正确地解决。最后，第二步的非盲超分模型是使用ground-truth核进行训练的。然而，在测试期间，它只能访问在第一步中估计的核。地面真实核与估计核之间的差异通常会导致非盲超分模型的性能大幅下降[14]。<br>&emsp;&emsp;针对这些缺点，我们提出了一种端到端网络，可以大大解决这些问题。我们仍然将其分成两个子问题，但是我们不是按顺序解决它们，而是采用交替优化算法，交替恢复SR图像并估计相应的模糊核。其数学表达式为：<br>$$\begin{cases}\mathbf{k}_{i+1}&#x3D;\underset{\mathbf{k}}{\operatorname{arg}\min}|\mathbf{y}-(\mathbf{x}_i\otimes\mathbf{k})\downarrow_s|<em>1\ \mathbf{x}</em>{i+1}&#x3D;\underset{\mathbf{x}}{\operatorname{arg}\min}|\mathbf{y}-(\mathbf{x}\otimes\mathbf{k}_i)\downarrow_s|_1+\phi(\mathbf{x})\end{cases}\tag{4}$$<br>&emsp;&emsp;我们通过卷积神经网络模块Estimator和Restorer交替解决这两个子问题。实际上，Estimator甚至有一个解析解。但是我们在实验中发现，解析解更耗时，而且不够健壮（当噪声没有完全去除时）。我们将迭代次数固定为$T$，并展开迭代过程，形成一个端到端可训练的网络，称为深交替网络（DAN）。<br>&emsp;&emsp;如图1所示，我们通过狄拉克函数来初始化核，即核的中心为1，其他为0。类似于[14, 37]的方法，核也被重塑并通过主成分分析（PCA）进行降维。我们在实践中将T设置为4，两个模块仅在最后一次迭代时受到L1损失的监督。整个网络可以很好地训练，因为两个模块的参数在不同迭代之间共享，没有对中间结果施加任何限制。<br>&emsp;&emsp;在DAN中，Estimator将LR和SR图像作为输入，这使得模糊核$k$的估计更容易。更重要的是，Restorer是使用Estimator估计的核进行训练的，而不是像先前的方法那样使用ground-truth核。因此，在测试期间，Restorer可以更容忍Estimator的估计误差。此外，与先前的两步解决方案相比，DAN中两个模块的结果都可以得到显著的改善，DAN很可能获得更好的最终结果。特别是，在缩放因子$s &#x3D; 1$的情况下，DAN变成了一个去模糊网络。由于篇幅有限，本文仅讨论SR情况。</p><h3 id="3-2-emsp-实例化卷积神经网络模块"><a href="#3-2-emsp-实例化卷积神经网络模块" class="headerlink" title="3.2&emsp;实例化卷积神经网络模块"></a>3.2&emsp;实例化卷积神经网络模块</h3><p>&emsp;&emsp;我们网络中的两个模块都有两个输入。Estimator采用LR图像和SR图像，Restorer采用LR图像和模糊核作为输入。我们将LR图像定义为基本输入，另一个输入是条件输入。例如，模糊核是Restorer的条件输入。在迭代过程中，两个模块的基本输入保持不变，但它们的条件输入会被重复更新。我们认为，将每个模块的输出与其条件输入密切相关非常重要。否则，迭代结果将在第一次迭代时崩溃为一个固定点。具体来说，如果Estimator无论SR图像的值如何输出相同的核，或者Restorer无论模糊核的值如何输出相同的SR图像，它们的输出将仅取决于基本输入，并且结果将在迭代过程中保持不变。<br>&emsp;&emsp;为了确保Estimator和Restorer的输出与它们的条件输入密切相关，我们提出了一个条件残差块（CRB）。在[39]中的残差块的基础上，我们在开头连接条件和基本输入：<br>$$f_{out}&#x3D;R(Concat([f_{basic},f_{cond}]))+f_{basic}\tag{5}$$<br>&emsp;&emsp;其中，$R(·)$表示CRB的残差映射函数，$Concat([·, ·])$表示连接操作。$fbasic$和$fcond$分别是基本输入和条件输入。如图2(c)所示，残差映射函数由两个3×3卷积层和一个通道注意力层[17]组成。Estimator和Restorer都是由CRB构建的。<br>&emsp;&emsp;<strong>Estimator。</strong> Estimator的整个结构如图2（b）所示。我们首先通过带有步幅s的卷积层对SR图像进行下采样。然后将特征图作为条件输入发送给所有的CRB。在网络的末端，我们通过全局平均池化来压缩特征，形成预测核的元素。由于核是通过PCA降维的，Estimator只需要估计模糊核的PCA结果。在实践中，Estimator有5个CRB，每个CRB的基本输入和条件输入都有32个通道。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DAN_V1.md/img-20230601161517.png" alt="Img"><br>&emsp;&emsp;<strong>Restorer。</strong> Restorer的整个结构如图2（a）所示。在Restorer中，我们将核在空间维度上拉伸到与LR图像相同的空间大小。然后将拉伸后的核作为条件输入发送给Restorer的所有CRB。我们使用PixelShuffle [28]层将特征放大到所需的尺寸。在实践中，Restorer有40个CRB，每个CRB的基本输入和条件输入分别有64个和10个通道。</p><h2 id="四、实验"><a href="#四、实验" class="headerlink" title="四、实验"></a>四、实验</h2><h3 id="4-1-合成测试图像实验"><a href="#4-1-合成测试图像实验" class="headerlink" title="4.1 合成测试图像实验"></a>4.1 合成测试图像实验</h3><h4 id="4-1-1-数据，训练和测试"><a href="#4-1-1-数据，训练和测试" class="headerlink" title="4.1.1 数据，训练和测试"></a>4.1.1 数据，训练和测试</h4><p>&emsp;&emsp;我们从DIV2K [1]和Flickr2K [11]收集了3450个HR图像作为训练集。为了与其他方法进行合理比较，我们使用两种不同的退化设置来训练模型。一种是[14]中的设置，它仅关注具有各向同性高斯模糊核的情况。另一种是[3]中的设置，它关注具有更一般和不规则的模糊核的情况。<br>&emsp;&emsp;<strong>设置1。</strong> 遵循[14]中的设置，核大小设置为21。在训练过程中，对于缩放因子4、3和2，核宽度在[0.2，4.0]、[0.2，3.0]和[0.2，2.0]中均匀采样。对于定量评估，我们从常用的基准数据集Set5 [4]、Set14 [34]、Urban100 [19]、BSD100 [24]和Manga109 [25]中收集HR图像。由于为了进行合理的比较需要确定的核，我们从范围[1.8，3.2]、[1.35，2.40]和[0.80，1.60]中均匀选择8个核，分别表示缩放因子4、3和2。HR图像首先由选定的模糊核进行模糊处理，然后进行下采样，形成合成测试图像。<br>&emsp;&emsp;<strong>设置2。</strong> 遵循[3]中的设置，我们将核大小设置为11。我们首先生成各向异性高斯核。两个轴的长度均匀分布在（0.6，5）之间，并旋转一个均匀分布在[−π，π]之间的随机角度。为了偏离正常的高斯分布，我们进一步应用均匀乘性噪声（最多为每个像素值的25%），并将其归一化为总和为1。对于测试，我们使用[3]中使用的基准数据集DIV2KRK。<br>&emsp;&emsp;训练期间所有缩放因子的输入尺寸为64×64。批量大小为64。每个模型训练4×105次迭代。我们使用Adam [22]作为优化器，其中β1 &#x3D; 0.9，β2 &#x3D; 0.99。初始学习率为2×10-4，并在每1×105次迭代时衰减一半。所有模型都在RTX2080Ti GPU上训练。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DAN_V1.md/img-20230601173941.png" alt="Img"></p><h4 id="4-1-2-定性结果"><a href="#4-1-2-定性结果" class="headerlink" title="4.1.2 定性结果"></a>4.1.2 定性结果</h4><p>&emsp;&emsp;<strong>设置1。</strong> 对于第一个设置，我们在由Gaussian8核合成的测试图像上评估我们的方法。我们主要将结果与设计用于盲SR的ZSSR [29]和IKC [14]进行比较。我们还包括与CARN [2]的比较。由于它不是为盲SR设计的，我们在CARN之前或之后执行去模糊方法[27]。在转换后的YCbCr空间的Y通道上，PSNR和SSIM结果如表1所示。<br>&emsp;&emsp;尽管CARN在双三次下采样的情况下取得了显着的结果，但当应用于具有未知模糊核的图像时，其性能会严重下降。当CARN后跟去模糊方法时，其性能得到了很大的改善，但仍然不如盲SR方法。ZSSR利用内部补丁重复对每个单独测试图像训练特定的网络。然而，ZSSR具有天生的缺陷：每个图像的训练样本是有限的，因此它无法为HR图像学习良好的先验知识。IKC也是盲SR的两步解决方案。尽管在IKC中估计的核的准确性得到了很大的提高，但最终结果仍然不理想。DAN在端到端方案中进行训练，这不仅比两步解决方案容易训练得多，而且还有可能达到更好的最优点。如表1所示，DAN在Manga109的三倍尺度上的PSNR结果甚至比IKC高4.95dB。对于其他尺度和数据集，DAN也大大优于IKC。<br>&emsp;&emsp;为了进行比较，我们展示了Urban100数据集中图像005的视觉结果，如图3所示。可以看出，CARN和ZSSR甚至无法恢复窗户的边缘。IKC表现更好，但边缘严重模糊。而DAN可以恢复锐利的边缘，并产生更加视觉上愉悦的结果。<br>&emsp;&emsp;<strong>设置2。</strong> 第二个设置涉及不规则模糊核，这更一般，但也更难解决。对于设置2，我们主要比较三种不同类别的方法：i）在双三次下采样的图像上训练的SOTA SR算法，如EDSR [23]和RCAN [39]，ii）为NTIRE竞赛设计的盲SR方法，如PDN [31]和WDSR [33]，iii）两步解决方案，即核估计方法和非盲SR方法的组合，如Kernel-GAN [3]和ZSSR [29]。在Y通道上的PSNR和SSIM结果如表2所示。<br>&emsp;&emsp;类似地，训练在双三次下采样的图像上的方法的性能受到域差异的限制。因此，它们的结果仅比插值略好。类别2中的方法是在NTIRE竞赛提供的合成图像上进行训练的。尽管这些方法在竞赛中取得了显着的成果，但它们仍然无法很好地推广到不规则模糊核。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DAN_V1.md/img-20230601174407.png" alt="Img"><br>&emsp;&emsp;对比类别3的方法可以让我们受益匪浅。具体来说，当提供GT核时，USRNet [35]取得了显着的结果，并且KernelGAN在核估计方面的表现也很好。然而，如表2所示，当它们结合在一起时，最终的SR结果比大多数其他方法还要差。这表明Estimator和Restorer之间的兼容性非常重要。此外，尽管更好的核估计方法可以提高SR结果，但总体性能仍然远不如DAN。对于尺度×2和×4，DAN的表现优于KernelGAN和ZSSR的组合分别为2.20dB和0.74dB。<br>&emsp;&emsp;我们展示了DIVKRK数据集中图像892的视觉结果，如图4所示。虽然KernelGAN和ZSSR的组合可以比插值产生稍微更锐利的边缘，但它会受到严重的伪影的影响。DAN的SR图像明显更加清晰，具有更可靠的细节。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DAN_V1.md/img-20230601174515.png" alt="Img"></p><h4 id="4-1-3-估计核研究"><a href="#4-1-3-估计核研究" class="headerlink" title="4.1.3 估计核研究"></a>4.1.3 估计核研究</h4>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DSSR</title>
      <link href="/2023/08/14/DSSR/"/>
      <url>/2023/08/14/DSSR/</url>
      
        <content type="html"><![CDATA[<h1 id="DSSR"><a href="#DSSR" class="headerlink" title="DSSR"></a>DSSR</h1><h1 id="Learning-Detail-Structure-Alternative-Optimization-for-Blind-Super-Resolution"><a href="#Learning-Detail-Structure-Alternative-Optimization-for-Blind-Super-Resolution" class="headerlink" title="Learning Detail-Structure Alternative Optimization for Blind Super-Resolution"></a>Learning Detail-Structure Alternative Optimization for Blind Super-Resolution</h1>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>CMOS</title>
      <link href="/2023/08/14/CMOS/"/>
      <url>/2023/08/14/CMOS/</url>
      
        <content type="html"><![CDATA[<h1 id="Better-“CMOS”-Produces-Clearer-Images-Learning-Space-Variant-Blur-Estimation-for-Blind-Image-Super-Resolution"><a href="#Better-“CMOS”-Produces-Clearer-Images-Learning-Space-Variant-Blur-Estimation-for-Blind-Image-Super-Resolution" class="headerlink" title="Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-Resolution"></a>Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-Resolution</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;现有的大多数盲目图像超分辨率（SR）方法都假设模糊核是空间不变的。然而，由于物体运动、失焦等原因，现实应用中涉及的模糊通常是空间变化的，这会导致先进的SR方法性能严重下降。为了解决这个问题，我们首先引入了两个新的数据集，即NYUv2-BSR和Cityscapes-BSR，以支持进一步研究具有空间变异模糊的盲目SR。基于这些数据集，我们设计了一个新颖的跨模态融合网络（CMOS），可以同时估计模糊和语义，从而提高SR结果。它涉及一个特征组合交互注意（GIA）模块，使得两种模态的交互更加有效并避免不一致性。由于其结构的普适性，GIA还可以用于其他特征的交互。在上述数据集和真实世界图像上，与最先进的方法进行的定性和定量实验表明了我们方法的优越性，例如，在NYUv2-BSR上相对于MANet1获得了+1.91↑ &#x2F; +0.0048↑的PSNR &#x2F; SSIM。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>&emsp;&emsp;盲目图像超分辨率旨在从具有未知退化的低分辨率（LR）图像重建高分辨率（HR）图像，因其在实际应用中的重要性而受到广泛关注[2,5,6,12,15,22-24,29]。通常使用两种退化模型，即双三次下采样[35]和传统退化[26,32]，来从HR图像生成LR图像。后者可以被建模为：<br>$$y&#x3D;(x\bigotimes k)\downarrow_s+n.\tag{1}$$<br>&emsp;&emsp;该方法假设LR图像y是通过先将HR图像x与模糊核k卷积，然后进行缩小操作（缩小因子为s）和加噪声n的操作得到的。此外，一些工作[38, 48]提出了更复杂和逼真的退化模型，也假设模糊是空间不变的。然而，在实际应用中，由于失焦和物体运动等因素，模糊通常会在空间上发生变化，因此不匹配会严重降低现有SR方法的性能。图1给出了一个例子，当LR图像受到空间变异模糊的影响时。由于KernelGAN [1]和DCLS [28]仅为一幅图像估计一个模糊核，因此存在许多不匹配。在图1的第一行中，这两种方法估计的核比补丁的实际核更尖锐，导致SR结果过度平滑，高频纹理显著模糊。在第二行中，估计的核比正确的核更平滑，SR结果显示由于过度增强高频边缘而引起的环状伪影。这种现象说明模糊不匹配会严重影响SR结果，导致不自然的输出。在本文中，我们专注于空间变异模糊估计，以确保估计的核对图像中的每个像素都是正确的。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708101405.png" alt="Img"><br>&emsp;&emsp;最近的一些工作[15, 23, 43]已经考虑了空间变异模糊。其中，MANet [23]是最具代表性的模型，它假设在一个小的补丁内，模糊是空间不变的。基于此，MANet使用适度的感受野来保持退化的局部性。然而，仍存在两个关键问题。1）由于没有包含空间变异模糊的可用数据集，MANet是在空间不变的图像上训练的，导致训练和测试阶段的模糊偏差。2）即使限制感受野的大小，估计结果在不同核心的边界仍然很差，导致空间变异模糊的均值预测。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708101427.png" alt="Img"><br>&emsp;&emsp;为了解决上述挑战，我们首先引入一种新的退化方法，并提出了两个相应的数据集，即NYUv2-BSR和Cityscapes-BSR，以支持SR领域中空间变异模糊的相关研究。作为初步探索，本文以失焦模糊为例进行研究，并根据物体的深度使用[19]中提出的方法生成模糊。此外，我们还在数据集中添加了一些空间不变的模糊，以便在它们上训练的模型可以应对空间变异和不变的情况。<br>&emsp;&emsp;此外，为了改善不同模糊区域边界处的性能，我们提出了一种名为跨模态融合网络（CMOS）的新型模型。我们的直觉是，锐利的语义边缘通常与失焦模糊边界对齐，可以帮助区分不同的模糊程度。这引起了一个关键问题，即如何有效地将语义引入到过程中。具体来说，我们首先同时预测模糊和语义，而不是使用语义作为额外的输入，这不仅避免了在测试阶段使用额外的信息，还使非盲目SR方法可以利用这两种模态恢复更细致的纹理。其次，为了增强边缘处的准确性，我们进行了语义和模糊特征之间的交互，以进行补充信息学习，这受到了多任务学习[36,42]的启发。然而，在某些情况下，这两种模态是不一致的。如图2所示，墙壁和上面的图片在语义图中完全不同，有清晰的边界。但它们的深度几乎相同，因此基于深度的模糊程度也非常相似。在这种情况下，两种模态不仅无法使用共同的特征，而且还可能互相产生负面影响。此外，由于我们在数据集中添加了一些带有均匀模糊图的空间不变模糊图像，这也会极大地增加不一致性。<br>&emsp;&emsp;受这些观察的启发，我们提出了一个特征分组交互注意力（GIA）模块，以帮助两种模态之间的交互。GIA具有两个平行流：一个沿空间维度操作，另一个沿通道维度操作。两个流都使用分组交互来处理输入特征并进行调整。此外，GIA具有基于流场[21]的上采样层，以支持不同分辨率的输入。其通用结构使其可以用于不仅仅是两种模态之间的交互。<br>&emsp;&emsp;本文的主要贡献如下：</p><ul><li>为了支持SR领域中的空间变异模糊研究，我们引入了一种新的失焦模糊退化模型，并提出了两个新的数据集，即NYUv2-BSR和Cityscapes-BSR。</li><li>我们设计了一种名为CMOS的新型模型，用于估计空间变异模糊，利用额外的语义信息来提高模糊预测的准确性。所提出的GIA模块用于使两种模态有效地交互。请注意，GIA是通用的，可以用于任何两个特征之间。</li><li>与现有的非盲SR模型相结合，CMOS可以估计空间变异和空间不变模糊，并在两种情况下实现SOTA SR性能。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h2 id="2-1-Degradation-Model"><a href="#2-1-Degradation-Model" class="headerlink" title="2.1. Degradation Model"></a>2.1. Degradation Model</h2><p>&emsp;&emsp;如果假设的退化与现实中的不同，SR方法会导致性能不佳。许多工作[4、45、49]使用传统模型（公式1）生成它们的训练数据。与双三次下采样[40、50]相比，虽然传统模型考虑了更多因素，但它仍然过于简单，无法模拟真实的退化。因此，Real-ESRGAN [38]通过反复应用传统模型提出了一个灵活的高阶退化模型，而BSRGAN [48]调整了传统模型的退化顺序，并使用随机混淆的模糊、下采样和噪声。梁等人[23]更进一步地模拟了空间变异模糊，通过将图像分成补丁并应用不同的核来实现。不幸的是，它无法很好地模拟真实情况。因此，为支持相关研究，我们将空间变异的失焦模糊引入到SR中，并提出了两个相应的数据集，即NYUv2-BSR和Cityscapes-BSR。</p><h2 id="2-2-Kernel-Estimation"><a href="#2-2-Kernel-Estimation" class="headerlink" title="2.2. Kernel Estimation"></a>2.2. Kernel Estimation</h2><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230707164736.png" alt="Img"><br>&emsp;&emsp;盲SR的主流方法之一是首先估计退化，然后将其用作非盲SR的先验信息。KernelGAN [1]提出从图像块的内部分布中学习核，而IKC [6]使用迭代校正方案学习核的PCA特征。罗等人[28]将模糊估计转移到LR空间，并学习核权重而不是核本身。然而，这些方法仅估计唯一的核，因此在空间变异情况下性能将显著降低。因此，KOALAnet [15]提出为每个像素学习特定的核，而MANet [23]设计了一个具有适应退化局部性的中等感受野的网络。然而，它们仍然存在一些限制，例如中等的感受野可能会限制模型的容量。相比之下，我们的CMOS可以通过语义信息有效而准确地预测空间变异模糊。</p><h2 id="2-3-Non-blind-SR"><a href="#2-3-Non-blind-SR" class="headerlink" title="2.3. Non-blind SR"></a>2.3. Non-blind SR</h2><p>&emsp;&emsp;非盲SR旨在恢复具有已知退化的图像。早期的非盲SR方法[13、14、18、25]基于双三次下采样，很难推广到具有更复杂退化的图像。为了解决这个问题，SRMD [49]首先提出将模糊和噪声拉伸到LR图像的大小，并将拼接的图像和退化映射作为输入来恢复HR对应物。随着SRMD的发展，SFTMD [6]使用SFT层[39]来组合拉伸退化映射，而不仅仅是简单的拼接，而UDVD [44]采用每个像素的动态卷积来更有效地处理跨图像的变异退化。此外，还研究了多种退化的零样本方法[11、32、34]。值得注意的是，我们的CMOS可以轻松地与大多数非盲SR方法结合使用，以实现出色的盲SR性能。</p><h2 id="3-The-Proposed-Datasets"><a href="#3-The-Proposed-Datasets" class="headerlink" title="3. The Proposed Datasets"></a>3. The Proposed Datasets</h2><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708100254.png" alt="Img"><br>&emsp;&emsp;为了支持空间变异模糊的研究，我们提出了两个新颖的数据集，NYUv2-BSR和CityscapesBSR，其中BSR代表盲图像SR。据我们所知，我们是第一个将失焦（一种最常见的空间变异模糊）引入盲图像SR的研究者。失焦是由深度差异引起的。不在对焦平面上的每个点都对应于图像平面上的一圈混淆（COC）。该模糊可以通过与COC直径相关的标准差σ的各向同性高斯核来模拟[17]，这可以使用薄透镜模型[30]计算COC直径来实现。我们采用[19]中提出的方法对图像进行模糊处理，并使用每个像素的σ构建地面实况模糊图。<br>&emsp;&emsp;如上所述，我们需要深度-彩色图像对来生成具有失焦模糊的图像。因此，我们选择NYUv2 [33]和Cityscapes [3]作为原始数据集。NYUv2是一个室内数据集。它包含1449对RGB和深度图像，其中795对用于训练，其余654对用于测试。Cityscapes是一个室外数据集，其中精细注释的部分包括训练、验证和测试集，分别包含2975、500和1525幅图像。由于Cityscapes中的深度图包含无效测量值，这些值不利于生成失焦图像，因此我们使用CREStereo [20]，一种基于深度学习的立体匹配方法，生成视差图并基于相机参数计算相应的深度图。图3显示了NYUv2和Cityscapes的原始RGB图像，以及生成的失焦图像和相应的模糊图。<br>&emsp;&emsp;就各向同性高斯核的参数而言，NYUv2和Cityscapes的核宽度范围分别设置为[0.0，5.0]和[0.0，15.0]。核大小固定为21×21和61×61，下采样比例因子设置为4。此外，1&#x2F;4的图像被空间不变的核模糊，使得在这些数据集上训练的模型不受空间变异情况的限制。表1显示了详细信息。此外，为确保实验的充分性和公正性，我们为每个数据集创建了五个测试组，每个组都有不同的1&#x2F;4图像使用空间不变核进行模糊处理。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708100212.png" alt="Img"></p><h2 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h2><p>&emsp;&emsp;如前所述，清晰的语义边缘可以增加边界附近空间变异模糊估计的准确性。受此启发，我们提出了一种交叉模态融合网络（CMOS），通过它们的相互监督来同时预测模糊和语义地图。</p><h2 id="4-1-Overview"><a href="#4-1-Overview" class="headerlink" title="4.1. Overview"></a>4.1. Overview</h2><p>&emsp;&emsp;受[36]启发，CMOS是一个多尺度网络，由三个主要阶段组成，如图4所示。在第一阶段，使用一个能够生成多尺度特征的全卷积编码器提取深层特征${F_0,F_1,…,F_n}$。接下来，在每个尺度$i$上，我们应用两个任务特定的头$head_i^b$和$head_i^s$来预测初始模糊和语义特征$F^i_{blur}$和$F^i_{seg}$。然后，我们使用一个提出的$GIA^i_m$模块来实现两种模态之间的有效信息交互，以相互监督的方式获得更准确的特征$\hat{\boldsymbol{F}}<em>{blur}^i$和$\hat{\boldsymbol{F}}</em>{seg}^i$，其公式化为：<br>$$\boldsymbol{F}<em>{blur}^i&#x3D;\operatorname{head}<em>b^i(\boldsymbol{F}<em>b^i),\tag{2}$$<br>$$\boldsymbol{F}</em>{seg}^i&#x3D;\text{head}<em>s^i(\boldsymbol{F}<em>s^i),\tag{3}$$<br>$$\hat{\boldsymbol{F}}</em>{blur}^i,\hat{\boldsymbol{F}}</em>{seg}^i&#x3D;\mathrm{GIA}</em>{m}^i(\boldsymbol{F}</em>{blur}^i,\boldsymbol{F}<em>{seg}^i),\tag{4}$$<br>&emsp;&emsp;其中，$F^i_b$和$F^i_s$表示任务特定头的输入。为了更好地利用多尺度信息，我们使用$GIA^i_b$和$GIA^i_s$来融合相邻的低尺度特征，因此头部的输入可以写成：<br>$$\boldsymbol{F}<em>b^0&#x3D;\boldsymbol{F}<em>s^0&#x3D;\boldsymbol{F}^0,\tag{5}$$<br>$$\boldsymbol{F}</em>{b}^{i}&#x3D;\mathrm{Sum}(\mathrm{GIA}</em>{b}^{i}(\boldsymbol{F}^{i},\hat{\boldsymbol{F}}</em>{blur}^{i-1})),\tag{6}$$<br>$$\boldsymbol{F}<em>s^i&#x3D;\mathrm{Sum}(\mathrm{GIA}<em>s^i(\boldsymbol{F}^i,\hat{\boldsymbol{F}}</em>{seg}^{i-1})),\tag{7}$$<br>&emsp;&emsp;其中，$Sum(·)$表示将模块的输出相加。在最高分辨率$n$处，任务特定特征被馈送到两个卷积层中，以生成辅助模糊和语义地图进行额外监督，这有助于进一步提高最终预测的准确性。<br>&emsp;&emsp;最后一个阶段包括$n + 1$个$GIA^i_l$模块，以获取每个尺度的最终特征$F^i_B$和$F^i_S$，如下所示：<br>$$F_B^i,F_S^i&#x3D;\mathrm{GIA}<em>l^i(\boldsymbol{F}</em>{blur}^i,\boldsymbol{F}</em>{seg}^i).\tag{8}$$<br>&emsp;&emsp;这些特征随后被拼接并进行卷积，以获得模糊和语义地图的预测结果。通过这种方式，我们可以为每个尺度构建一种更简短的监督路径，并进一步促进模糊和语义之间的交互。此外，由于模糊是在高分辨率空间中完成的，因此我们使用双线性插值将输出上采样，倍数为s。</p><h2 id="4-2-Grouping-Interactive-Attention-Module"><a href="#4-2-Grouping-Interactive-Attention-Module" class="headerlink" title="4.2. Grouping Interactive Attention Module"></a>4.2. Grouping Interactive Attention Module</h2><p>&emsp;&emsp;GIA的设计旨在帮助模糊和语义更有效地交互，并避免不一致性。此外，由于其具有通用结构，它还可以用于其他特征。GIA涉及两个并行流，在空间和通道维度上操作，它可以通过使用基于流的上采样模块[21]处理不同分辨率的输入。<br>&emsp;&emsp;<strong>空间分组特征交互。</strong> 输入特征在大多数补丁上可能相似，但在某些补丁上可能不同，如图2所示，挂在墙上的图片使得模糊和语义地图之间存在差异。因此，我们提出在通用的空间注意力[7，46，47]机制中调整空间加权图，以利用相似信息并避免不一致性。在图4(b)的上半部分，首先对每个输入进行了处理，我们提出调整通用空间注意机制中的空间权重图，以利用相似信息并避免不一致性。<br>&emsp;&emsp;在图4(b)的上半部分，每个输入首先通过卷积层并被分成由$F^j_w$表示的窗口。这些窗口随后经过另一个卷积层进一步处理，然后被馈送到特征交互模块（第4.2节的最后一部分）。空间调整权重图$M^j_a ∈ R^{1×H×W}$可以通过交互后的1×1卷积层获得。此外，每个输入都有自己的空间权重图$M^j_o ∈ R^{1×H×W}$，直接从窗口中提取，使用另一个1×1卷积层。因此，对应于两个输入的输出$F^j$可以表示为：<br>$$\boldsymbol{F}^{j}&#x3D;\mathrm{Mul}(\boldsymbol{F}<em>{w}^{j},\mathrm{Add}(\boldsymbol{M}</em>{o}^{j},\alpha\boldsymbol{M}<em>{a}^{j})),j&#x3D;1,2,\tag{9}$$<br>其中α是一个可学习的参数。最后，窗口被恢复成特征，并通过一个3×3卷积层平滑可能的接缝，得到最终的输出。<br>&emsp;&emsp;<strong>通道分组特征交互。</strong> 由于空间特征交互集中于局部细节，我们进一步引入通道分组特征交互来校准受[8]启发的全局信息。首先，我们通过应用全局平均池化和一个MLP层将输入$F^j</em>{in}$转换为通道注意向量$A^j_o ∈ R^C$。然后，将这些向量馈送到特征交互模块中，通过另一个MLP层获得两个调整注意向量$A^j_a ∈ R^C$，将这两个特征进行整合。类似于空间特征交互，最终的输出可以通过以下方式获得：<br>$$\boldsymbol{F}^{\boldsymbol{j}}&#x3D;\mathrm{Mul}(\boldsymbol{F}<em>{in}^{j},\mathrm{Add}(\boldsymbol{A}</em>{o}^{j},\beta\boldsymbol{A}<em>{a}^{j})),j&#x3D;1,2\tag{10}$$<br>其中β是一个可学习的参数。由于全局信息对于模糊[31]和语义估计[27]都非常重要，通道维度的特征交互也是必不可少的。<br>&emsp;&emsp;<strong>特征组交互。</strong> 该模块旨在以组的方式交互空间或通道特征。对于空间交互，输入大小为$C × H × W$。我们将每个像素的特征视为一组，并将分组特征的大小设置为$N × D$，其中$N &#x3D; HW，D &#x3D; C$。对于通道交互，输入大小为$C$。它将被分成长度为$D$的$N$组，其中$C &#x3D; ND$。通过这种方式，空间和通道输入都可以表示为$G_i ∈ R^{N×D}$，其中i表示两个不同的输入。然后，我们使用内积进行特征交互，并获得交互特征$F</em>{fuse} ∈ R^{N×N}$，<br>$$F_{fuse}&#x3D;G_1{G_2}^T.\tag{11}$$<br>&emsp;&emsp;之后，对于空间交互，可以通过将$F_{fuse}$重塑为$H×W×N$获得其中一个输出，另一个输出可以通过将$F_{fuse}$重塑为$N×H×W$获得。对于通道交互，两个最终输出相同，可以通过简单地将$F_{fuse}$展平来获得。</p><h2 id="4-3-Loss-Function"><a href="#4-3-Loss-Function" class="headerlink" title="4.3. Loss Function"></a>4.3. Loss Function</h2><p>&emsp;&emsp;我们使用平均绝对误差（MAE）来衡量模糊估计的损失，并使用交叉熵（CE）损失来衡量语义分割的损失。如图4(a)所示，辅助损失$L_1$和损失$L_3$都是MAE，而辅助损失$L_2$和损失$L_4$都是CE，具体如下：<br>$$\mathcal{L}<em>1&#x3D;\mathcal{L}<em>3&#x3D;\frac{1}{H\times W}\sum</em>{i&#x3D;1}^{H}\sum</em>{j&#x3D;1}^{W}\lVert B_{i,j}-\hat{B}<em>{i,j}\rVert_1\tag{12}$$<br>$$\mathcal{L}<em>2&#x3D;\mathcal{L}<em>4&#x3D;-\frac1{H\times W}\sum</em>{i&#x3D;1}^H\sum</em>{j&#x3D;1}^W\sum</em>{c&#x3D;1}^C\boldsymbol{S}<em>{i,j}^c\log(\hat{\boldsymbol{S}}</em>{i,j}^c)\tag{13}$$<br>&emsp;&emsp;其中$\hat{B}<em>{i,j}$和${B}</em>{i,j}$分别表示在位置$（i，j）$处的估计模糊图和相应的真实模糊图。类似地，$\hat{\boldsymbol{S}}<em>{ij}^{c}$和$S</em>{ij}^{c}$表示$c$类别在位置$（i，j）$处的估计语义图和真实语义图。$C$是对象类别的数量，$H，W$是地图的高度和宽度。我们没有采用特定的损失权重策略，而是简单地将损失相加。<br>$$\mathcal{L}&#x3D;\mathcal{L}_1+\mathcal{L}_2+\mathcal{L}_3+\mathcal{L}_4\tag{14}$$</p><h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h2><h2 id="5-1-Experimental-Setup"><a href="#5-1-Experimental-Setup" class="headerlink" title="5.1. Experimental Setup"></a>5.1. Experimental Setup</h2><p>&emsp;&emsp;<strong>CMOS的设置。</strong> 我们选择HRNet [37]作为我们的主干网络，并将前两个卷积的步幅改为1。这相当于输入LR图像的4个尺度（1、1&#x2F;4、1&#x2F;8、1&#x2F;16）。任务特定的头部被实现为两个基本残差块[9]。对于语义分割，我们在NYUv2-BSR中使用官方的40个类别，在Cityscapes-BSR中使用19个类别。所有实验均使用预训练的ImageNet权重进行。<br>&emsp;&emsp;<strong>非盲超分辨率的设置。</strong> 对于非盲超分辨率，我们使用[23]中提出的RRDB-SFT模型。为了将模糊和语义信息输入模型，我们使用了GIA模块。最后，我们使用CMOS估计的模糊和语义地图来微调RRDB-SFT模型。超分辨率图像和高分辨率图像之间的损失也是MAE。<br>&emsp;&emsp;<strong>实现细节。</strong> 我们选择了640×480的图像大小，用于NYUv2-BSR和Cityscapes-BSR。我们通过使用${1，1.2，1.5}$中的随机比例进行缩放来增强训练数据，并将模糊值除以比例。我们还以0.5的概率翻转训练样本。我们使用$β_1 &#x3D; 0.9$和$β_2 &#x3D; 0.99$的Adam优化器[16]训练模型，批量大小为8，训练700个epochs。学习率初始化为0.0001，采用带有10个热身epochs的余弦学习率调度。使用PyTorch实现，在RTX 3090 GPU上训练CMOS大约需要28小时。<br>&emsp;&emsp;<strong>评估指标。</strong> 对于模糊估计，我们使用PSNR和SSIM [41]。对于语义分割，我们使用mIoU。对于由RRDB-SFT生成的最终SR图像，其中模糊和语义地图是由CMOS估计的，我们在YCbCr空间的Y通道上比较PSNR&#x2F;SSIM。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708102231.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708102242.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708102251.png" alt="Img"></p><h2 id="5-2-Comparison-with-the-State-of-the-Arts"><a href="#5-2-Comparison-with-the-State-of-the-Arts" class="headerlink" title="5.2. Comparison with the State-of-the-Arts"></a>5.2. Comparison with the State-of-the-Arts</h2><p>&emsp;&emsp;我们将CMOS与现有的盲超分辨率模型进行比较：KernelGAN [1]、KOALAnet [15]、DCLS [28]、DAN [10]和MANet [23]以及上限模型（在给定真实模糊和语义地图的情况下的RRDB-SFT）。我们使用它们的官方实现和设置在NYUv2-BSR和Cityscapes-BSR上重新训练了所有比较方法。KernelGAN是一种无监督方法，仅在测试时对LR图像进行训练。DCLS和DAN是针对空间不变模糊的端到端方法，而KOALAnet和MANet是针对空间变异模糊的两阶段方法。由于我们使用了MANet中提出的非盲超分辨率模型（即RRDB-SFT），因此我们采用相同的设置以确保公平性。<br>&emsp;&emsp;<strong>定量比较。</strong> 如表2和表3所示，CMOS在两个提出的数据集中的不同测试组中均表现最佳。值得注意的是，仅估计图像的一个模糊核的方法（即Kernel GAN，DCLS和DAN）在真实核空间变异时都会出现严重的性能下降。虽然KOALAnet为不同的图像像素估计不同的核，但它不包括任何用于空间变异特性的特殊处理，并且也产生不良的结果。MANet考虑到模糊的局部性质，因此表现相对较好。相比之下，所提出的模型CMOS有效地利用语义信息帮助空间变异模糊估计和非盲超分辨率，大幅优于MANet。<br>&emsp;&emsp;<strong>定性比较。</strong> 我们在图5中展示了几个代表性的视觉样本。可以观察到，我们的CMOS在去除模糊和避免伪影方面均优于以前的方法。其他方法可能会产生振铃伪影（尤其是KernelGAN），或者无法恢复纹理细节，导致补丁仍然模糊。</p><h2 id="5-3-Ablation-Study"><a href="#5-3-Ablation-Study" class="headerlink" title="5.3. Ablation Study"></a>5.3. Ablation Study</h2><p>&emsp;&emsp;本节中的所有实验均使用NYUv2-BSR进行训练，指标（即PSNR、SSIM和mIoU）是指跨5个测试集（第3节）的平均值。<br>&emsp;&emsp;<strong>使用空间变异模糊进行训练的重要性。</strong> 根据[23]，由于中等的感受野，即使在空间不变的模糊图像上训练，MANet也可以处理空间变异的情况。但我们认为有必要使用包含空间变异模糊的图像进行训练。为了证明这一点，我们训练了两个MANet模型：一个在提出的NYUv2-BSR数据集上，另一个在从NYUv2数据集生成的空间不变模糊图像上。比较结果如表4所示。显然，使用空间变异模糊图像进行训练可以显著提高超分辨率图像的PSNR和SSIM，分别提高了10.62 dB和0.2266。这表明，在训练和测试阶段保持图像模糊类型的一致性非常重要。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708103622.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708103636.png" alt="Img"><br>&emsp;&emsp;<strong>GIA模块的有效性。</strong> 我们取出GIA模块的组成部分，即基于流的上采样（F）、通道交互（C）和空间交互（S），以验证其有效性。我们分别记录最佳PSNR和mIoU模型。如表5所示，仅使用基于流的上采样略微改善了结果，当与通道交互结合时，性能可以显著提高。此外，利用所有三个组件，即完整的GIA模块，可以产生更大的改进。<br>&emsp;&emsp;<strong>语义信息在SR中的有效性。</strong> 为了说明语义信息有助于SR，我们对它进行了削弱，仅将模糊地图输入到RRDB-SFT中。值得注意的是，我们在这里使用了真实的模糊和语义地图。如表6所示，添加语义地图可以改善最终SR结果的PSNR（+0.34 dB↑）和SSIM（+0.0022↑）。我们认为，语义信息可能使网络利用它所学习的相关对象的纹理特征，并且清晰的语义边缘也可能有助于SR。<br>&emsp;&emsp;<strong>多任务学习（MTL）的有效性。</strong> 为了证明MTL的有效性，首先，我们对模糊和语义地图进行分别预测，并将它们与CMOS的结果进行比较。其次，我们比较仅利用预测的模糊地图与同时利用预测的模糊和语义地图所实现的SR结果。如表7所示，联合估计提高了语义分割的结果（mIoU +1.66↑），尽管模糊估计的性能略有下降。但总体上，MTL可以将最终SR结果的PSNR&#x2F;SSIM提高+1.28↑&#x2F;+0.002↑，证明语义对整个SR过程有用。<br>&emsp;&emsp;<strong>辅助监督的重要性。</strong> 我们在CMOS中去除辅助监督，以查看它是否对我们的框架必要。如表8所示，在多尺度结构中没有辅助监督，虽然SSIM略有增加，但PSNR和mIoU分别下降了0.38↓和0.24%↓。因此，辅助监督可以在整体上提高CMOS的性能。</p><h2 id="5-4-Experiments-on-Real-Wrold-SR"><a href="#5-4-Experiments-on-Real-Wrold-SR" class="headerlink" title="5.4. Experiments on Real-Wrold SR"></a>5.4. Experiments on Real-Wrold SR</h2><p>&emsp;&emsp;由于实际图像没有真实的地面真值，因此我们只比较不同方法的视觉结果。如图6所示，与我们数据集中的结果类似，KernelGAN仍会产生振铃伪影，特别是在室外场景中。DAN、DCLS和KOALAnet都会产生模糊结果，而MANet表现略好。相比之下，CMOS可以产生逼真自然的纹理，结果最清晰。</p><h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>&emsp;&emsp;本文将焦外模糊引入SR，并提出了两个新的数据集：NYUv2-BSR和Cityscapes-BSR。此外，我们进一步提出了一种新的模型CMOS，可以同时估计模糊和语义地图。通过将语义信息纳入模型，我们可以恢复更细腻的SR结果。GIA模块用于在空间和通道维度上实现有效的特征交互。在提出的数据集和实际图像上的广泛实验表明，我们的模型在与现有的非盲模型集成时可以实现SOTA性能。致谢：本工作得到中国国家自然科学基金（61836015）的支持。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>DRL-DASR</title>
      <link href="/2023/08/14/DRL-DASR/"/>
      <url>/2023/08/14/DRL-DASR/</url>
      
        <content type="html"><![CDATA[<h1 id="DRL-DASR"><a href="#DRL-DASR" class="headerlink" title="DRL-DASR"></a>DRL-DASR</h1><h1 id="Unsupervised-Degradation-Representation-Learning-for-Blind-Super-Resolution"><a href="#Unsupervised-Degradation-Representation-Learning-for-Blind-Super-Resolution" class="headerlink" title="Unsupervised Degradation Representation Learning for Blind Super-Resolution"></a>Unsupervised Degradation Representation Learning for Blind Super-Resolution</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;大多数现有基于卷积神经网络（CNN）的超分辨率（SR）方法都是基于一个假设，即降采样是固定和已知的（例如双三次插值）。然而，当真实降采样与这个假设不同时，这些方法的性能会严重下降。为了处理现实世界中的各种未知降采样情况，先前的方法依赖于降采样估计来重建SR图像。然而，降采样估计方法通常耗时，并且可能由于大的估计误差而导致SR失败。在本文中，我们提出了一种无监督的降采样表示学习方案，用于盲SR而无需明确的降采样估计。具体而言，我们学习抽象表示，以在表示空间中区分各种降采样情况，而不是在像素空间中进行明确的估计。此外，我们引入了一种基于学习表示的、具有灵活适应各种降采样情况的降采样感知SR（DASR）网络。实验证明，我们的降采样表示学习方案可以提取有区分性的表示，以获取准确的降采样信息。在合成和真实图像上的实验表明，我们的网络实现了盲SR任务的最新性能。代码可在<a href="https://github.com/LongguangWang/DASR">https://github.com/LongguangWang/DASR</a> 获得。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>&emsp;&emsp;单幅图像超分辨率（SR）旨在从低分辨率（LR）观测中恢复高分辨率（HR）图像。最近，基于卷积神经网络（CNN）的方法[9、22、24、2、32]由于深度神经网络的强大特征表示能力，已经成为SR研究的主流方法。作为典型的反问题，SR与退化模型[3]高度耦合。大多数现有的基于CNN的方法都是基于一个假设，即降采样是已知和固定的（例如双三次插值）。然而，当真实的降采样与这个假设不同时，这些方法的性能会严重下降[12]。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724110120.png" alt="Img"><br>&emsp;&emsp;为了处理现实世界应用中的各种降采样情况，已经提出了几种方法[42、33、40、34]来研究非盲SR问题。具体而言，这些方法使用一组降采样情况（例如，不同组合的高斯模糊、运动模糊和噪声）进行训练，并假定在推理时已知测试LR图像的降采样情况。当真实的降采样情况已知时，这些非盲方法可以产生有前途的SR结果。<br>&emsp;&emsp;为了对未知降采样情况的实际图像进行超分辨率处理，需要进行降采样估计[28、3]，为非盲SR网络[42、33、40、34]提供降采样信息。然而，这些非盲方法对降采样估计非常敏感。因此，估计误差可能会被SR网络进一步放大，导致明显的伪影[12]。为解决这个问题，Gu等人[12]提出了一种迭代核校正（IKC）方法，通过观察之前的SR结果来校正估计的降采样情况。通过迭代地校正降采样情况，可以逐步生成无伪影的结果。由于降采样估计方法[28、3]和IKC [12]在测试时需要大量迭代，因此这些方法很耗时。<br>&emsp;&emsp;与以上方法明确从LR图像估计降采样不同，我们研究了一种不同的方法，即通过学习降采样表示来区分潜在降采样情况和其他情况。受到对比学习[13、10、35、17、5]的最新进展的启发，我们使用对比损失来进行无监督的降采样表示学习，通过在潜在空间中将正对比对与负对比对进行对比（如图1所示）。降采样表示学习的好处有两个方面：首先，与提取完整表示来估计降采样相比，更容易学习抽象表示来区分不同的降采样情况。因此，我们可以获得有区分性的降采样表示，以在单个推理中提供准确的降采样信息。其次，降采样表示学习不需要来自基准降采样的监督。因此，它可以以无监督的方式进行，并且更适用于具有未知降采样的实际应用。<br>&emsp;&emsp;在本文中，我们介绍了一种针对盲SR的无监督降采样表示学习方案。具体而言，我们假设图像中的降采样是相同的，但对于不同的图像可能是不同的，这是文献[42、3、40]中广泛使用的一般情况。因此，在降采样表示空间中，图像补丁应该与相同降采样的同一图像中的其他补丁相似，并且与来自其他图像（即具有不同降采样）的补丁不相似，如图1所示。此外，我们提出了一种具有灵活适应不同降采样的降采样感知SR（DASR）网络，该网络基于学习到的表示通过预测卷积核和通道调制系数来融合降采样信息以进行特征适应。实验结果表明，我们的网络可以处理各种降采样，且在盲设置下对合成和实际图像都能产生有前途的结果。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>&emsp;&emsp;本节中，我们简要回顾了几个针对基于CNN的单幅图像超分辨率和对比学习的最新进展的主要工作。</p><h3 id="2-1-Single-Image-Super-Resolution"><a href="#2-1-Single-Image-Super-Resolution" class="headerlink" title="2.1. Single Image Super-Resolution"></a>2.1. Single Image Super-Resolution</h3><p>&emsp;&emsp;<strong>针对单一降采样的SR。</strong> 作为先驱性工作，SRCNN [9]使用三层网络学习单幅图像SR的LR-HR映射。从那时起，基于CNN的方法由于其有前途的性能已经主导了SR的研究。Kim等人[21]提出了一个20层网络，采用了残差学习策略。Lim等人[24]遵循了残差学习思想，修改了残差块以构建一个非常深和宽的网络，即EDSR。然后，Zhang等人[45]结合了残差学习和密集连接，构建了一个超过100层的残差密集网络（RDN）。Haris等人[14]引入了多个上采样&#x2F;下采样层来提供误差反馈机制，并使用自校正特征产生了优越的结果。最近，RCAN [44]和SAN [7]进一步引入了通道注意力和二阶通道注意力，以利用特征相关性来提高性能。<br>&emsp;&emsp;<strong>多重降采样的SR。</strong> 尽管上述SR方法取得了重要进展，但它们针对的是固定的双三次降采样，并且当实际降采样与双三次降采样不同时性能严重下降[12]。为了处理各种降采样，已经进行了一些工作[42、38、40、20]来研究非盲SR问题。具体而言，在SRMD [42]中首先将降采样作为附加输入用于在不同降采样下超分辨LR图像。随后，在UDVD [38]中进一步加入动态卷积，以实现比SRMD更好的性能。最近，Zhang等人[40]开发了一个展开SR网络（USRnet）来处理不同的降采样，通过交替解决数据子问题和先验子问题。Hussein等人[20]引入了一个封闭形式的校正滤波器，将LR图像转换为与双三次降采样生成的图像匹配的图像。然后，可以使用针对双三次降采样训练的现有网络来超分辨变换后的LR图像。<br>&emsp;&emsp;还研究了零样本方法以实现多重降采样的SR。在ZSSR [33]中，使用降采样和LR图像作为输入，在测试时进行训练。因此，网络可以适应给定的降采样。然而，ZSSR需要数千次迭代才能收敛，非常耗时。为了解决这个限制，MZSR [34]中使用基于优化的元学习，在推理过程中使网络适应于特定的降采样，只需要几次迭代即可完成。<br>&emsp;&emsp;由于这些先前提到的方法使用降采样作为输入，它们高度依赖于盲SR中的降采样估计方法[28、3]。因此，降采样估计误差最终可能会引入不希望的伪影到SR结果中[12]。为了解决这个问题，Gu等人[12]提出了一个迭代核校正（IKC）方法，通过观察先前的SR结果来纠正估计的降采样。Luo等人[25]通过迭代地估计降采样和恢复SR图像，开发了一个深度交替网络（DAN）。</p><h2 id="2-2-Contrastive-Learning"><a href="#2-2-Contrastive-Learning" class="headerlink" title="2.2. Contrastive Learning"></a>2.2. Contrastive Learning</h2><p>&emsp;&emsp;对比学习已经证明其在无监督表示学习中的有效性。以往的方法[8、43、29、11]通常通过最小化输出与固定目标（例如自编码器的输入本身）之间的差异来进行表示学习。对比学习不使用预定义和固定的目标，而是最大化表示空间中的互信息。具体而言，查询样本的表示应该吸引正样本，同时排斥负样本。正样本可以是输入的转换版本[37、5、17]，输入的多个视图[35]以及同一图像中的相邻补丁[30、18]。在本文中，将生成相同降采样的图像补丁视为正样本，并进行对比学习以获得内容不变降采样表示，如图1所示。</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><h3 id="3-1-Problem-Formulation"><a href="#3-1-Problem-Formulation" class="headerlink" title="3.1. Problem Formulation"></a>3.1. Problem Formulation</h3><p>&emsp;&emsp;一个低分辨率图像$I^{LR}$的降采样模型可以表述如下：<br>$$I^{LR}&#x3D;(I^{HR}\otimes k)\downarrow_s+n,\tag{1}$$<br>其中，$I^{HR}$是高分辨率图像，$k$是模糊核，$⊗$表示卷积运算，$↓s$表示缩小因子为$s$的下采样操作，$n$通常表示加性白噪声。根据[42、12]的方法，我们使用双三次下采样器作为下采样操作。在本文中，我们首先研究了一个无噪声的降采样模型，使用各向同性高斯核，然后研究了一个更一般的降采样模型，使用各向异性高斯核和噪声。最后，我们在真实世界的降采样情况下测试了我们的网络。</p><h3 id="3-2-Our-Method"><a href="#3-2-Our-Method" class="headerlink" title="3.2. Our Method"></a>3.2. Our Method</h3><p>&emsp;&emsp;我们的盲SR框架包括一个降采样编码器和一个降采样感知SR网络，如图2所示。首先，LR图像被输入降采样编码器（图2（a）），以获得降采样表示。然后，将此表示结合到降采样感知SR网络（图2（b））中，以产生SR结果。</p><h4 id="3-2-1-Degradation-Representation-Learning"><a href="#3-2-1-Degradation-Representation-Learning" class="headerlink" title="3.2.1 Degradation Representation Learning"></a>3.2.1 Degradation Representation Learning</h4><p>&emsp;&emsp;降采样表示学习的目标是以无监督的方式从LR图像中提取出一个具有判别性的表示。如图1所示，我们使用对比学习框架[17]进行降采样表示学习。请注意，我们假设每个图像中的降采样是相同的，但对于不同的图像则可能不同。<br>&emsp;&emsp;<strong>表述。</strong> 给定一个图像补丁（在图1中用橙色框标注），从同一LR图像中提取的其他补丁（例如用红色框标注的补丁）可以视为正样本。相反，来自其他LR图像的补丁（例如用蓝色框标注的补丁）可以称为负样本。然后，我们使用具有六层的卷积网络对查询、正样本和负样本补丁进行降采样表示编码（如图2(a)所示）。如SimCLR [5]和MoCo v2 [6]所建议的那样，将得到的表示进一步输入到两层多层感知机（MLP）投影头中，以获得$x$、$x+$和$x−$。鼓励$x$与$x+$相似，同时与$x−$不相似。按照MoCo [17]的方法，使用InfoNCE损失来衡量相似性。即，<br>$$L_x&#x3D;-\mathrm{log}\frac{\exp(x\cdot x^+&#x2F;\tau)}{\sum_{n&#x3D;1}^N\exp(x\cdot x_n^-&#x2F;\tau)},\tag{2}$$<br>其中，$N$是负样本的数量，$τ$是温度超参数，$·$表示两个向量之间的点积。<br>&emsp;&emsp;正如现有的对比学习方法[5、17、31]所强调的那样，一个包含丰富的负样本集合的大型字典对于良好的表示学习至关重要。为了获得内容不变的降采样表示，我们维护一个包含具有各种内容和降采样的样本的队列。在训练期间，首先随机选择$B$个LR图像（即$B$种不同的降采样），然后从每个图像中随机裁剪两个补丁。接下来，将这$2B$个补丁使用我们的降采样编码器编码为${p^1_i，p^2_i∈\mathbb{R}^256}$，其中$p^1_i$是第$i$个图像的第一个补丁的嵌入。对于第$i$个图像，我们称$p^1_i$和$p^2_i$为查询和正样本。总损失定义为：<br>$$L_{degrad}&#x3D;\sum_{i&#x3D;1}^B-\log\frac{\exp(p_i^1\cdot p_i^2&#x2F;\tau)}{\sum_{j&#x3D;1}^{N_{queue}}\exp(p_i^1\cdot p_{queue}^j&#x2F;\tau)},\tag{3}$$<br>其中，$N_{queue}$是队列中样本的数量，$p^j_{queue}$表示第$j$个负样本。<br>&emsp;&emsp;<strong>讨论。</strong> 现有的降采样估计方法[28、3、12]旨在在像素级别估计降采样（通常是模糊核）。也就是说，这些方法学习提取降采样的完整表示。然而，它们在推断过程中需要大量迭代，因此耗时较长。例如，KernelGAN在测试期间进行网络训练，单张图像需要超过60秒[3]。与这些方法不同，我们旨在学习一个“良好”的抽象表示，以区分特定的降采样和其他降采样，而不是显式地估计降采样。如第4.2节所示，我们的降采样表示学习方案既有效又高效，并且可以在单次推断中获得具有判别性的表示。此外，我们的方案不需要来自于基准降采样的监督，可以以无监督的方式进行。</p><h4 id="3-2-2-Degradation-Aware-SR-Network"><a href="#3-2-2-Degradation-Aware-SR-Network" class="headerlink" title="3.2.2 Degradation-Aware SR Network"></a>3.2.2 Degradation-Aware SR Network</h4><p>&emsp;&emsp;通过进行降采样表示学习，我们提出了一种降采样感知SR（DASR）网络，使用得到的表示对LR图像进行超分辨率处理，如图2(b)所示。<br>&emsp;&emsp;<strong>网络架构。</strong> 图2(b)展示了我们DASR网络的架构。我们使用降采样感知块（DA块）作为构建块，并采用RCAN [44]的高层结构。我们的DASR网络由5个残差组组成，每组包含5个DA块。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724113654.png" alt="Img"><br>&emsp;&emsp;在每个DA块中，使用两个DA卷积层根据降采样表示调整特征，如图2(c)所示。受到模型训练不同恢复级别的卷积核具有相似模式但具有不同统计特性的观察启发，我们的DA卷积层学习在降采样表示的条件下预测深度卷积的卷积核。具体而言，将降采样表示$R$送入两个全连接（FC）层和一个重塑层，以生成一个卷积核$w∈\mathbb{R}^{C×1×3×3}$。然后，使用3×3的深度卷积（使用$w$）和1×1的卷积来处理输入特征$F$，以生成$F_1$。此外，受到CResMD [16]的启发（它使用控制变量来重新缩放不同通道以处理多个降采样），我们的DA卷积层还学习基于降采样表示生成调制系数，以进行通道级别的特征调整。具体而言，将$R$传递到另外两个FC层和一个sigmoid激活层中，以生成通道级别的调制系数$v$。然后，$v$用于重新缩放$F$中的不同通道分量，生成$F_2$。最后，将$F_1$与$F_2$相加并传递到后续层以生成输出特征$F_out$。<br>&emsp;&emsp;<strong>讨论。</strong> 现有的用于多种降采样的SR网络[42、38]通常将降采样表示与图像特征连接起来，然后将它们馈送到CNN中以利用降采样信息。然而，由于降采样表示和图像特征之间的领域差距，直接使用卷积对它们进行整体处理会引入干扰[12]。与这些网络不同，通过学习基于降采样表示预测卷积核和调制系数，我们的DASR可以很好地利用降采样信息来适应特定的降采样。如第4.2节所示，我们的DASR受益于DA卷积，可以实现对各种降采样的灵活适应，具有更好的SR性能。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Datasets-and-Implementation-Details"><a href="#4-1-Datasets-and-Implementation-Details" class="headerlink" title="4.1. Datasets and Implementation Details"></a>4.1. Datasets and Implementation Details</h3><p>&emsp;&emsp;我们按照公式1合成了LR图像进行训练和测试。与[12]类似，我们使用DIV2K [1]中的800张训练图像和Flickr2K [36]中的2650张训练图像作为训练集，并包括四个基准数据集（Set5 [4]、Set14 [39]、B100 [27]和Urban100 [19]）进行评估。高斯核的大小根据[12]固定为$21×21$。我们首先在仅具有各向同性高斯核的无噪声降采样上训练我们的网络。对于$×2&#x2F;3&#x2F;4$ SR，核宽度$σ$的范围分别设置为$[0.2,2.0]$、$[0.2,3.0]$和$[0.2,4.0]$。然后，我们的网络在具有非各向同性高斯核和噪声的更一般降采样上进行训练。考虑到具有由高斯概率密度函数$N(0，Σ)$（具有零均值和不同协方差矩阵$Σ$）所表征的非各向同性高斯核。协方差矩阵$Σ$由两个随机特征值$λ_1，λ_2 ∼ U(0.2，4)$和一个随机旋转角$θ ∼ U(0，π)$决定。噪声水平范围设置为$[0，25]$。<br>&emsp;&emsp;在训练过程中，随机选择32张HR图像，并通过随机旋转和翻转进行数据增强。然后，我们随机选择上述范围内的32个高斯核来生成LR图像。对于一般的降采样，高斯噪声也被添加到生成的LR图像中。接下来，随机裁剪64个$48×48$大小的LR补丁（如第3.2.1节所示的每个LR图像中的两个补丁）和它们对应的HR补丁。在我们的实验中，将Eq. 3中的τ和Nqueue设置为$0.07$和$8192$。使用$β_1 &#x3D; 0.9$和$β_2 &#x3D; 0.999$的Adam方法[23]进行优化。我们首先通过优化Ldegrad在100个epoch中训练降采样编码器。初始学习率设为$1×10^{−3}$，并在60个epoch后降至$1×10^{−4}$。然后，我们训练整个网络500个epoch。初始学习率设置为$1 × 10^{−4}$，并在每125个epoch后减半。总损失函数定义为 $L &#x3D; L_{SR} + L_{degrad}$，其中$L_{SR}$是SR结果和HR图像之间的$L_1$损失。</p><h3 id="4-2-Experiments-on-Noise-Free-Degradations-with-Isotropic-Gaussian-Kernels"><a href="#4-2-Experiments-on-Noise-Free-Degradations-with-Isotropic-Gaussian-Kernels" class="headerlink" title="4.2. Experiments on Noise-Free Degradations with Isotropic Gaussian Kernels"></a>4.2. Experiments on Noise-Free Degradations with Isotropic Gaussian Kernels</h3><p>&emsp;&emsp;我们首先对仅具有各向同性高斯核的无噪声降采样进行了消融实验。然后，我们将我们的DASR与几种最近的SR网络进行比较，包括RCAN [44]、SRMD [42]、MZSR [33]和IKC [12]。RCAN是一种针对双三次降采样的最先进的PSNR导向的SR方法。MZSR是一种针对具有各向同性&#x2F;非各向同性高斯核的降采样的非盲零样本SR方法。SRMD是一种针对具有各向同性&#x2F;非各向同性高斯核和噪声的非盲SR方法。IKC是一种盲SR方法，仅考虑具有各向同性高斯核的降采样。注意，我们没有将DAN [25]、USRnet [40]和修正滤波器[20]包括在比较中，因为它们的降采样模型与我们的不同。这些方法使用s倍下采样器1而不是双三次下采样器作为公式1中的下采样操作。为了与[25，40，20]进行公平比较，我们使用它们的降采样模型重新训练了我们的DASR，并在补充材料中提供了结果。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724115422.png" alt="Img"><br>&emsp;&emsp;<strong>降采样表示学习。</strong> 降采样表示学习用于生成判别性表示以提供降采样信息。为了证明其有效性，我们引入了一个网络变体（模型1），通过删除降采样表示学习来实现。具体来说，在训练过程中排除了Ldegrad，而没有改变网络。此外，去除了降采样编码器的单独训练，直接对整个网络进行了500个epoch的训练。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724115441.png" alt="Img"><br>&emsp;&emsp;首先，我们比较了模型1和模型4学习到的降采样表示。具体来说，我们使用$B100$生成不同降采样的LR图像，并将它们馈送到模型1和模型4中，以产生降采样表示。然后，使用T-SNE方法[26]可视化这些表示。如图3(b)所示，我们的降采样表示学习方案可以生成具有区分性的聚类。没有降采样表示学习，各种核宽度的降采样不能很好地区分，如图3(a)所示。这证明了降采样表示学习有助于我们的降采样编码器学习具有区分性的表示，以提供准确的降采样信息。我们进一步比较了模型1和模型4的SR性能。如果去除降采样表示学习，模型1无法很好地处理多种降采样，并且产生较低的PSNR值，特别是对于大的核宽度。相反，模型4从降采样表示学习提供的准确的降采样信息中受益，以实现更好的SR性能。<br>&emsp;&emsp;<strong>退化感知卷积。</strong> 通过使用降采样编码器，提取的降采样表示被DA卷积结合起来，通过预测卷积核和通道调制系数来实现对不同降采样的灵活适应。为了证明这两个关键组件的有效性，我们首先引入了一个变体（模型2），通过将DA卷积替换为普通卷积来实现。具体来说，在将降采样表示馈送到普通卷积之前，降采样表示被拉伸并与图像特征串联起来，类似于[42]。然后，我们开发了另一个变体（模型3），通过去除通道调制系数分支来实现。注意，我们调整了模型2和模型3中的通道数，以确保可比较的模型大小。从表1中我们可以看到，我们的DASR从动态卷积核和通道调制系数中受益，产生更好的各种降采样结果。<br>&emsp;&emsp;<strong>盲SR vs.非盲SR。</strong> 我们进一步通过提供真实的降采样来研究我们的DASR网络的上限性能。具体来说，我们用5个全连接层替换了降采样编码器，直接从真实的降采样（即模糊核）中学习表示。然后，从头开始训练这个网络变体（模型5）500个epoch。当提供真实的降采样时，模型5取得了改进的性能，并且在很大程度上优于SRMDNF。此外，在盲目设置下，SRMDNF对于降采样估计误差非常敏感，如果估计不准确，则PSNR值会降低（例如，$σ&#x3D;3.4$时，$27.55 vs. 26.66 &#x2F; 26.18$）。相比之下，我们的DASR（模型4）通过降采样表示学习受益，实现了更好的盲目SR性能。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724144121.png" alt="Img"><br>&emsp;&emsp;<strong>退化表征的研究。</strong> 我们的降采样表示旨在从LR图像中提取内容不变的降采样信息。为了证明这一点，我们进行了实验，研究不同图像内容对我们的降采样表示的影响。具体来说，给定一张HR图像，我们首先使用高斯核$k$生成一张LR图像$I_1$。然后，我们随机选择另外9张HR图像，使用同样的$k$生成LR图像$(I_i(i &#x3D; 2, 3, …10))$。接下来，从$I_i(i &#x3D; 1, 2, …10)$中提取降采样表示，以实现对$I_1$的超分辨率。注意，$I_i(i &#x3D; 2, 3, …10)$和$I_1$共享同样的降采样，但具有不同的图像内容。从图4中我们可以看到，我们的网络使用从不同图像内容学习的降采样表示实现了相对稳定的性能。这表明我们的降采样表示对图像内容的变化具有鲁棒性。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724143953.png" alt="Img"></p><p>&emsp;&emsp;<strong>与之前网络的比较。</strong> 我们将DASR与RCAN、SRMD、MZSR和IKC进行了比较。根据它们的默认设置，使用这些网络的预训练模型进行评估。定量结果如表2所示，可视化结果如图5所示。请注意，由于MZSR2&#x2F;IKC的其他比例因子的预训练模型不可用，因此仅测试了它们的$×2&#x2F;4$ SR结果。对于非盲SR方法（SRMD和MZSR），我们首先进行了降采样估计以提供降采样信息。由于KernelGAN非常耗时（表1），因此在IKC中使用了预测子网络来估计降采样。<br>&emsp;&emsp;从表2中可以看出，RCAN在双三次降采样（即核宽度0）上产生了最高的PSNR结果，但当测试降采样与双三次降采样不同时，性能相对较差。尽管SRMDNF和MZSR可以适应估计的降采样，但这些方法对降采样估计非常敏感，如表1所示。因此，SRMDNF和MZSR可能会放大降采样估计误差，导致SR性能受限。由于使用了迭代校正方案来校正估计的降采样，IKC在PSNR值方面优于SRMDNF。但是，IKC由于其迭代次数而耗时。与IKC相比，我们的DASR网络在更短的运行时间内实现了对不同降采样的更好性能。这是因为，我们的降采样表示学习方案可以在单个推理中提取“好”的表示来区分不同的降采样。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724143807.png" alt="Img"><br>&emsp;&emsp;图5展示了不同方法获得的可视化结果。由于RCAN是在固定的双三次降采样上训练的，因此当真实降采样与双三次降采样不同时，它无法可靠地恢复缺失的细节。尽管SRMDNF可以处理多种降采样，但降采样估计误差可能导致失败。通过迭代地校正估计的降采样，IKC实现了比SRMDNF更好的性能。与其他方法相比，我们的DASR产生了具有更清晰细节和更高感知质量的结果。</p><h3 id="4-3-Experiments-on-General-Degradations-with-Anisotropic-Gaussian-Kernels-and-Noises"><a href="#4-3-Experiments-on-General-Degradations-with-Anisotropic-Gaussian-Kernels-and-Noises" class="headerlink" title="4.3. Experiments on General Degradations with Anisotropic Gaussian Kernels and Noises"></a>4.3. Experiments on General Degradations with Anisotropic Gaussian Kernels and Noises</h3><p>&emsp;&emsp;我们进一步对具有各向异性高斯核和噪声的常见降采样进行实验。我们首先分析从常见降采样中学习到的降采样表示，然后在盲设置下将我们的DASR与RCAN、SRMDNF和IKC的性能进行比较。<br>&emsp;&emsp;<strong>退化表征的研究。</strong> 我们进行了实验，研究两个不同组成部分（即模糊核和噪声）对我们的降采样表示的影响。我们首先在图6（a）中可视化了各种模糊核的无噪声降采样表示。然后，我们随机选择一个模糊核，并在图6（b）中可视化了不同噪声水平的降采样表示。可以观察到，我们的降采样编码器可以轻松地将具有不同噪声水平的降采样聚类到具有区分性的群组中，并大致区分各种模糊核。<br>&emsp;&emsp;<strong>与之前网络的比较。</strong> 我们使用9种典型的模糊核和不同的噪声水平进行性能评估。为了使用RCAN、SRMDNF和IKC对带噪声的LR图像进行超分辨率，我们首先在盲设置下使用DnCNN [41]（一种先进的去噪方法）对LR图像进行去噪处理。由于IKC的预训练模型仅针对各向同性高斯核进行了训练，因此我们进一步在各向异性高斯核上对该模型进行微调以进行公平比较。对于SRMDNF，使用微调后的IKC模型的预测器子网络来估计降采样。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724144618.png" alt="Img"><br>&emsp;&emsp;从表3中可以看出，由于RCAN仅在双三次降采样上训练，因此其对复杂降采样的性能相对较低。由于SRMDNF对降采样估计误差敏感，其在复杂降采样上的性能受限。通过迭代校正估计的降采样，IKC表现出与SRMDNF相当的性能。然而，由于需要大量迭代，IKC更耗时。与IKC专注于像素级降采样估计不同，我们的DASR探索了一种有效而高效的方法来学习区分不同降采样的判别表示。使用我们的降采样表示学习方案，DASR在各种模糊核和噪声水平上的PSNR优于IKC，运行时间缩短了7倍以上。图7进一步说明了不同方法产生的可视化结果。我们的DASR获得了更好的视觉质量，而其他方法则存在明显的模糊伪影。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724144853.png" alt="Img"></p><h3 id="4-4-Experiments-on-Real-Degradations"><a href="#4-4-Experiments-on-Real-Degradations" class="headerlink" title="4.4. Experiments on Real Degradations"></a>4.4. Experiments on Real Degradations</h3><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/DRL-DASR.md/img-20230724144815.png" alt="Img"><br>&emsp;&emsp;我们进一步对真实降采样进行实验，以展示我们的DASR的有效性。按照[42]的方法，使用在各向同性高斯核上训练的DASR进行对真实图像的评估。可视化结果如图8所示。可以观察到，我们的DASR产生了更为优异的视觉效果，具有更清晰的细节和更少的模糊伪影。</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>&emsp;&emsp;本文提出了一种针对不同降采样的盲超分辨率问题的无监督降采样表示学习方案。我们使用对比学习方法提取判别性表示来区分不同的降采样，而不是明确估计降采样。此外，我们引入了一种基于学习表示的灵活适应不同降采样的降采样感知超分辨率（DASR）网络。实验结果表明，我们的降采样表示学习方案可以提取具有判别性的表示以获得准确的降采样信息。我们的网络在处理各种降采样的盲超分辨率问题上实现了最先进的性能。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>ESRGAN</title>
      <link href="/2023/08/14/ESRGAN/"/>
      <url>/2023/08/14/ESRGAN/</url>
      
        <content type="html"><![CDATA[<h1 id="ESRGAN"><a href="#ESRGAN" class="headerlink" title="ESRGAN"></a>ESRGAN</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>&emsp;&emsp;超分辨率生成对抗网络（SRGAN）[1]是一项开创性的工作，能够在单张图像超分辨率期间生成逼真的纹理。然而，产生的细节常常伴随着不愉快的伪影。为了进一步提高视觉质量，我们深入研究了SRGAN的三个关键组件——网络架构、对抗损失和感知损失，并对每个组件进行了改进，从而得出了增强版SRGAN（ESRGAN）。特别地，我们引入了残差内残差稠密块（RRDB）作为基本的网络构建单元，而没有批归一化。此外，我们借鉴了相对论GAN [2]的思想，让鉴别器预测相对真实性而不是绝对值。最后，我们通过使用激活之前的特征来改进感知损失，这可以为亮度一致性和纹理恢复提供更强的监督。由于这些改进，我们提出的ESRGAN在视觉质量上实现了更加逼真自然的纹理，比SRGAN表现更好，并赢得了PIRM2018-SR挑战赛的第一名[3]。代码可在 <a href="https://github.com/xinntao/ESRGAN">https://github.com/xinntao/ESRGAN</a> 上获得。</p><h2 id="1-简介"><a href="#1-简介" class="headerlink" title="1.简介"></a>1.简介</h2><p>&emsp;&emsp;单张图像超分辨率（SISR）作为一种基本的低层次视觉问题，已经引起了研究界和人工智能公司的越来越多的关注。SISR旨在从单个低分辨率（LR）图像中恢复高分辨率（HR）图像。自Dong等人提出的SRCNN的先驱工作[4]以来，深度卷积神经网络（CNN）方法已经迎来了繁荣的发展。各种网络架构设计和训练策略不断改进了SR性能，尤其是峰值信噪比（PSNR）值[5,6,7,1,8,9,10,11,12]。然而，这些以PSNR为导向的方法往往会输出过于平滑的结果，缺乏足够的高频细节，因为PSNR指标在本质上与人类观察者的主观评价不一致[1]。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529145131.png" alt="Img"><br>&emsp;&emsp;已经提出了几种感知驱动方法来改善SR结果的视觉质量。例如，感知损失[13,14]被提出在特征空间而不是像素空间中优化超分辨率模型。生成对抗网络[15]被引入到SR中，由[1,16]使用，以鼓励网络倾向于更像自然图像的解决方案。语义图像先验进一步被纳入以改善恢复的纹理细节[17]。追求视觉效果的一个里程碑是SRGAN [1]。基本模型采用残差块[18]构建，并在GAN框架中使用感知损失进行优化。通过所有这些技术，SRGAN显着提高了重建的整体视觉质量，超过了以PSNR为导向的方法。<br>&emsp;&emsp;然而，如图1所示，SRGAN的结果与真实图像之间仍存在明显差距。在本研究中，我们重新审视SRGAN的关键组件，并在三个方面改进模型。首先，我们通过引入残差内残差稠密块（RDDB）改进网络结构，这种块的容量更大且更易于训练。我们还像[20]一样去掉了批归一化（BN）[19]层，并使用残差缩放[21,20]和更小的初始化来促进训练非常深的网络。其次，我们使用相对论平均GAN（RaGAN）[2]改进判别器，该判别器学习判断“一个图像是否比另一个图像更真实”，而不是“一个图像是真实的还是伪造的”。我们的实验表明，这种改进有助于生成更逼真的纹理细节。第三，我们提出了一种改进的感知损失，通过使用VGG激活之前的特征而不是像SRGAN一样在激活之后使用。我们经验性地发现，调整后的感知损失提供了更锐利的边缘和更具视觉吸引力的结果，这将在第4.4节中展示。大量实验证明，增强版的SRGAN，即ESRGAN，在锐度和细节方面始终优于最先进的方法（请参见图1和图7）。<br>&emsp;&emsp;我们采用ESRGAN的一种变体参加PIRM-SR挑战赛[3]。这个挑战赛是第一个基于[22]以感知质量为导向评估SR性能的竞赛，其中作者声称失真和感知质量是相互矛盾的。感知质量由Ma分数[23]和NIQE[24]的非参考指标来评判，即感知指数&#x3D;1&#x2F;2((10-Ma)+NIQE)。较低的感知指数表示更好的感知质量。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/ESRGAN.md/img-20230530103239.png" alt="Img"></p><p>&emsp;&emsp;如图2所示，感知-失真平面被划分为三个区域，由均方根误差（RMSE）的阈值定义。在每个区域中，达到最低感知指数的算法成为该区域的冠军。我们主要关注区域3，因为我们的目标是将感知质量提高到一个新的高度。由于前面提到的改进和在第4.6节中讨论的一些其他调整，我们提出的ESRGAN在PIRM-SR挑战赛（区域3）中获得了第一名，并获得了最佳感知指数。<br>&emsp;&emsp;为了平衡视觉质量和RMSE &#x2F; PSNR，我们进一步提出了网络插值策略，可以连续调整重建风格和平滑度。另一种选择是图像插值，直接以像素为单位插值图像。我们采用这种策略参加区域1和区域2。网络插值和图像插值策略及其差异在第3.4节中进行了讨论。</p><h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2.相关工作"></a>2.相关工作</h2><p>&emsp;&emsp;我们专注于采用深度神经网络来解决SR问题。作为先驱性工作，Dong等人[4,25]提出了SRCNN，以端到端的方式学习从LR感知指数到HR图像的映射，取得了对先前工作的卓越性能。此后，该领域见证了各种网络架构的出现，例如具有残差学习的更深网络[5]、拉普拉斯金字塔结构[6]、残差块[1]、递归学习[7,8]、密集连接网络[9]、深度反向投影[10]和残差稠密网络[11]。具体而言，Lim等人[20]通过消除残差块中不必要的BN层并扩大模型规模提出了EDSR模型，取得了显着的改进。张等人[11]提出在SR中使用有效的残差稠密块，并进一步探索了具有通道注意力的更深网络[12]，取得了最先进的PSNR性能。除了监督学习外，还引入了其他方法，如强化学习[26]和无监督学习[27]来解决通用的图像恢复问题。<br>&emsp;&emsp;已经提出了几种方法来稳定训练非常深的模型。例如，残差路径被开发出来以稳定训练并提高性能[18,5,12]。残差缩放首先由Szegedy等人[21]使用，并且在EDSR中也使用。对于一般的深度网络，He等人[28]提出了一种针对VGG风格网络的强大初始化方法，没有BN层。为了便于训练更深的网络，我们开发了一种紧凑而有效的残差内残差稠密块，这也有助于提高感知质量。<br>&emsp;&emsp;针对提高SR结果的视觉质量，也提出了感知驱动方法。基于更接近感知相似性的思想[29,14]，提出了感知损失[13]，通过在特征空间而不是像素空间中最小化误差来增强视觉质量。发展了上下文损失[30]，通过使用聚焦于特征分布而非仅比较外观的目标，生成具有自然图像统计数据的图像。Ledig等人[1]提出了SRGAN模型，使用感知损失和对抗损失来倾向于输出位于自然图像流形上的结果。Sajjadi等人[16]开发了类似的方法，并进一步探索了局部纹理匹配损失。基于这些工作，Wang等人[17]提出了空间特征转换方法，在图像中有效地融入语义先验并改善恢复的纹理。<br>&emsp;&emsp;在文献中，通常通过使用GAN的对抗训练来实现照片般逼真的效果[15]。最近，有许多研究致力于开发更有效的GAN框架。WGAN [31]提出了最小化Wasserstein距离的合理有效近似，并通过权重剪裁对鉴别器进行正则化。其他用于鉴别器的改进正则化方法包括梯度剪裁[32]和谱归一化[33]。发展了相对论鉴别器[2]，不仅可以增加生成数据为真实数据的概率，而且同时可以降低真实数据为真实数据的概率。在本研究中，我们通过采用更有效的相对论平均GAN来增强SRGAN。<br>&emsp;&emsp;目前，超分辨率算法通常会使用几个广泛使用的失真度量标准进行评估，例如PSNR和SSIM。然而，这些指标在本质上与人类观察者的主观评估存在差异[1]。非参考指标用于感知质量评估，包括Ma分数[23]和NIQE[24]，这两者都用于在PIRM-SR挑战赛[3]中计算感知指数。在最近的一项研究中，Blau等人[22]发现失真和感知质量是相互矛盾的。</p><h2 id="3-提出方法"><a href="#3-提出方法" class="headerlink" title="3.提出方法"></a>3.提出方法</h2><p>&emsp;&emsp;我们的主要目标是提高SR的整体感知质量。在本节中，我们首先描述我们提出的网络架构，然后讨论鉴别器和感知损失的改进。最后，我们描述了网络插值策略，以平衡感知质量和PSNR。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529152626.png" alt="Img"></p><h3 id="3-1-网络架构"><a href="#3-1-网络架构" class="headerlink" title="3.1 网络架构"></a>3.1 网络架构</h3><p>&emsp;&emsp;为了进一步提高SRGAN的恢复图像质量，我们主要对生成器G的结构进行了两个修改：1）移除所有BN层；2）用提出的残差内残差稠密块（RRDB）替换原始的基本块，该块结合了多级残差网络和密集连接，如图4所示。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529152748.png" alt="Img"><br>::: tip Dense block<br>“Dense block”是一种用于深度卷积神经网络（CNN）中的模块化结构。它由黄俊炜等人在2016年提出，是对ResNet中残差块的改进。Dense block的特点是将每个层的特征图与之前所有层的特征图级联起来，从而增加了信息的传递和重用，使得网络更加深层、更加丰富和复杂的特征表示。在Dense block中，每个层的输出都被传递给后面的所有层，形成了密集的连接（dense connection），因此被称为“密集块”。相比于ResNet的残差块，Dense block在参数数量不变的情况下，可以实现更好的性能，因此被广泛应用于计算机视觉领域的各种任务，如图像分类、目标检测和图像分割等。<br>:::</p><p>&emsp;&emsp;在不同的以PSNR为导向的任务中，包括SR [20]和去模糊 [35]，移除BN层已被证明可以提高性能并减少计算复杂度。BN层在训练期间使用批次中的均值和方差来规范化特征，并在测试期间使用整个训练数据集的估计均值和方差。当训练和测试数据集的统计数据差异很大时，BN层往往会引入不愉快的伪影，并限制泛化能力。我们经验性地观察到，当网络更深且在GAN框架下进行训练时，BN层更容易引入伪影。这些伪影有时会在迭代和不同设置之间出现，违反了需要在训练期间保持稳定性能的需求。因此，我们移除BN层以实现稳定的训练和一致的性能。此外，移除BN层有助于提高泛化能力，减少计算复杂度和内存使用。<br>&emsp;&emsp;我们保留了SRGAN的高级架构设计（参见图3），并使用了一个新颖的基本块，即如图4所示的RRDB。基于更多的层和连接总是可以提高性能的观察结果[20,11,12]，所提出的RRDB比SRGAN中原始残差块具有更深和更复杂的结构。具体来说，如图4所示，所提出的RRDB具有残差内残差结构，其中在不同的级别上使用残差学习。 [36]提出了类似的网络结构，也应用了多级残差网络。然而，我们的RRDB与[36]不同之处在于，我们在主路径中使用了密集块[34]，如[11]中所述，从而使得网络容量变得更高，受益于密集连接。<br>&emsp;&emsp;除了改进的架构之外，我们还利用了几种技术来促进训练非常深的网络：1）残差缩放[21,20]，即通过在将残差添加到主路径之前乘以0到1之间的常数来缩小残差，以防止不稳定性；2）小的初始化，我们经验性地发现当初始参数方差变小时，残差架构更容易训练。更多讨论可以在补充材料中找到。<br>&emsp;&emsp;所提出的网络的训练细节和有效性将在第4节中介绍。</p><h3 id="3-2-相对鉴别器（Relativistic-Discriminator）"><a href="#3-2-相对鉴别器（Relativistic-Discriminator）" class="headerlink" title="3.2 相对鉴别器（Relativistic Discriminator）"></a>3.2 相对鉴别器（Relativistic Discriminator）</h3><p>&emsp;&emsp;除了改进生成器的结构外，我们还根据相对论GAN [2]的思想增强了鉴别器。与SRGAN中的标准鉴别器D不同，它估计输入图像x是真实且自然的概率，相对论鉴别器试图预测真实图像$x_r$相对于虚假图像$x_f$更真实的概率，如图5所示。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529160341.png" alt="Img"><br>::: tip 相对鉴别器<br>相对平均鉴别器（Relativistic Average Discriminator）是一种用于深度生成对抗网络（GAN）的鉴别器结构。它由Alexia Jolicoeur-Martineau在2018年提出，旨在解决GAN中存在的模式崩溃和训练不稳定等问题。相对平均鉴别器的核心思想是引入相对比较的概念，将鉴别器的输出从简单的真假二分类改为真实样本相对于生成样本更真实的概率，即计算真实样本比生成样本更真实的平均概率。这种相对比较可以使得鉴别器更好地判断样本的真实度，从而避免出现模式崩溃的情况。相对平均鉴别器的设计使得生成器在产生样本时更加注重真实样本的特征，从而可以生成更加真实的样本。近年来，相对平均鉴别器已经被广泛应用于各种GAN模型中，取得了很好的效果。<br>相对平均鉴别器（Relativistic Average Discriminator）的核心思想是引入相对比较的概念，将鉴别器的输出从简单的真假二分类改为真实样本相对于生成样本更真实的概率。具体而言，相对平均鉴别器的输出是真实样本x和生成样本y在鉴别器下的概率，即：</p><p>$$D(x,y) &#x3D; sigmoid(C(x,y) - E[yp_g][C(x,y)])$$</p><p>其中，C(x,y)是判别器的输出，表示x和y之间的相似程度；$E[yp_g][C(x,y)]$是生成样本y~p_g的期望相似程度。相对平均鉴别器的核心思想是将生成样本的相似程度从生成样本的期望中减去，从而得到真实样本相对于生成样本更真实的概率。</p><p>相对平均鉴别器的优点在于，它可以使得鉴别器更好地判断样本的真实度，从而避免出现模式崩溃的情况。在传统的GAN中，鉴别器只能简单地判断样本是真实还是伪造的，而相对平均鉴别器可以更加细致地区分样本的真实度，从而使得生成器更加注重真实样本的特征，生成的样本更加真实、多样化和有趣。</p><p>相对平均鉴别器已经被广泛应用于各种GAN模型中，如StyleGAN2、ProGAN和BigGAN等。它不仅可以提高GAN的生成质量和多样性，还可以使得GAN更加稳定和可靠。相对平均鉴别器的设计思想也启发了人们对GAN的更进一步改进和优化，如GAN的多尺度训练、交替训练等技术。<br>:::</p><p>&emsp;&emsp;具体来说，我们用相对论平均鉴别器RaD [2]替换了标准鉴别器，表示为$D_{Ra}$。SRGAN中的标准鉴别器可以表示为$D(x)&#x3D;\sigma(C(x))$，其中σ是sigmoid函数，$C(x)$是未变换的鉴别器输出。然后，相对论平均鉴别器定义为$D_{R a}(x_{r},x_{f})&#x3D;\sigma(C(x_{r})-\mathbb{E}<em>{x</em>{f}}[C(x_{f})])$，其中$\mathbb{E}<em>{x</em>{f}}[\cdot]$表示对小批量中所有虚假数据进行平均的操作。然后定义鉴别器损失如下：<br>$$L_{D}^{R a}&#x3D;-\mathbb{E}<em>{x</em>{r}}[\log(D_{R a}(x_{r},x_{f}))]-\mathbb{E}<em>{x</em>{f}}[\log(1-D_{R a}(x_{f},x_{r}))]. \tag{1}$$<br>&emsp;&emsp;生成器的对抗损失采用对称形式：<br>$$L_G^{Ra}&#x3D;-\mathbb{E}<em>{x_r}[\log(1-D</em>{Ra}(x_r,x_f))]-\mathbb{E}<em>{x_f}[\log(D</em>{Ra}(x_f,x_r))],\tag{2}$$<br>&emsp;&emsp;其中$x_f &#x3D; G(xi)$，$x_i$表示输入的低分辨率图像。观察到生成器的对抗损失包含$x_r$和$x_f$。因此，在对抗训练中，我们的生成器从生成数据和真实数据的梯度中受益，而在SRGAN中，只有生成的部分起作用。在第4.4节中，我们将展示鉴别器的这种修改有助于学习更锐利的边缘和更详细的纹理。</p><h3 id="3-3-感知损失"><a href="#3-3-感知损失" class="headerlink" title="3.3 感知损失"></a>3.3 感知损失</h3><p>&emsp;&emsp;我们还通过对激活之前的特征进行约束，而不是像SRGAN中那样在激活之后进行约束，开发了一种更有效的感知损失$L_{percep}$。<br>&emsp;&emsp;基于更接近感知相似性的想法[29,14]，Johnson等人[13]提出了感知损失，并在SRGAN[1]中进行了扩展。感知损失先前定义在预训练深度网络的激活层上，其中最小化了两个激活特征之间的距离。与传统方法相反，我们建议使用激活层之前的特征，这将克服原始设计的两个弱点。首先，激活的特征非常稀疏，特别是在非常深的网络之后，如图6所示。例如，在VGG19-543层之后的“baboon”图像中，激活的神经元平均百分比仅为11.17％。稀疏的激活提供了较弱的监督，因此导致了较差的性能。其次，使用激活后的特征也会导致与地面实况图像相比不一致的重建亮度，这将在第4.4节中进行展示。<br>&emsp;&emsp;因此，生成器的总损失为：<br>$$L_G&#x3D;L_\mathrm{percep}+\lambda L_G^{Ra}+\eta L_1,\tag{3}$$<br>其中$L_1&#x3D;\mathbb{E}_{x_i}||G(x_i)-y||_1$是内容损失，用于评估恢复图像$G(xi)$与真实图像$y$之间的1范数距离，而$λ$和$η$是平衡不同损失项的系数。<br>&emsp;&emsp;我们在PIRM-SR挑战中还探索了感知损失的一种变体。与通常用于图像分类的VGG网络采用的感知损失不同，我们开发了一种更适合SR的感知损失——MINC损失。它基于一个针对材料识别[38]进行微调的VGG网络，该网络专注于纹理而非对象。虽然MINC损失带来的感知指数增益很小，但我们仍然认为探索专注于纹理的感知损失对于SR至关重要。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529161903.png" alt="Img"></p><h3 id="3-4-网络插值（Network-Interpolation）"><a href="#3-4-网络插值（Network-Interpolation）" class="headerlink" title="3.4 网络插值（Network Interpolation）"></a>3.4 网络插值（Network Interpolation）</h3><p>&emsp;&emsp;为了在保持良好感知质量的同时消除基于GAN的方法中的不良噪声，我们提出了一种灵活而有效的策略——网络插值。具体来说，我们首先训练一个以PSNR为导向的网络GPSNR，然后通过微调获得一个基于GAN的网络GGAN。我们插值这两个网络的所有对应参数，得到一个插值模型$G_{INTERP}$，其参数为：<br>$$\theta_G^{\mathrm{INTERP}}&#x3D;(1-\alpha)\theta_G^{\mathrm{PSNR}}+\alpha\theta_G^{\mathrm{GAN}},\tag{4}$$<br>$\theta_G^{\mathrm{INTERP}}$,$\theta_G^{\mathrm{PSNR}}$和$\theta_G^{\mathrm{GAN}}$分别是是$G_{\mathrm{INTERP}}$,$G_{\mathrm{PSNR}}$和$G_{GAN}$的参数，$\alpha \in [0,1]$是插值参数。<br>&emsp;&emsp;所提出的网络插值具有两个优点。首先，插值模型能够在任何可行的α值下产生有意义的结果，而不会引入伪影。其次，我们可以在不重新训练模型的情况下连续平衡感知质量和保真度。<br>&emsp;&emsp;我们还探索了平衡PSNR导向和基于GAN的方法效果的替代方法。例如，可以直接插值它们的输出图像（逐像素插值），而不是网络参数。然而，这种方法未能实现噪声和模糊之间的良好平衡，即插值图像要么太模糊，要么太嘈杂且伴有伪影（见第4.5节）。另一种方法是调整内容损失和对抗损失的权重，即在公式（3）中的参数λ和η。但这种方法需要调整损失权重和微调网络，因此过于昂贵，无法实现对图像样式的连续控制。</p><h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4.实验"></a>4.实验</h2><h3 id="4-1-训练细节"><a href="#4-1-训练细节" class="headerlink" title="4.1 训练细节"></a>4.1 训练细节</h3><p>&emsp;&emsp;与SRGAN [1]一样，所有实验都使用LR和HR图像之间的4倍缩放因子进行。我们使用MATLAB双三次插值内核函数对HR图像进行下采样以获得LR图像。小批量大小设置为16。裁剪的HR图像块的空间大小为128×128。我们观察到，训练更深的网络受益于更大的图像块大小，因为扩大的感受野有助于捕获更多的语义信息。然而，这会增加训练时间并消耗更多的计算资源。这种现象在PSNR导向的方法中也有所观察（请参见补充材料）。<br>&emsp;&emsp;训练过程分为两个阶段。首先，我们使用L1损失训练一个以PSNR为导向的模型。学习率初始化为2×10−4，并在每2×105次小批量更新后降低2倍。然后，我们将训练好的PSNR导向模型用作生成器的初始化。使用公式（3）中的损失函数训练生成器，其中λ&#x3D;5×10−3，η&#x3D;1×10−2。学习率设置为1×10−4，并在[50k、100k、200k、300k]次迭代时减半。使用像素级损失进行预训练有助于GAN方法获得更具视觉吸引力的结果。原因是：1）它可以避免生成器陷入不良的局部最优；2）在预训练之后，鉴别器接收到相对较好的超分辨图像，而不是在最开始时接收到极端的伪造图像（黑色或带有噪声的图像），这有助于它更多地关注纹理鉴别。<br>&emsp;&emsp;我们使用Adam [39]优化器，其中β1 &#x3D; 0.9，β2 &#x3D; 0.999。我们交替更新生成器和鉴别器网络，直到模型收敛。我们使用两种设置来训练我们的生成器——其中一个包含16个残差块，容量类似于SRGAN，另一个是更深的模型，有23个RRDB块。我们使用PyTorch框架实现我们的模型，并使用NVIDIA Titan Xp GPU进行训练。</p><h3 id="4-2-数据"><a href="#4-2-数据" class="headerlink" title="4.2 数据"></a>4.2 数据</h3><p>&emsp;&emsp;我们主要使用DIV2K数据集[40]进行训练，该数据集是用于图像恢复任务的高质量（2K分辨率）数据集。除了包含800张图像的DIV2K训练集之外，我们还寻找其他具有丰富和多样纹理的数据集进行训练。为此，我们进一步使用由Flickr网站收集的2650张2K高分辨率图像组成的Flickr2K数据集[41]，以及OutdoorSceneTraining（OST）[17]数据集来丰富我们的训练集。我们经验性地发现，使用这个具有更丰富纹理的大型数据集有助于生成器产生更自然的结果，如图8所示。<br>&emsp;&emsp;我们在RGB通道中训练模型，并使用随机水平翻转和90度旋转对训练数据集进行增强。我们在广泛使用的基准数据集上评估我们的模型——Set5 [42]、Set14 [43]、BSD100 [44]、Urban100 [45]和PIRM自我验证数据集（在PIRM-SR挑战中提供）。</p><h3 id="4-3-定性结果"><a href="#4-3-定性结果" class="headerlink" title="4.3 定性结果"></a>4.3 定性结果</h3><p>&emsp;&emsp;我们将我们的最终模型与包括SRCNN [4]、EDSR [20]、RCAN [12]在内的最先进的PSNR导向方法，以及包括SRGAN [1]和EnhanceNet [16]在内的感知驱动方法在几个公共基准数据集上进行比较。由于没有有效且标准的感知质量度量标准，我们在图7中提供了一些代表性的定性结果。同时，我们还提供了在YCbCr颜色空间的亮度通道上评估的PSNR以及PIRM-SR挑战中使用的感知指数作为参考。<br>&emsp;&emsp;从图7中可以看出，我们提出的ESRGAN在锐度和细节方面优于以前的方法。例如，ESRGAN可以比以PSNR为导向的方法产生更锐利和更自然的狒狒胡须和草纹理（见图像43074），PSNR导向的方法往往生成模糊的结果，以及以前的基于GAN的方法，纹理不自然且包含不愉快的噪声。ESRGAN能够在建筑物中生成更多的细节结构（见图像102061），而其他方法要么无法产生足够的细节（SRGAN），要么添加不必要的纹理（EnhanceNet）。此外，以前的基于GAN的方法有时会引入不愉快的伪影，例如SRGAN会在脸上添加皱纹。我们的ESRGAN消除了这些伪影，产生自然的结果。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529170835.png" alt="Img"></p><h3 id="4-4-消融研究"><a href="#4-4-消融研究" class="headerlink" title="4.4 消融研究"></a>4.4 消融研究</h3><p>::: tip 消融研究<br>消融研究是指在机器学习或深度学习中，通过对模型的组成部分进行逐一去除或改变，以验证它们对模型性能的影响。消融研究的目的是帮助研究者理解模型的工作原理，找到最重要的组成部分，以及提供改进模型性能的方向。在消融研究中，通过对比不同模型的性能差异，可以推断出哪些组成部分最为重要，或哪些设计决策可能导致性能下降。<br>:::<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529171734.png" alt="Img"></p><p>&emsp;&emsp;为了研究提出的ESRGAN中每个组件的影响，我们逐步修改基线SRGAN模型并比较它们之间的差异。整体的视觉比较如图8所示。每列代表一个模型，其配置显示在顶部。红色标志表示与上一个模型相比的主要改进。下面提供详细的讨论。<br>&emsp;&emsp;<strong>BN层的移除。</strong> 我们首先移除所有的BN层，以获得稳定和一致的性能，避免出现伪影。这并不会降低性能，但可以节约计算资源和内存使用。在某些情况下，可以从图8中第二列和第三列中观察到轻微的改进（例如，图像39）。此外，我们观察到，当网络更深、更复杂时，具有BN层的模型更容易引入不愉快的伪影。可以在补充材料中找到相关示例。<br>&emsp;&emsp; <strong>在感知损失的激活之前。</strong> 我们首先证明，在使用激活之前的特征可以得到更准确的重建图像亮度。为了消除纹理和颜色的影响，我们使用高斯核过滤图像，并绘制其灰度图像的直方图。图9a显示了每个亮度值的分布情况。使用激活的特征会使分布向左偏斜，导致输出变暗，而使用激活之前的特征会产生更准确的亮度分布，更接近于真实值。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529172019.png" alt="Img"><br>&emsp;&emsp;我们还可以观察到，在使用激活之前的特征的情况下，可以产生更锐利的边缘和更丰富的纹理，如图9b所示（例如鸟羽毛）和图8中的第三列和第四列（例如见到的“叶子”和“石头”图像），因为激活之前的密集特征提供了比稀疏激活更强的监督。<br>&emsp;&emsp; <strong>RaGAN。</strong> RaGAN使用了改进的相对论鉴别器，已经证明有助于学习更锐利的边缘和更详细的纹理。例如，在图8的第5列中，生成的图像比左侧的图像更锐利，纹理更丰富（例如狒狒、第39张图像和第43074张图像）。<br>&emsp;&emsp; <strong>使用RRDB的更深网络。</strong> 使用提出的RRDB的更深模型可以进一步提高恢复的纹理质量，特别是对于像图8中图像6的屋顶这样的规则结构，因为深层模型具有强大的表示能力来捕捉语义信息。此外，我们发现深层模型可以减少不愉快的噪声，例如图8中的图像20。<br>&emsp;&emsp;与SRGAN声称深层模型越来越难以训练不同，我们的深层模型在易于训练的情况下展现了其卓越的性能，这要归功于上述改进，特别是提出的不含BN层的RRDB。</p><h3 id="4-5-网络插值"><a href="#4-5-网络插值" class="headerlink" title="4.5 网络插值"></a>4.5 网络插值</h3><p>::: tip Network Interpolation<br>Network Interpolation（网络插值）是指在深度学习中，通过将两个或多个不同的神经网络模型进行“插值”，产生一个新的模型。具体来说，它是通过将两个模型的权重进行加权平均来产生一个新的模型，以实现在两个模型之间进行平滑的过渡。这个新的模型可以在某种程度上保留原始模型的优点，同时具有改进的性能。Network Interpolation在深度学习中被广泛应用，特别是在生成模型和图像处理任务中，如图像超分辨率和图像风格转换等。<br>:::<br>&emsp;&emsp;我们比较了网络插值和图像插值策略在平衡PSNR导向模型和基于GAN的方法结果方面的影响。我们在两个方案上应用简单的线性插值。插值参数α从0到1选择，间隔为0.2。<br>&emsp;&emsp;如图10所示，纯GAN方法产生了锐利的边缘和更丰富的纹理，但是还存在一些不愉快的伪影，而纯PSNR导向方法输出的是卡通风格的模糊图像。通过采用网络插值，可以减少不愉快的伪影，同时保持纹理。相比之下，图像插值未能有效地消除这些伪影。<br>&emsp;&emsp;有趣的是，观察到网络插值策略在图10中提供了平衡感知质量和保真度的平滑控制。</p><h3 id="4-6-PIRM-SR挑战"><a href="#4-6-PIRM-SR挑战" class="headerlink" title="4.6 PIRM-SR挑战"></a>4.6 PIRM-SR挑战</h3><p>&emsp;&emsp;我们使用ESRGAN的一个变体参加PIRM-SR挑战赛[3]。具体来说，我们使用了提出的具有16个残差块的ESRGAN，同时根据感知指数进行了一些经验性的修改。1）作为感知损失的一种变体，我们使用MINC损失，如第3.3节所讨论的。尽管感知指数方面的提升较小，但我们仍然认为探索侧重于纹理的感知损失对于SR至关重要。2）用于学习感知指数的Pristine数据集[24]也被用于我们的训练；3）由于PSNR的限制，我们还使用高达η&#x3D;10的L1损失权重；4）我们还使用背投影[46]作为后处理，可以提高PSNR，但有时会降低感知指数。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/FILES/ESRGAN.md/img-20230529172438.png" alt="Img"><br>&emsp;&emsp;对于需要更高PSNR的其他区域1和2，我们在我们的ESRGAN结果和PSNR导向方法RCAN[12]的结果之间使用图像插值。尽管我们观察到使用网络插值方案可以获得更美观的结果，但是图像插值方案实现了更低的感知指数（值越低越好）。我们提出的ESRGAN模型在PIRM-SR挑战赛（区域3）中获得了第一名，并获得了最佳的感知指数。</p><h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5 结论"></a>5 结论</h2><p>&emsp;&emsp;我们提出了一个ESRGAN模型，其感知质量比以前的SR方法一直保持较好的表现。该方法在PIRM-SR挑战赛中以感知指数获得了第一名。我们提出了一种新颖的架构，其中包含多个不含BN层的RDDB块。此外，我们采用了有用的技术，包括残差缩放和更小的初始化来促进所提出的深度模型的训练。我们还引入了使用相对论GAN作为鉴别器，学习判断一张图像是否比另一张更真实的方法，引导生成器恢复更详细的纹理。此外，我们通过使用激活之前的特征来增强感知损失，这提供了更强的监督，从而恢复更准确的亮度和逼真的纹理。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Real-ESRGAN</title>
      <link href="/2023/08/14/Real-ESRGAN/"/>
      <url>/2023/08/14/Real-ESRGAN/</url>
      
        <content type="html"><![CDATA[<h1 id="Real-ESRGAN"><a href="#Real-ESRGAN" class="headerlink" title="Real-ESRGAN"></a>Real-ESRGAN</h1><h1 id="Real-ESRGAN-Training-Real-World-Blind-Super-Resolution-with-Pure-Synthetic-Data"><a href="#Real-ESRGAN-Training-Real-World-Blind-Super-Resolution-with-Pure-Synthetic-Data" class="headerlink" title="Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data"></a>Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data</h1><p>[toc]<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620103043.png" alt="Img"></p><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;虽然盲超分辨率已经尝试了很多次，以恢复具有未知和复杂退化的低分辨率图像，但它们仍然远远无法解决一般的真实世界退化图像。在这项工作中，我们将强大的ESRGAN扩展到实际的恢复应用程序（即Real-ESRGAN），它是用纯合成数据进行训练的。具体而言，引入了高阶退化建模过程，以更好地模拟复杂的真实世界退化。我们还考虑到了合成过程中常见的环绕和过冲伪影。此外，我们采用了带有频谱归一化的U-Net鉴别器，以增加鉴别器的能力并稳定训练动态。广泛的比较表明，Real-ESRGAN在各种真实数据集上的视觉表现优于先前的工作。我们还提供了高效的实现来动态合成训练对。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>&emsp;&emsp;单图像超分辨率（SR）[13、10、27]是一个活跃的研究课题，旨在从低分辨率（LR）图像重建高分辨率（HR）图像。自SRCNN [9]的开创性工作以来，深度卷积神经网络（CNN）方法在SR领域带来了繁荣的发展。然而，大多数方法[21、27、20、25、50]假设理想的双三次降采样核，这与实际退化不同。这种退化不匹配使得这些方法在实际应用中不切实际。<br>&emsp;&emsp;相反，盲超分辨率[35、2、56]旨在恢复受到未知和复杂退化的低分辨率图像。根据底层退化过程，现有方法可以粗略地分为显式建模和隐式建模两类。经典的退化模型[11、29]，包括模糊、下采样、噪声和JPEG压缩（更多细节见第3.1节），广泛应用于显式建模方法[56、16、34]中。然而，真实世界的退化通常太复杂，无法用多种退化的简单组合来建模。因此，这些方法在真实世界的样本中很容易失败。隐式建模方法[54、12、46]利用生成对抗网络（GAN）[14]进行数据分布学习以获得退化模型。然而，它们受限于训练数据集中的退化，并且无法很好地推广到分布之外的图像。读者可以参考最近的盲超分辨率综述[28]以获取更全面的分类。<br>&emsp;&emsp;在这项工作中，我们旨在通过使用更实用的退化过程合成训练对，将强大的ESRGAN [50]扩展到恢复一般真实世界的LR图像。真实的复杂退化通常来自于不同退化过程的复杂组合，例如相机成像系统、图像编辑和互联网传输。例如，当我们用手机拍照时，照片可能会有几种退化，比如相机模糊、传感器噪声、锐化伪影和JPEG压缩。然后我们进行一些编辑并上传到社交媒体应用程序，这会引入进一步的压缩和不可预测的噪声。当图像在互联网上被多次共享时，上述过程变得更加复杂。<br>&emsp;&emsp;这促使我们将经典的“一阶”退化模型扩展为 <strong>“高阶”退化建模</strong> 以适用于真实世界的退化，即使用多个重复的退化过程来建模，每个过程都是经典的退化模型。根据经验，我们采用二阶退化过程以在简单性和有效性之间取得良好平衡。最近的一项工作[55]还提出了一种随机洗牌策略来合成更实用的退化。然而，它仍然涉及固定数量的退化过程，不清楚所有洗牌的退化是否有用。相反，高阶退化建模更加灵活，试图模拟真实的退化生成过程。我们还在合成过程中加入了$sinc$滤波器以模拟<strong>常见的环绕和过冲伪影</strong>。<br>&emsp;&emsp;由于退化空间比ESRGAN大得多，训练也变得具有挑战性。具体而言，1）鉴别器需要更强大的能力来区分复杂的训练输出的真实性，同时需要更准确的梯度反馈来进行局部细节增强。因此，我们将ESRGAN中的VGG风格鉴别器改进为<strong>U-Net设计</strong>[41、52、39]。2）U-Net结构和复杂的退化也增加了训练的不稳定性。因此，我们采用<strong>谱归一化（SN）正则化</strong>[37、41]来稳定训练动态。通过这些改进，我们能够轻松训练Real-ESRGAN，并实现局部细节增强和伪影抑制的良好平衡。<br>&emsp;&emsp;总之，在这项工作中，1）我们提出了高阶退化过程来建模实际退化，并利用$sinc$滤波器来建模常见的环绕和过冲伪影。2）我们采用了几个必要的修改（例如带有谱归一化的U-Net鉴别器）来增加鉴别器的能力和稳定训练动态。3）使用纯合成数据训练的Real-ESRGAN能够恢复大多数真实世界的图像，并实现比以前的工作更好的视觉性能，使其在实际应用中更加实用。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>&emsp;&emsp;自从SRCNN [9、10]提出以来，图像超分辨率领域[21、24、45、17、25、27、58、22、44、57、7、30]已经见证了各种发展。为了实现视觉上令人满意的结果，通常使用生成对抗网络[15]作为损失监督，将解决方案推向自然流形[26、40、50、49]。大多数方法假定双三次下采样核，并且通常在真实图像中失败。最近的一些工作还将强化学习或GAN引入到图像恢复前的先验中[53、6、47]。<br>&emsp;&emsp;在盲超分辨率方面，已经有几项优秀的探索。第一类涉及显式的退化表示，通常包括两个组成部分：退化预测和条件恢复。上述两个组件可以分别[2、56]或联合（迭代）[16、34、46]执行。这些方法依赖于预定义的退化表示（例如退化类型和级别），通常考虑简单的合成退化。此外，不准确的退化估计必然会导致伪影。<br>&emsp;&emsp;另一类是尽可能接近真实数据获取&#x2F;生成训练对，然后训练一个统一的网络来解决盲超分辨率问题。训练对通常是1）通过特定相机捕获，随后进行繁琐的对齐[5、51]；2）或直接从未配对数据中学习，使用循环一致性损失[54、33]；3）或使用估计的模糊核和提取的噪声补丁进行合成[60、19]。然而，1）捕获的数据仅限于与特定相机相关的退化，因此可能不能很好地推广到其他真实图像；2）使用未配对数据学习细粒度退化是具有挑战性的，结果通常不尽人意。<br><strong>退化模型。</strong> 经典的退化模型[11、29]被广泛采用于盲超分辨率方法[56、16、34]中。然而，真实世界的退化通常过于复杂，难以明确建模。因此，隐式建模试图在网络中学习一个退化生成过程[54、12、46]。在这项工作中，我们提出了一个灵活的高阶退化模型来合成更实用的退化。</p><h2 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3. Methodology"></a>3. Methodology</h2><h3 id="3-1-Classical-Degradation-Model"><a href="#3-1-Classical-Degradation-Model" class="headerlink" title="3.1. Classical Degradation Model"></a>3.1. Classical Degradation Model</h3><p>&emsp;&emsp;盲超分辨率旨在从具有未知和复杂退化的低分辨率图像中恢复高分辨率图像。通常采用经典的退化模型[11、29]来合成低分辨率输入。一般地，首先将真实图像$y$与模糊核$k$卷积，然后进行尺度因子$r$的下采样操作，得到低分辨率图像$x$并添加噪声$n$，最后还采用$JPEG$压缩，因为它在真实世界的图像中被广泛使用。<br>$$\boldsymbol x&#x3D;\mathcal D(\boldsymbol y)&#x3D;[(\boldsymbol y\otimes\boldsymbol k)\downarrow_r+\boldsymbol n]<em>{\text{JPG}},\tag{1}$$<br>其中$\mathcal D$表示退化过程。接下来，我们简要回顾这些常用的退化方式。详细设置在第4.1节中指定。更多描述和示例在附录A中。<br><strong>模糊。</strong> 我们通常将模糊退化建模为与线性模糊滤波器（核）的卷积。各向同性和各向异性的高斯滤波器是常见的选择。对于具有$2t+1$个核大小的高斯模糊核$k$，它的$(i, j) ∈ [−t, t]$元素是从高斯分布中抽样的，形式上是：<br>$$\boldsymbol{k}(i,j)&#x3D;\frac{1}{N}\exp(-\frac{1}{2}\boldsymbol{C}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{C}),\quad\boldsymbol{C}&#x3D;[i,j]^T,\tag{2}$$<br>其中Σ是协方差矩阵；C是空间坐标；N是归一化常数。协方差矩阵可以进一步表示如下：<br>$$\boldsymbol\Sigma&#x3D;\boldsymbol R\begin{bmatrix}\sigma_1^2&amp;0\ 0&amp;\sigma_2^2\end{bmatrix}\boldsymbol R^T,\quad(\boldsymbol R \text{ is the rotation matrix})\tag{3}$$<br>$$&#x3D;\begin{bmatrix}cos\theta&amp;-sin\theta\ sin\theta&amp;cos\end{bmatrix}\begin{bmatrix}\sigma_1^2&amp;0\0&amp;\sigma_2^2\end{bmatrix}\begin{bmatrix}cos\theta&amp;sin\theta\-sin\theta&amp;cos\theta\end{bmatrix},\tag{4}$$<br>&emsp;&emsp;其中，$\sigma</em>{1}$和$\sigma_{2}$是两个主轴方向上的标准差（即协方差矩阵的特征值）；$\theta$是旋转角度。当$\sigma_{1}&#x3D;\sigma_{2}$时，$k$是各向同性的高斯模糊核；否则$k$是各向异性核。<br><strong>讨论。</strong> 尽管高斯模糊核被广泛用于建模模糊退化，但它们可能并不能很好地近似真实相机模糊。为了包括更多不同形状的核，我们进一步采用了广义高斯模糊核[31]和一个平台形分布。它们的概率密度函数（$pdf$）分别是$\frac{1}{N}\exp(-\frac{1}{2}(\boldsymbol{C}^{T}\boldsymbol{\Sigma}^{-1}\boldsymbol{C})^{\beta}$和$\frac{1}{N}\frac{1}{1+(\boldsymbol{C}^T\boldsymbol{\Sigma}^{-1}\boldsymbol{C})^{\beta}}$，其中$\beta$是形状参数。经验证实验发现，使用这些模糊核可以为几个真实样本产生更清晰的输出。<br><strong>噪声。</strong> &emsp;我们考虑了两种常用的噪声类型 - 1）加性高斯噪声和2）泊松噪声。加性高斯噪声的概率密度函数等于高斯分布的概率密度函数。噪声强度由高斯分布的标准差（即sigma值）控制。当RGB图像的每个通道都有独立的样本噪声时，合成的噪声是彩色噪声。我们还通过将相同的样本噪声应用于所有三个通道来合成灰色噪声[55、38]。<br>&emsp;&emsp;泊松噪声遵循泊松分布。它通常用于近似模拟由统计量子波动引起的传感器噪声，即在给定曝光水平下感应到的光子数的变化。泊松噪声强度与图像强度成正比，不同像素的噪声是独立的。<br><strong>调整大小（下采样）。</strong> &emsp;下采样是SR中合成低分辨率图像的基本操作。更一般地，我们考虑下采样和上采样，即调整大小操作。有几种大小调整算法-最近邻插值、区域调整、双线性插值和双立方插值。不同的调整大小操作带来不同的影响-一些会产生模糊的结果，而一些则可能输出带有过冲伪影的过于锐利的图像。<br>&emsp;&emsp;为了包括更多多样化和复杂的调整大小效果，我们考虑从上述选择中进行随机调整大小操作。由于最近邻插值会引入对齐问题，因此我们排除它，只考虑区域、双线性和双立方操作。<br><strong>JPEG压缩。</strong>&emsp;JPEG压缩是数字图像常用的有损压缩技术。它首先将图像转换为YCbCr颜色空间，并对色度通道进行下采样。然后将图像分成8×8块，每个块都进行二维离散余弦变换（DCT）转换，然后对DCT系数进行量化。有关JPEG压缩算法的更多细节可以在[43]中找到。JPEG压缩通常会引入令人不满的块状伪影。<br>压缩图像的质量由质量因子$q\in[0,100]$确定，其中较低的$q$表示更高的压缩比和更差的质量。我们使用PyTorch实现的DiffJPEG[32]。</p><h3 id="3-2-High-order-Degradation-Model"><a href="#3-2-High-order-Degradation-Model" class="headerlink" title="3.2. High-order Degradation Model"></a>3.2. High-order Degradation Model</h3><p>&emsp;&emsp;当我们采用上述经典退化模型来合成训练对时，训练出的模型确实可以处理一些真实样本。然而，它仍然无法解决现实世界中的一些复杂退化问题，特别是未知噪声和复杂伪影（见图3）。这是因为合成的低分辨率图像仍然与真实的退化图像存在很大差距。因此，我们将经典的退化模型扩展为高阶退化过程，以模拟更实际的退化。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620111042.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620111050.png" alt="Img"><br>&emsp;&emsp;经典的退化模型只包括一些基本的退化过程，可以视为一阶建模。然而，现实生活中的退化过程非常多样化，通常包括一系列程序，包括相机成像系统、图像编辑、互联网传输等。例如，当我们想要恢复从互联网下载的低质量图像时，其底层退化涉及到不同退化过程的复杂组合。具体而言，原始图像可能是多年前用手机拍摄的，必然包含着相机模糊、传感器噪声、低分辨率和JPEG压缩等退化。然后，图像被编辑了，进行了锐化和调整大小操作，引入了过冲和模糊伪影。之后，它被上传到某些社交媒体应用程序中，这引入了进一步的压缩和不可预测的噪声。由于数字传输也会带来伪影，当图像在互联网上传播多次时，这个过程变得更加复杂。<br>&emsp;&emsp;这样一个复杂的退化过程无法用经典的一阶模型来建模。因此，我们提出了一个高阶退化模型。一个$n$阶模型包含$n$个重复的退化过程（如公式5所示），其中每个退化过程采用相同的过程但不同的超参数的经典退化模型（公式1）。需要注意的是，这里的“高阶”与数学函数中使用的不同。它主要指的是相同操作的实现次数。[55]中的随机洗牌策略也可能包括重复的退化过程（例如双重模糊或JPEG），但我们强调高阶退化过程是关键，表明并非所有洗牌的退化都是必要的。为了保持图像分辨率在合理范围内，公式1中的下采样操作被替换为随机调整大小操作。根据经验，我们采用二阶退化过程，因为它可以解决大多数真实情况并保持简单性。图2描述了我们纯合成数据生成流程的整体流程。<br>$$\boldsymbol{x}&#x3D;\mathcal{D}^{n}(\boldsymbol{y})&#x3D;(\mathcal{D}<em>{n}\circ\cdots\circ\mathcal{D}</em>{2}\circ\mathcal{D}_{1})(\boldsymbol{y}).\tag{5}$$<br>值得注意的是，改进后的高阶退化过程并不完美，无法覆盖现实世界中的所有退化空间。相反，它仅通过修改数据合成过程扩展了以前盲目SR方法的可解退化边界。在图11中可以找到几个典型的限制场景。</p><h3 id="3-3-Ringing-and-overshoot-artifacts"><a href="#3-3-Ringing-and-overshoot-artifacts" class="headerlink" title="3.3. Ringing and overshoot artifacts"></a>3.3. Ringing and overshoot artifacts</h3><p>&emsp;&emsp;环绕伪影通常出现在图像中的锐利转换附近，看起来像边缘附近的条纹或“幽灵”。过冲伪影通常与环绕伪影结合在一起，表现为边缘转换处的跳跃增加。这些伪影的主要原因是信号没有高频带宽限制。这些伪影非常普遍，通常由锐化算法、JPEG压缩等产生。图5（顶部）显示了一些遭受环绕和过冲伪影的真实样本。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620112335.png" alt="Img"><br>&emsp;&emsp;我们使用$sinc$滤波器来合成训练对的环绕和过冲伪影。$sinc$滤波器是一种理想化的滤波器，能够截断高频信号。$sinc$滤波器的核可以表示为：<br>$\boldsymbol{k}(i,j)&#x3D;\frac{\omega_c}{2\pi\sqrt{i^2+j^2}}J_1(\omega_c\sqrt{i^2+j^2}),$其中$（i，j）$是核心坐标； $ω_c$是截止频率；$J_1$是第一类贝塞尔函数。图5（底部）显示了具有不同截止频率的$sinc$滤波器及其相应的滤波图像。观察到它能够很好地合成环绕和过冲伪影（尤其是由过度锐化效果引入的）。这些伪影在视觉上类似于图5（顶部）中前两个真实样本中的伪影。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620112349.png" alt="Img"><br>&emsp;&emsp;我们在两个地方采用$sinc$滤波器：模糊过程和合成的最后一步。最后一个$sinc$滤波器和JPEG压缩的顺序会随机交换，以涵盖更大的退化空间，因为有些图像可能首先进行过度锐化（产生过冲伪影），然后再进行JPEG压缩，而有些图像可能首先进行JPEG压缩，然后再进行锐化操作。</p><h3 id="3-4-Networks-and-Training"><a href="#3-4-Networks-and-Training" class="headerlink" title="3.4. Networks and Training"></a>3.4. Networks and Training</h3><p><strong>ESRGAN生成器。</strong> 我们采用与ESRGAN [50]相同的生成器（SR网络），即具有多个残差内部稠密块（RRDB）的深度网络，如图4所示。我们还将原始的×4 ESRGAN架构扩展到具有×2和×1的放大倍数。由于ESRGAN是一个复杂的网络，因此我们首先采用像素解组（像素重排的逆操作[42]）来减小空间大小并增大通道大小，然后将输入送入主要的ESRGAN架构。因此，大部分计算是在较小的分辨率空间中执行的，这可以减少GPU内存和计算资源的消耗。<br><strong>使用谱标准化（SN）的U-Net鉴别器。</strong> 由于Real-ESRGAN的目标是解决比ESRGAN更大的退化空间，因此ESRGAN中鉴别器的原始设计不再适用。具体而言，Real-ESRGAN中的鉴别器需要更强的判别能力来处理复杂的训练输出。它不仅需要区分全局风格，还需要为局部纹理提供准确的梯度反馈。受[41, 52]的启发，我们将ESRGAN中的VGG-style鉴别器改进为具有跳过连接的U-Net设计（如图6所示）。U-Net为每个像素输出真实度值，并可以为生成器提供详细的逐像素反馈。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230719093042.png" alt="Img"><br>&emsp;&emsp;与此同时，U-Net结构和复杂的退化过程也增加了训练不稳定性。我们采用谱标准化正则化[37]来稳定训练动态。此外，我们观察到谱标准化也有助于缓解GAN训练引入的过度锐化和令人烦恼的伪影。通过这些调整，我们能够轻松地训练Real-ESRGAN，并实现局部细节增强和伪影抑制的良好平衡。<br><strong>训练过程</strong> 分为两个阶段。首先，我们使用L1损失训练一个以PSNR为导向的模型。所得到的模型称为Real-ESRNet。然后，我们使用训练好的PSNR-oriented模型作为生成器的初始化，并使用L1损失、感知损失[20]和GAN损失[14, 26, 4]的组合来训练Real-ESRGAN。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Datasets-and-Implementation"><a href="#4-1-Datasets-and-Implementation" class="headerlink" title="4.1. Datasets and Implementation"></a>4.1. Datasets and Implementation</h3><p><strong>训练细节。</strong> &emsp;与ESRGAN类似，我们采用DIV2K [1]、Flickr2K [45]和OutdoorSceneTraining [49]数据集进行训练。训练HR块大小设置为256。我们使用四个NVIDIA V100 GPU进行训练，总批量大小为48。我们采用Adam优化器[23]。Real-ESRNet从ESRGAN进行微调以实现更快的收敛。我们使用学习率2 × 10−4对Real-ESRNet进行1000K次迭代训练，同时使用学习率1 × 10−4对Real-ESRGAN进行400K次迭代训练。我们采用指数移动平均（EMA）进行更稳定的训练和更好的性能。Real-ESRGAN使用L1损失、感知损失和GAN损失的组合进行训练，分别采用权重{1，1，0.1}。我们使用预训练的VGG19网络[20]中的{conv1，…conv5}特征映射（带权重{0.1，0.1，1，1，1}）在激活之前作为感知损失。我们的实现基于BasicSR [48]。<br><strong>退化细节。</strong> &emsp;我们采用二阶退化模型，以取得简单和有效的平衡。除非另有说明，否则两个退化过程具有相同的设置。我们采用高斯核、广义高斯核和平台形状核，概率分别为{0.7, 0.15, 0.15}。模糊核大小从{7、9、…21}中随机选择。模糊标准差σ从[0.2, 3]中采样（对于第二个退化过程为[0.2, 1.5]）。形状参数β对于广义高斯核和平台形状核分别从[0.5, 4]和[1, 2]中采样。我们还使用sinc核，概率为0.1。我们以0.2的概率跳过第二个模糊退化过程。<br>&emsp;&emsp;我们采用高斯噪声和泊松噪声，概率为{0.5，0.5}。噪声sigma范围和泊松噪声比例分别设置为[1，30]和[0.05，3]（第二个退化过程为[1，25]和[0.05，2.5]）。灰色噪声概率设置为0.4。JPEG压缩质量因子设置为[30，95]。最终的sinc滤波器的应用概率为0.8。更多细节可以在发布的代码中找到。<br><strong>训练对池。</strong> 为了提高训练效率，所有退化过程都在PyTorch中实现，并使用CUDA加速，因此我们能够动态地合成训练对。然而，批处理限制了批次中合成退化的多样性。例如，批次中的样本不能具有不同的调整比例因子。因此，我们采用训练对池来增加批次中的退化多样性。在每次迭代中，训练样本随机从训练对池中选择，形成一个训练批次。我们在实现中将池大小设置为180。<br><strong>在训练过程中锐化真实图像。</strong> 我们进一步展示了一个训练技巧，可以在不引入可见伪影的情况下，视觉上提高图像的清晰度。一种典型的锐化图像的方法是使用后处理算法，例如非锐化掩蔽（USM）。然而，这种算法往往会引入过冲伪影。我们经验性地发现，在训练过程中对真实图像进行锐化可以实现更好的清晰度和过冲伪影抑制的平衡。我们将使用锐化的真实图像训练的模型称为Real-ESRGAN+（比较结果显示在图7中）。</p><h3 id="4-2-Comparisons-with-Prior-Works"><a href="#4-2-Comparisons-with-Prior-Works" class="headerlink" title="4.2. Comparisons with Prior Works"></a>4.2. Comparisons with Prior Works</h3><p>&emsp;&emsp;我们将我们的Real-ESRGAN与几种最先进的方法进行了比较，包括ESRGAN [50]、DAN [34]、CDC [51]、RealSR [19]和BSRGAN [55]。我们在几个不同的测试数据集上进行测试，包括RealSR [5]、DRealSR [51]、OST300 [49]、DPED [18]、ADE20K验证集 [59]和来自互联网的图像。由于现有的感知质量度量无法很好地反映人类在细粒度尺度上的实际感知偏好 [3]，因此我们在图7中呈现了几个代表性的视觉样本。量化结果也包含在附录B中供参考。<br>&emsp;&emsp;从图7中可以看出，我们的Real-ESRGAN在去除伪影和恢复纹理细节方面优于以前的方法。使用锐化的真实图像训练的Real-ESRGAN+可以进一步提高视觉清晰度。具体而言，第一个样本包含过冲伪影（字母周围的白色边缘）。直接上采样将不可避免地放大这些伪影（例如，DAN和BSRGAN）。Real-ESRGAN考虑了这些常见的伪影，并使用sinc滤波器模拟它们，从而有效地去除了环绕和过冲伪影。第二个样本包含未知和复杂的退化。大多数算法无法有效消除它们，而使用二阶退化过程进行训练的Real-ESRGAN可以。Real-ESRGAN还能够为实际样本恢复更真实的纹理（例如，砖、山和树纹理），而其他方法要么无法去除退化要么添加不自然的纹理（例如，RealSR和BSRGAN）。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620114125.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620114135.png" alt="Img"></p><h3 id="4-3-Ablation-Studies"><a href="#4-3-Ablation-Studies" class="headerlink" title="4.3. Ablation Studies"></a>4.3. Ablation Studies</h3><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230724093938.png" alt="Img"><br><strong>二阶退化模型。</strong> 我们对Real-ESRNet进行了去除退化的消融研究，因为它更可控，可以更好地反映退化的影响。我们用经典的退化模型替换Real-ESRNet中的二阶过程来生成训练对。如图8（顶部）所示，使用经典的一阶退化模型训练的模型无法有效去除墙上的噪声或麦田中的模糊，而Real-ESRNet可以处理这些情况。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230724094005.png" alt="Img"><br><strong>sinc滤波器。</strong> 如果在训练期间不使用sinc滤波器，恢复的结果将放大输入图像中存在的环绕和过冲伪影，如图8（底部）所示，特别是在文本和线条周围。相反，使用sinc滤波器训练的模型可以去除这些伪影。<br><strong>带SN正则化的U-Net判别器。</strong> 我们首先采用ESRGAN设置，包括VGG风格的判别器及其损失权重。然而，从图9可以看出，这个模型无法恢复详细的纹理（如砖头和灌木），甚至在灌木枝干上产生不愉快的伪影。使用U-Net设计可以改善局部细节，但会引入不自然的纹理，并增加训练不稳定性。SN正则化可以提高恢复的纹理并稳定训练动态。<br><strong>更复杂的模糊核。</strong> 我们在模糊合成中删除了广义高斯核和平台形状核。如图10所示，在一些真实样本上，该模型无法去除模糊并恢复锐利的边缘，而Real-ESRGAN可以。然而，在大多数样本上，它们的差异很小，表明广泛使用的高阶退化过程的高斯核已经可以覆盖大部分真实模糊空间。由于我们仍然可以观察到略微更好的性能，因此在Real-ESRGAN中采用这些更复杂的模糊核。</p><h3 id="4-4-Limitations"><a href="#4-4-Limitations" class="headerlink" title="4.4. Limitations"></a>4.4. Limitations</h3><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230724094035.png" alt="Img"><br>&emsp;&emsp;尽管Real-ESRGAN能够恢复大多数真实世界图像，但仍存在一些限制。如图11所示，1）一些恢复的图像（尤其是建筑和室内场景）由于混叠问题产生了扭曲的线条。2）GAN训练在一些样本上引入了不愉快的伪影。3）它无法去除真实世界中超出分布的复杂退化。更糟糕的是，它可能会放大这些伪影。这些缺点对Real-ESRGAN的实际应用产生了很大的影响，需要在未来的研究中解决。</p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>&emsp;&emsp;本文中，我们使用纯合成训练对来训练实用的Real-ESRGAN，用于真实世界的盲超分辨率问题。为了合成更实用的退化，我们提出了高阶退化过程，并采用sinc滤波器模拟常见的环绕和过冲伪影。我们还利用带有谱归一化正则化的U-Net判别器，以增加判别器的能力并稳定训练动态。使用合成数据训练的Real-ESRGAN能够为大多数真实世界图像增强细节，同时消除恼人的伪影。<br><strong>鸣谢。</strong> 本工作部分得到中国国家自然科学基金（61906184）、中国上海市科委（编号21DZ1100800和21DZ1100100）的支持。</p><h2 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h2><h3 id="A-Details-of-Classical-Degradation-Model"><a href="#A-Details-of-Classical-Degradation-Model" class="headerlink" title="A. Details of Classical Degradation Model"></a>A. Details of Classical Degradation Model</h3><p>&emsp;&emsp;在本节中，我们提供了经典退化模型中使用的每种退化类型的更多细节（尤其是示例）。</p><h4 id="A-1-Blur"><a href="#A-1-Blur" class="headerlink" title="A.1. Blur"></a>A.1. Blur</h4><p>&emsp;&emsp;各向同性和各向异性高斯滤波器是模糊核的常见选择。我们在图12中展示了几个高斯核及其相应的模糊图像。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620115610.png" alt="Img"><br>&emsp;&emsp;为了包括更多样化的核形状，我们进一步采用广义高斯模糊核[31]和一个平台形状分布。图13展示了形状参数β如何控制核形状。经验证明，包括这些模糊核可以为几个真实样本产生更清晰的输出。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620115651.png" alt="Img"></p><h4 id="A-2-Noise"><a href="#A-2-Noise" class="headerlink" title="A.2. Noise"></a>A.2. Noise</h4><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620115719.png" alt="Img"><br>&emsp;&emsp;图14展示了加性高斯噪声和泊松噪声。泊松噪声的强度与图像强度成正比，并且不同像素的噪声独立于彼此。如图14所示，泊松噪声在暗区域具有较低的噪声强度。</p><h4 id="A-3-Resize"><a href="#A-3-Resize" class="headerlink" title="A.3. Resize"></a>A.3. Resize</h4><p>&emsp;&emsp;存在多种调整大小算法。我们比较以下调整大小操作：最近邻插值、区域调整大小、双线性插值和双三次插值。我们研究这些调整大小操作的不同效果。我们首先将一张图像按四倍比例缩小，然后将其放大到原始大小。执行不同的降采样和上采样算法，不同组合的结果如图15所示。观察到不同的调整大小操作会产生非常不同的效果-有些会产生模糊的结果，而有些则可能会输出具有过冲伪影的过度锐利的图像。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620115757.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620115811.png" alt="Img"></p><h4 id="A-4-JPEG-compression"><a href="#A-4-JPEG-compression" class="headerlink" title="A.4. JPEG compression"></a>A.4. JPEG compression</h4><p>&emsp;&emsp;我们使用PyTorch实现的DiffJPEG。我们观察到，DiffJPEG压缩的图像与cv2包压缩的图像略有不同。图16展示了典型的JPEG压缩伪影以及使用不同包时引起的差异。这种差异可能会产生合成和真实样本之间的额外差距。在本工作中，为了简单起见，我们仅采用DiffJPEG，并将在后续中解决这种差异问题。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230620115856.png" alt="Img"></p><h3 id="B-Quantitative-Comparisons"><a href="#B-Quantitative-Comparisons" class="headerlink" title="B. Quantitative Comparisons"></a>B. Quantitative Comparisons</h3><p>&emsp;&emsp;我们提供非参考图像质量评估- NIQE [36] 作为参考。需要注意的是，现有的感知质量度量无法很好地反映人类对细粒度尺度上的感知偏好[3]。<br>&emsp;&emsp;我们将我们的Real-ESRGAN与几种最先进的方法进行了比较，包括ESRGAN [50]、DAN [34]、CDC [51]、RealSR [19]和BSRGAN [55]。我们在几个不同的测试数据集上进行测试，包括RealSR [5]、DRealSR [51]、OST300 [49]、DPED [18]、ImageNet验证[8]和ADE20K验证[59]。结果如表1所示。尽管我们的Real-ESRGAN+没有为NIQE得分进行优化，但它在大多数测试数据集上仍然产生较低的NIQE得分。</p><h3 id="C-More-Qualitative-Comparisons"><a href="#C-More-Qualitative-Comparisons" class="headerlink" title="C. More Qualitative Comparisons"></a>C. More Qualitative Comparisons</h3><p>&emsp;&emsp;我们展示了与之前的工作的更多定性比较。如图17所示，我们的Real-ESRGAN在消除伪影和恢复纹理细节方面优于以前的方法。使用经过锐化的地面实况进行训练的Real-ESRGAN+可以进一步提高视觉清晰度。其他方法通常无法消除复杂的伪影（第一组示例）和过冲伪影（第二、三组示例），或无法为各种场景恢复逼真和自然的纹理（第四、五组示例）。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Real-ESRGAN.md/img-20230724100831.png" alt="Img"></p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>KMSR</title>
      <link href="/2023/08/14/KMSR/"/>
      <url>/2023/08/14/KMSR/</url>
      
        <content type="html"><![CDATA[<h1 id="KMSR"><a href="#KMSR" class="headerlink" title="KMSR"></a>KMSR</h1><h1 id="Kernel-Modeling-Super-Resolution-on-Real-Low-Resolution-Images"><a href="#Kernel-Modeling-Super-Resolution-on-Real-Low-Resolution-Images" class="headerlink" title="Kernel Modeling Super-Resolution on Real Low-Resolution Images"></a>Kernel Modeling Super-Resolution on Real Low-Resolution Images</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;深度卷积神经网络（CNN）通过对应一对高分辨率和低分辨率图像的训练，在单幅图像超分辨率方面达到了最先进的性能，并超过了先前的基于信号处理的方法。然而，当应用于真实照片时，它们的性能受到限制。原因在于它们的训练数据：低分辨率（LR）图像是通过对应的高分辨率（HR）图像的双三次插值得到的。应用的卷积核与真实世界的相机模糊明显不同。因此，虽然当前的CNN能够很好地超分辨率双三次下采样的LR图像，但它们通常在相机捕获的LR图像上失败。<br>&emsp;&emsp;为了改善深度超分辨率CNN在真实照片上的泛化能力和鲁棒性，我们提出了一个核建模超分辨率网络（KMSR），它在训练中加入了模糊核建模。我们提出的KMSR由两个阶段组成：我们首先使用生成对抗网络（GAN）构建一个真实模糊核池，然后使用生成的核构建HR和相应的LR图像，训练超分辨率网络。我们广泛的实验验证证明了我们的单幅图像超分辨率方法在具有未知模糊核的照片上的有效性。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>&emsp;&emsp;单幅图像超分辨率方法旨在通过恢复高频细节，从单个低分辨率（LR）图像重建出高分辨率（HR）图像。经典的超分辨率（SR）算法[40, 41, 57]通过分析模糊核和真实图像属性来恢复HR图像。相比之下，许多现代SR方法[21, 45, 49]尝试学习从LR图像到HR图像的映射。最近，出现了几种基于卷积神经网络（CNN）的SR模型[8, 17, 26, 31, 42, 55]。所有这些基于学习的方法都需要大量配对的LR和HR图像进行训练。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608093619.png" alt="Img"><br>&emsp;&emsp;获取这样的真实场景配对LR和HR参考图像是非常困难的。因此，当前基于CNN的SR网络依赖于合成的LR图像[44]。最常见的技术是对HR图像应用双三次插值[25]。然而，双三次卷积核与真实相机模糊[32]不同。相机捕获图像中高频细节的损失是由于多种因素引起的，例如光学模糊、大气模糊、相机抖动和镜头畸变[34]。因此，尽管这些基于CNN的SR网络在双三次下采样的LR图像上表现良好，但它们在真实照片上的性能受到限制，因为它们是在错误的卷积核假设下运行的[11, 32]。基于生成对抗网络（GAN）的方法[3, 30, 39, 47]可以扩展到在非配对数据集上训练SR网络，但它们仍然依赖于不真实的模糊核。因此，在具有未知相机模糊的实际LR照片上进行超分辨率仍然是一个具有挑战性的问题。<br>&emsp;&emsp;为了使用真实相机模糊生成合成LR图像，我们可以使用核估计算法[28, 29, 35]从真实LR照片中提取真实的模糊核。然而，由于每个相机、镜头、光圈和大气条件组合可能会产生不同的模糊核，因此生成足够大且多样化的数据集[28, 29]以训练SR网络是具有挑战性的。<br>&emsp;&emsp;一种方法是使用许多模糊核[36]生成合成LR图像，这将提高SR网络的泛化能力。使用核估计器，我们首先从真实照片中提取模糊核，并将它们用于训练GAN。GAN最初是在[12]中提出的一类神经网络，它们可以学习生成与给定训练数据相同分布的合成样本[2]。因此，我们利用GAN近似复杂分布的能力[24, 30, 38, 50]，通过学习和生成额外的模糊核，扩充我们通过核估计获得的有限核集。<br>&emsp;&emsp;我们的核建模超分辨率（KMSR）因此包含两个阶段，如图1所示。我们首先通过使用核估计算法从照片中提取真实的模糊核，并通过训练GAN来扩充核池，生成一个GAN增强的真实模糊核池。然后，我们使用从核池中采样的核构建配对的LR-HR训练数据集，并训练一个深度CNN进行SR。<br>&emsp;&emsp;本文的主要贡献如下：<br>（1）我们引入了KMSR，通过在框架中加入真实的模糊核来改善针对真实照片的盲目SR，提高网络对未知模糊核的泛化能力；<br>（2）我们展示了GAN可以可靠地生成真实的模糊核；<br>（3）我们通过实验在真实图像上证明，所提出的KMSR在视觉质量和客观指标方面都实现了最先进的结果。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><h3 id="2-1-CNN-based-Image-Super-Resolution"><a href="#2-1-CNN-based-Image-Super-Resolution" class="headerlink" title="2.1. CNN-based Image Super-Resolution"></a>2.1. CNN-based Image Super-Resolution</h3><p>&emsp;&emsp;针对超分辨率的深度网络架构是一个活跃的研究课题，因为它们在合成的LR图像上表现良好[44]。Dong等人[8]采用3层CNN学习从插值LR图像到HR图像的端到端映射。他们取得了与传统SR方法相当的结果。全局[26]和局部[17, 31]残差学习策略可以用于降低学习难度和简化训练，从而优化SR网络的性能。Shi等人[42]建议使用子像素上采样进一步增加网络的接受域；这提供更多的上下文信息，有助于生成更准确的细节。<br>&emsp;&emsp;所有这些网络都是使用配对的LR-HR数据进行训练，并经常使用固定的下采样程序生成合成的LR图像。这导致网络在真实照片上的泛化能力较差，因为实际的图像采集不符合学习的模型。一些方法提出使用不同的光学变焦来捕获真实的LR-HR图像对[5, 54]，但在这些数据集上训练的网络仅限于特定的相机型号。最近的方法确实提出将退化参数包括模糊核纳入网络[14, 43, 51, 52, 53]。但是，这些方法仅依赖于模糊核估计算法，因此处理任意模糊核的能力有限。本文通过在创建训练数据集时对现实核进行建模来解决问题，从而提高了SR网络的实用性和泛化能力。</p><h3 id="2-2-Blur-Kernel-Estimation"><a href="#2-2-Blur-Kernel-Estimation" class="headerlink" title="2.2. Blur-Kernel Estimation"></a>2.2. Blur-Kernel Estimation</h3><p>&emsp;&emsp;近年来，我们目睹了单图像去模糊以及模糊核估计方面的重大进展。基于最大后验概率（MAP）公式的高效方法已经开发出不同的似然函数和图像先验[4]。特别是，在MAP估计框架中提出了用于核估计的启发式边缘选择方法[7]。为了更好地恢复模糊核并更好地重建图像去模糊的清晰边缘，一些基于示例的方法[16]利用来自外部数据集的模糊输入和示例图像中包含的信息。最近，Pan等人[35]使用了暗通道先验[19]来简单高效地估计自然图像的模糊核。由于它们在去模糊任务中取得了显著的性能[29]，我们采用了他们的核估计算法来收集真实图像的模糊核。</p><h3 id="2-3-Generative-Adversarial-Network"><a href="#2-3-Generative-Adversarial-Network" class="headerlink" title="2.3. Generative Adversarial Network"></a>2.3. Generative Adversarial Network</h3><p>&emsp;&emsp;GAN被提出用于近似难以处理的概率计算[24, 30, 38, 50, 59]，并在一些SR网络中用于提高视觉质量[30, 39, 47]。然而，训练GAN可能会很棘手和不稳定，并且很难生成不带伪影的HR图像[24, 38, 50]。DCGAN [37] 提供了一些构建和训练GAN的有用指导。WGAN [1, 15]通过克服在生成网络、判别模型和网络架构设计之间维持训练平衡的困难来进一步改进GAN训练。多个应用程序还展示了它们增强有限的深度学习训练数据的能力[2, 6]。因此，我们采用WGAN-GP [15]，WGAN的改进版本，生成大量的核，然后利用这些核生成逼真的LR图像，用于训练我们的KMSR网络。</p><h2 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3. Proposed Method"></a>3. Proposed Method</h2><p>&emsp;&emsp;本节介绍我们针对真实照片的核建模超分辨率解决方案：KMSR。它由核池创建阶段和CNN类型的SR网络组成（见图1）。我们首先介绍了我们获得LR和HR图像的成像模型。然后，我们讨论了核池生成和SR网络架构的详细信息。</p><h3 id="3-1-Kernel-Modeling-Blind-Super-Resolution"><a href="#3-1-Kernel-Modeling-Blind-Super-Resolution" class="headerlink" title="3.1. Kernel Modeling Blind Super-Resolution"></a>3.1. Kernel Modeling Blind Super-Resolution</h3><p>&emsp;&emsp;设$y$为大小为$r_1×r_2$像素的<strong>HR</strong>图像，$x$为$y$的<strong>LR</strong>观测，大小为$⌊r_1&#x2F;s⌋×⌊r_2&#x2F;s⌋$，其中$s&gt;1$为下采样因子。$x$和$y$之间的关系可以表示为[11]:<br>$$x&#x3D;(y<em>k)\downarrow^s+n \tag{1}$$<br>其中$k$表示未知的模糊核，$↓^s$表示一个因子为$s$的降采样算子，$n$为噪声。在这里，我们假设LR图像采集模型中没有噪声，即$n&#x3D;0$。<br>&emsp;&emsp;我们使用传统的双三次插值将LR图像按照相同的因子$s$上采样到所需的大小$r_1×r_2$，得到一个粗略的HR图像$x’$：<br>$$x’ &#x3D; (x</em>b_s) \tag{2}$$<br>其中$b_s$是尺度为$s$的双三次上采样核。因此，我们有：<br>$$x’&#x3D;((y<em>k)\downarrow^s)<em>b_s \tag{3}$$<br>简单的，<br>$$x’&#x3D;y</em>k’\tag{4}$$<br>当$k’&#x3D;(k</em>b_s)\downarrow^s$</p><h3 id="3-2-模糊核池"><a href="#3-2-模糊核池" class="headerlink" title="3.2 模糊核池"></a>3.2 模糊核池</h3><p>&emsp;&emsp;在构建配对训练数据集之前，需要从真实照片中估计逼真的模糊核。然后，这些核被用于更好地训练用于核建模和核生成的GAN。估计得到的核和GAN生成的核的组合形成了用于构建配对LR-HR训练数据的大型核池。</p><h4 id="3-2-1-Blur-Kernel-Estimation"><a href="#3-2-1-Blur-Kernel-Estimation" class="headerlink" title="3.2.1 Blur-Kernel Estimation"></a>3.2.1 Blur-Kernel Estimation</h4><p>&emsp;&emsp;为了生成一组逼真的模糊核$K’&#x3D;{k’_1，k’_2，…，k’_e}$，我们首先从双三次上采样的LR图像（或粗糙的HR图像）$x’$中随机提取一个大小为$d×d$的补丁$p$。然后我们使用[35]中的模糊核估计算法从$p$中估计出大小为25×25的模糊核$k’$。他们基于暗通道先验[19]的图像去模糊的标准公式如下：<br>$$\underset{p,k’}{min}|\nabla p*k’-\nabla p|+\theta|k’|_2^2+\mu|\nabla p|_0+|\nabla p^{dark}|_0 \tag{5}$$<br>&emsp;&emsp;其中$p$是从$x’$中提取的补丁，$p^{dark}$是补丁的暗通道[19]。使用坐标下降法交替求解潜在补丁$p$和模糊核$k’$。详细信息可以在[35]中找到。为了消除缺乏高频细节（例如从天空、墙壁等提取的补丁）的补丁，模糊核估计算法可能失败，我们定义了以下对$p$的约束：<br>$$|Mean(p)-Var(p)|\geq\alpha\cdot Mean(p) \tag{6}$$<br>&emsp;&emsp;其中$Mean(p)$和$Var(p)$分别计算平均强度和方差，$α∈（0,1）$。如果满足约束条件，则$p$将被视为有效的补丁，并将从$p$估计的模糊核$k’$添加到集合$K’$中。<br>&emsp;&emsp;我们从每个双三次上采样的LR图像x’中提取5个补丁。我们将补丁大小设置为$d &#x3D; 512$，$α &#x3D; 0.003$。</p><h4 id="3-2-2-Kernel-Modeling-with-GAN"><a href="#3-2-2-Kernel-Modeling-with-GAN" class="headerlink" title="3.2.2 Kernel Modeling with GAN"></a>3.2.2 Kernel Modeling with GAN</h4><p>&emsp;&emsp;在实践中，输入的LR图像可能很难获取，并且可能仅限于少数相机型号。此外，模糊核估计算法[35]计算成本高昂。因此，上一小节中收集的核的数量和多样性可能受到限制，仅使用这些核训练深度CNN的结果将不足以满足需求。因此，我们建议对估计的核集$K’$建模模糊核分布，并生成包含更多逼真且多样化的模糊核实例的更大的模糊核池$K^+$。我们使用GAN生成这样的逼真模糊核。<br>&emsp;&emsp;我们使用WGAN-GP [15]作为我们GAN的目标函数，它是WGAN [1]的改进版本：<br>$$L&#x3D;\underset{\tilde{f}\sim\mathbb{P}<em>g}{\mathbb{E}}[D(\tilde{f})]-\underset{f\sim\mathbb{P}<em>r}{\mathbb{E}}[D(f)]]+\lambda\underset{\hat{f}\sim\mathbb{P}<em>f}{\mathbb{E}}[(\Bigl\lVert\nabla D(\hat{f})\Bigr\rVert_2-1)^2] \tag{7}$$<br>&emsp;&emsp;其中，$D$是判别网络，$\mathbb{P}<em>r$是在$K’$上的分布，$\mathbb{P}</em>{g}$是生成器分布。$\mathbb{P}</em>{g}$被定义为在从$\mathbb{P}<em>r$和$\mathbb{P}</em>{g}$中采样的点对之间均匀采样的直线上采样的分布。$f$，$\tilde{f}$，$\hat{f}$分别是遵循分布$\mathbb{P}</em>{r}$，$\mathbb{P}</em>{g}$和$\mathbb{P}_{\hat{f}}$的随机样本。有关更多详细信息，请参见[15]。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608105038.png" alt="Img"><br>&emsp;&emsp;我们采用类似于DCGAN [37]的网络架构。生成网络G从长度为100的服从N(0,1)分布的向量z生成一个模糊核样本。它包含4个滤波器大小为4×4的分数步幅卷积层[10]，带有批归一化[23]、ReLU激活函数[33]以及一个滤波器大小为8×8的最终卷积层。G的滤波器数量从第二个单元到倒数第二个单元分别为1025、512、256、1。判别网络D以模糊核样本为输入，识别它是否为伪造的，它包含3个卷积层，带有实例批归一化[46]和LeakyReLU激活函数[48]。D的滤波器数量从第一个单元到第三个单元分别为256、512、1024。<br>&emsp;&emsp;训练好的GAN模型$G$用于生成用于增广$K’$的模糊核样本，直到最终的核池$K+ &#x3D; K’ ∪ {G(z1), G(z2), G(z3), …}$被获得。与[35]中的核归一化类似，我们对生成的核应用总和为一和非负约束。</p><h3 id="3-3-Super-Resolution-with-CNN"><a href="#3-3-Super-Resolution-with-CNN" class="headerlink" title="3.3. Super-Resolution with CNN"></a>3.3. Super-Resolution with CNN</h3><p>&emsp;&emsp;之前的方法[8、17、31]提出通过训练大型数据集来解决SR问题，这些方法在合成数据上取得了令人印象深刻的结果。深度神经网络隐式地从配对训练数据集中学习潜在模型，因此不需要显式了解图像先验知识。因此，在我们的SR框架中，我们利用了CNN。<br>&emsp;&emsp;我们按以下方式创建训练数据集：将HR图像分成大小为$m×m$的小块，形成集合$Y &#x3D; {y_1，y_2，…，y_t}$。在第3.2.1节中获得的$K+$中随机选择模糊核与$Y$中的补丁卷积，以获得$X′ &#x3D; {x′_1，x′_2，…，x′_t}$，其中$x′_j &#x3D; y_j∗k’_l$。$X′$和$Y$集合形成了配对训练数据集${X′，Y}$。<br>&emsp;&emsp;CNN的网络结构如图2所示，包含16个残差块[20]。采用零填充以确保一致的输入和输出维度。我们的网络的目标函数是L1，使网络能够获得更好的性能[56]。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Implementation-Details"><a href="#4-1-Implementation-Details" class="headerlink" title="4.1. Implementation Details"></a>4.1. Implementation Details</h3><p>&emsp;&emsp;我们利用DPED [22]图像构建逼真的模糊核集$K’$。DPED [22]是一个大规模的数据集，包含超过22K张由3种不同低端手机型号拍摄的真实照片。我们根据相机型号将数据集分为两部分，DPED训练集和DPED测试集。DPED训练集由使用Blackberry Passport和Sony Xperia Z拍摄的照片组成，作为提取Sec. 3.2.1中逼真的模糊核ke的参考真实摄影LR集。DPED测试集包含使用iPhone3GS拍摄的照片，用作验证数据集。我们使用来自[35]的核估计代码从DPED训练集中收集了1000个逼真的模糊核$K’ &#x3D; {k’_1，k’_2，…，k’_1000}$。我们在核建模GAN G的训练中使用这些核。我们将批大小设置为32，$λ &#x3D; 10$用于损失函数（见公式5）。$G$训练了20,000个epoch。通过使用训练好的$G$生成1,000个核并将它们添加到$K’$中，我们获得了扩展的模糊核池$K+$。<br>&emsp;&emsp;我们使用DIV2K [44]的训练集作为HR图像，从中提取大小为128×128的补丁。在SR网络的训练过程中，我们构建了配对数据集${X′，Y}$：在每个epoch中，每个HR补丁都会与从$K+$中随机选择的核$k’$卷积，以获得一组粗略的HR补丁。我们使用ADAM优化器[27]训练我们的SR网络，并将批大小设置为32。学习率初始化为10^-4，并在每10个epoch时减半。源代码公开在线上可获得1。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608111833.png" alt="Img"></p><h3 id="4-2-Estimated-Kernels"><a href="#4-2-Estimated-Kernels" class="headerlink" title="4.2. Estimated Kernels"></a>4.2. Estimated Kernels</h3><p>&emsp;&emsp;我们首先研究了模糊核的分布。我们在图3和图4中展示了使用KMSR生成的模糊核$k’_e$的示例。我们还可视化了Matlab双三次插值核和三个各向同性高斯核gsigma，其中sigmas（g1.25，g1.6和g1.7）通常用于合成×2 SR中的LR图像。请注意，与其他核的低通形状相比，双三次插值核是带通的。双三次插值核的设计是为了保持图像的清晰度，并避免在下采样操作期间出现混叠[25]。正如[11]所述，双三次插值核不是图像获取中真实模糊核的适当近似，因为相机模糊是低通的，并且通常更多地削弱了场景的高频信息。在图4中，还请注意，由KMSR生成的核涵盖了广泛的分布范围，包括比双三次插值核更好地近似真实相机模糊[34]的高斯核。因此，KMSR能够生成非常多样化的粗略HR图像。</p><h3 id="4-3-Experiments-on-Bicubic-and-Gaussian-Blur-Kernels"><a href="#4-3-Experiments-on-Bicubic-and-Gaussian-Blur-Kernels" class="headerlink" title="4.3.Experiments on Bicubic and Gaussian Blur-Kernels"></a>4.3.Experiments on Bicubic and Gaussian Blur-Kernels</h3><p>&emsp;&emsp;在本节中，我们通过将不同的模糊核应用于DIV2K [44]数据集的验证集上生成的合成LR图像，评估了KMSR和其他基于CNN的SR网络。我们在两个上采样因子$s &#x3D; 2（×2 SR）$和$s &#x3D; 4（×4 SR）$上进行测试，并使用四个不同的核在DIV2K [44]验证集上生成四个合成LR数据集。我们包括了抗锯齿双三次插值核，因为它被许多算法使用，尽管它不是真实图像的物理可行相机模糊[11]。我们还测试了3个各向同性高斯核$g_{1.25}$ [58]，$g_{1.6}$[9]和$g_{1.7}$ [18]；它们通常用作合成LR图像的模糊核[34]。这四个核在图3的第一行中可视化。<br>&emsp;&emsp;我们将我们提出的KMSR与最先进的基于CNN的SR方法进行比较：SRCNN [8]（我们使用9-5-5模型），VDSR [26]，EDSR [31]和DBPN [17]。我们使用各自作者发布的代码和模型。请注意，这四个网络仅使用双三次插值核在相应的LR图像从HR图像生成过程中进行训练。<br>&emsp;&emsp;不同SR网络在不同LR数据集上的定量结果如表1所示。尽管KMSR在使用双三次插值核生成的LR图像上产生了更差的结果，但它在所有其他实验设置上的表现都优于所有其他网络，包括上采样因子s &#x3D; 2和s &#x3D; 4。我们还可以观察到，仅使用双三次插值LR图像进行训练的SR网络的性能受到限制，当双三次插值核偏离真实模糊核时，这些网络的PSNR改进不到0.4dB（表1中第3列）。即使使用更深层的网络，EDSR [31]和DBPN [17]也无法超越SRCNN [8]和VDSR [26]这些浅层网络。通过建模真实核，我们的KMSR在PSNR上优于所有其他网络，最高可达1.91dB。使用$g_1.6$作为模糊核和$s &#x3D; 2$作为上采样因子的视觉比较如图5所示。请注意，由于使用更真实的模糊核进行训练，KMSR产生的结果在视觉上比其他方法更清晰。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608115627.png" alt="Img"></p><h3 id="4-4-Experiments-on-Realistic-Kernels"><a href="#4-4-Experiments-on-Realistic-Kernels" class="headerlink" title="4.4. Experiments on Realistic Kernels"></a>4.4. Experiments on Realistic Kernels</h3><p>&emsp;&emsp;为验证所提出的KMSR对于具有真实未知核的图像的能力，我们在×2和×4 SR上进行了实验，用未见于KMSR训练中的真实模糊核合成LR图像。我们从DEPD测试数据集（即iPhone3GS图像）中收集了100个模糊核。然后，我们将这些模糊核应用于使用DIV2K [44]验证集生成粗略的HR图像。表2显示了不同SR网络的结果PSNR和SSIM。与之前一样，仅使用双三次插值核的SR网络在这些图像上的表现受到限制。这凸显了CNN-based SR网络对于在创建训练数据集时使用错误核的敏感性。如果算法将应用于真实相机数据，则模糊核建模是改进SR网络的有前途的途径。<br>&emsp;&emsp;我们在图6中呈现了定性结果。KMSR成功重建了HR图像中的详细纹理和边缘，并产生了更好的输出。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608144336.png" alt="Img"></p><h3 id="4-5-Experiments-on-Real-Photographs"><a href="#4-5-Experiments-on-Real-Photographs" class="headerlink" title="4.5. Experiments on Real Photographs"></a>4.5. Experiments on Real Photographs</h3><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608144324.png" alt="Img"><br>&emsp;&emsp;我们还在真实照片上进行了×2 SR实验。图7展示了KMSR在DEPD测试数据集中iPhone3GS拍摄的一张照片上的输出结果。感知驱动的SR方法通常可以恢复更详细的纹理，并实现更好的视觉质量，优于之前的SR网络。除了我们比较的四种SR方法之外，我们还展示了感知优化的SR网络ESRGAN [47]的输出结果。值得注意的是，仅使用双三次下采样LR图像进行训练的网络往往会产生过于平滑的图像，而KMSR可以恢复出具有更好细节的锐利图像。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608144350.png" alt="Img"><br>&emsp;&emsp;由于在这个实验中没有参考HR图像，因此我们在一个众包网站上进行了心理视觉实验来验证我们的方法。我们只与EDSR [31]和DBPN [17]这两个最先进的基于CNN的SR网络进行比较。请注意，由于显示设备的分辨率限制，我们无法显示全分辨率图像。我们从DEPD-testing随机选择了50张图像，并从每张图像中裁剪了大小为500×500的补丁。对于每个补丁，我们向参与者展示了EDSR [31]，DBPN [17]和我们的KMSR的SR结果。我们要求他们在这三个结果中选择最清晰和最锐利的图像。为了避免偏差，三个SR图像的顺序被随机打乱。总共有35个用户参加了实验，每个用户标记了所有50个图像。心理视觉实验的结果如表3所示。对于50个图像中的44个，KMSR的输出优于另外两种方法，这表明KMSR能够产生比其他两种SR网络更好的视觉效果。</p><h3 id="4-6-Experiments-on-Zoom-in-Super-Resolution"><a href="#4-6-Experiments-on-Zoom-in-Super-Resolution" class="headerlink" title="4.6. Experiments on Zoom-in Super-Resolution"></a>4.6. Experiments on Zoom-in Super-Resolution</h3><p>&emsp;&emsp;为进一步验证所提出的KMSR的性能，我们在使用相同相机但不同焦距拍摄的图像上进行了实验。我们使用24-70mm变焦镜头拍摄照片对。35mm焦距照片用作LR图像，而在同一位置拍摄的70mm焦距照片用作LR图像的参考HR图像，进行×2 SR。我们使用小光圈（f &#x2F; 22）拍摄所有照片以最小化景深差异。我们从LR图像中裁剪大小为250×250的补丁，并从参考HR图像中裁剪大小为500×500的补丁。为了对齐补丁，我们进行水平和垂直对齐的网格搜索，然后在LR补丁上应用不同的SR网络。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608144605.png" alt="Img"><br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/KMSR.md/img-20230608144615.png" alt="Img"></p><h3 id="4-7-Ablation-studies"><a href="#4-7-Ablation-studies" class="headerlink" title="4.7. Ablation studies"></a>4.7. Ablation studies</h3><p>&emsp;&emsp;为展示使用真实核的有效性并展示我们使用的核估计算法[35]的精度，我们训练和测试了所提出网络的另一个版本$KMSR_{A1}$，而不收集真实核。在建立用于$KMSR_{A1}$的核池$K’<em>{A1}$时，我们使用双三次下采样的HR图像作为LR图像，即我们估计在双三次下采样的、双三次上采样的粗略HR图像$X’</em>{A1}$上的模糊核$k’<em>{A1}$。然后我们按照KMSR的相同程序进行。我们在$K’</em>{A1}$上训练了一个GAN并生成了用于训练$KMSR_{A1}$的更大的核池$K+<em>{A1}$。我们在不同的实验设置上测试$KMSR_A1$，定量结果如表5所示。对于高斯和真实核，$KMSR</em>{A1}$实现了与最先进的SR网络（见表1）相当的结果，这意味着$KMSR_{A1}$能够学习从双三次下采样的LR图像到HR图像的映射。结果还表明，我们使用$K_+$的真实核对KMSR进行训练实现了显著的性能提升（表1的最后一列）。<br>&emsp;&emsp;为测试GAN在提高泛化性能方面的贡献，我们训练了$KMSR_{A2}$，这是没有使用GAN但使用简单的数据增强来扩展核池的KMSR。在这种情况下，$KMSR_{A2}$仅在包含原始估计核$k’$及其旋转、翻转和缩放版本的$K′_{A2}$上进行训练。结果如表5所示。平均而言，KMSR在$KMSR_{A2}$上获得了0.5dB的改进，这使我们相信使用GAN来增强核池会产生比简单的数据增强更多样化的表示。这进一步验证了将GAN合并到增强真实核池中的有效性。</p><h3 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h3><p>&emsp;&emsp;我们通过建模真实模糊核来提高基于CNN的SR网络在真实LR图像上的性能。与现有方法使用双三次核在成像模型中获取LR训练图像不同，我们通过使用从真实照片估计的一组真实模糊核来生成SR训练数据集。我们进一步通过训练GAN来输出额外的真实核来增强模糊核池。我们的KMSR能够产生视觉上合理的HR图像，通过定量指标、定性比较和心理视觉实验证明。KMSR提供了一种可行的解决方案，实现基于CNN的SR处理真实照片。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>SRGAN</title>
      <link href="/2023/08/14/SRGAN/"/>
      <url>/2023/08/14/SRGAN/</url>
      
        <content type="html"><![CDATA[<h1 id="SRGAN"><a href="#SRGAN" class="headerlink" title="SRGAN"></a>SRGAN</h1><h2 id="二、方法"><a href="#二、方法" class="headerlink" title="二、方法"></a>二、方法</h2><p>&emsp;&emsp;在超分辨率图像重建中，目标是从低分辨率输入图像$I_{LR}$估计出高分辨率的超分辨率图像$I_{SR}$。其中，$I_{LR}$是其高分辨率对应物$I_{HR}$的低分辨率版本。在训练中，只有高分辨率图像可用。$I_{LR}$是通过对$I_{HR}$应用高斯滤波器，然后进行下采样操作（下采样因子为$r$）得到的。对于具有$C$个颜色通道的图像，$I_{LR}$由大小为$W×H×C$的实值张量描述，而$I_{HR}$和$I_{SR}$分别由大小为$rW×rH×C$的实值张量描述。<br>&emsp;&emsp;我们的最终目标是训练一个生成函数$G$，为给定的低分辨率输入图像估计其对应的高分辨率图像。为了实现这一目标，我们训练一个作为前馈CNN的生成器网络$G_{\theta_{G}}$，其中$\theta_{G}$是参数化的。这里，$\theta_{G}&#x3D;{W_{1:L};b_{1:L}}$表示一个$L$层深的网络的权重和偏置，通过优化SR特定的损失函数$l^{SR}$来获得。对于具有相应$I_{n}^{LR},(n&#x3D; 1, . . . ,N)$的训练图像$I_{n}^{HR},(n&#x3D; 1, . . . ,N)$ 我们解决以下问题：<br>$$\hat{\theta}<em>G&#x3D;\arg\min\limits</em>{\theta_G}\frac{1}{N}\sum_{n&#x3D;1}^N l^{SR}(G_{\theta_G}(I_n^{LR}),I_n^{HR}) \tag{1}$$<br>&emsp;&emsp;在这项工作中，我们将特别设计一个感知损失$I^{SR}$，作为几个损失组件的加权组合，以模拟恢复的超分辨率图像的不同理想特征。这些单独的损失函数在第2.2节中将被更详细地描述。</p><h3 id="2-1对抗网络架构"><a href="#2-1对抗网络架构" class="headerlink" title="2.1对抗网络架构"></a>2.1对抗网络架构</h3><p>&emsp;&emsp;根据Goodfellow等人[22]的定义，我们进一步定义一个判别器网络$D_{\theta_{D}}$，我们在与$G_{\theta_{G}}$交替优化的过程中解决对抗性的最小最大问题：<br>$$\begin{aligned}<br>\operatorname*{min}<em>{\theta</em>{G}}\operatorname*{max}<em>{\theta</em>{D}}&amp; \mathbb{E}<em>{I^{HR}\sim p</em>{\operatorname{train}}(I^{HR})}[\log D_{\theta_D}(I^{HR})]\mathbb{+}  \<br>&amp;\mathbb{E}<em>{I^{L R}\sim p</em>{G}(I^{L R})}[\log(1-D_{\theta_{D}}(G_{\theta_{G}}(I^{L R}))]<br>\end{aligned} \tag{2}$$<br>&emsp;&emsp;这种公式的一般思想是，它允许训练一个生成模型$G$，其目的是欺骗一个可微分的判别器$D$，后者被训练用于区分超分辨率图像和真实图像。通过这种方法，我们的生成器可以学习创建高度类似于真实图像的解决方案，从而使$D$难以对其进行分类。这鼓励出现在自然图像子空间、流形中的感知上更优的解决方案。这与通过最小化像素级误差度量（如MSE）获得的SR解决方案形成对比。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/SRGAN.md/img-20230529215846.png" alt="Img"><br>::: tip ParametricReLU<br>$$\operatorname{P}\operatorname{ReLU(x)}&#x3D;\begin{cases}\operatorname{x}&amp;,\operatorname{x}&gt;0\[6pt]a_i\operatorname{x}&amp;,\operatorname{x}\leq0\end{cases}$$</p><p>:::</p><p>&emsp;&emsp;我们的非常深的生成器网络$G$的核心如图4所示，其中包含$B$个具有相同布局的残差块。受Johnson等人[33]的启发，我们采用了Gross和Wilber[24]提出的块布局。具体而言，我们使用两个具有小型3×3卷积核和64个特征映射的卷积层，后面跟随批量归一化层[32]和ParametricReLU [28]作为激活函数。我们使用两个经过训练的子像素卷积层，如Shi等人[48]所提出的那样，来增加输入图像的分辨率。<br>::: tip BN<br>在残差块中，BN（Batch Normalization）层的作用是对每个卷积层的输出进行归一化，从而加速模型训练和提高模型的泛化能力。</p><p>在卷积神经网络中，由于每层网络的输入分布会随着深度的增加而发生变化，这可能会导致训练过程出现梯度消失或爆炸的问题。而BN层可以通过对每个Batch的数据进行均值和方差的归一化，使得每层网络的输入分布更加稳定，从而加速模型的收敛，并提高模型的泛化能力。</p><p>此外，BN层还可以降低模型对超参数的依赖性，提高模型的鲁棒性。因此，在残差块中使用BN层可以加速模型训练，提高模型性能，同时使得模型更加稳定和鲁棒。<br>:::<br>::: tip PixelShuffle x2<br>PixelShuffle x2是一种上采样技术，可以将低分辨率图像转换为高分辨率图像。具体来说，PixelShuffle x2将一个大小为HxWxC的低分辨率图像转换为一个大小为2Hx2WxC&#x2F;4的高分辨率图像，其中C表示图像的通道数。</p><p>PixelShuffle x2的实现方法是将输入图像分成大小为2x2的像素块，并将它们重新排列成一个大小为1x1x4C的输出块。这个输出块中的4C个元素表示输入块中的4个像素，其中C个元素属于相同的通道。最后，将所有输出块合并起来，形成一个大小为2Hx2WxC&#x2F;4的高分辨率图像。</p><p>在SRGAN中，PixelShuffle x2通常用于将低分辨率图像的通道数增加4倍，从而实现高分辨率图像的生成。这个过程通常是在SRGAN的生成器网络中完成的，由一系列卷积和PixelShuffle x2操作组成。<br>:::</p><p>&emsp;&emsp;为了将真实的高分辨率图像与生成的超分辨率样本区分开来，我们训练了一个判别器网络。该网络的架构如图4所示。我们遵循Radford等人[44]总结的架构指南，使用LeakyReLU激活函数$（α &#x3D; 0.2）$，并避免在整个网络中使用最大池化。判别器网络被训练来解决公式2中的最大化问题。它包含八个卷积层，每个层都增加了3×3的滤波器核的数量，从64个核增加到512个核，这与VGG网络[49]相同。滑动卷积被用来在每次特征数加倍时降低图像分辨率。最终得到512个特征映射，然后通过两个稠密层和最后的sigmoid激活函数来获得样本分类的概率。<br>&emsp;&emsp;我们的感知损失函数$l^{SR}$的定义对于我们的生成器网络的性能至关重要。虽然$l^{SR}$通常基于MSE [10, 48]进行建模，但我们改进了Johnson等人[33]和Bruna等人[5]的方法，并设计了一种考虑感知相关特征的损失函数。我们将感知损失定义为内容损失($l_X^{SR}$)和对抗性损失分量的加权和，具体如下：<br>$$l^{SR}&#x3D;\quad l^{SR}<em>\mathrm X+10^{-3}l^{SR}</em>{Gen} \tag{3}$$</p><h4 id="2-2-1内容损失"><a href="#2-2-1内容损失" class="headerlink" title="2.2.1内容损失"></a>2.2.1内容损失</h4><p>按像素计算的均方误差损失为：<br>$$l_{M S E}^{S R}&#x3D;\frac{1}{r^{2}W H}\sum_{x&#x3D;1}^{r W}\sum_{y&#x3D;1}^{r H}(I_{x,y}^{H R}-G_{\theta_{G}}(I^{L R})_{x,y})^{2} \tag{4}$$<br>&emsp;&emsp;这是图像超分辨率中最广泛使用的优化目标，许多最先进的方法都依赖于它[10, 48]。然而，虽然MSE优化问题的解决方案通常具有特别高的PSNR，但它们往往缺乏高频内容，导致感知上不令人满意的解决方案，纹理过于平滑（参见图2）。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/SRGAN.md/img-20230529222459.png" alt="Img"></p><p>::: tip PSNR<br>PSNR（Peak Signal-to-Noise Ratio）是一种用于衡量图像或视频质量的指标，它通常用于评估图像超分辨率算法的性能。PSNR的计算是基于图像的均方误差（MSE）来进行的。</p><p>假设原始图像为$I$，重建的图像为$I’$，它们的大小均为$M×N$。则PSNR的计算公式如下：</p><p>$$PSNR &#x3D; 20 * lg(MAXp) - 10 * lg(MSE)$$</p><p>其中，$MAX_p$是像素值的最大可能取值（例如对于8位灰度图像，$MAXp$为255），$MSE$是均方误差，计算公式如下：</p><p>$$MSE &#x3D; 1 &#x2F; (M * N) * ΣΣ(I(i,j) - I’(i,j))^2$$</p><p>其中，$i$和$j$分别表示图像中的行和列，$ΣΣ$表示对所有像素求和。</p><p>PSNR的单位是分贝（dB），通常情况下，PSNR值越高，表示重建图像与原始图像的相似度越高，质量也就越好。但需要注意的是，PSNR并不是完美的图像质量度量标准，它只关注于像素级别的误差，而忽略了人眼的感知差异。因此，即使PSNR值很高，重建的图像仍可能在视觉上不如原始图像。<br>:::<br>&emsp;&emsp;我们不再依赖于像素级别的损失函数，而是基于Gatys等人[19]、Bruna等人[5]和Johnson等人[33]的思想，使用更接近感知相似度的损失函数。我们定义了基于Simonyan和Zisserman[49]所描述的预训练19层VGG网络中的ReLU激活层的VGG损失。其中，$\phi_{i,j}$表示在VGG19网络的第i个最大池化层之前、第j个卷积层之后（经过激活函数）得到的特征映射，我们将其视为已知。然后，我们将VGG损失定义为重建图像$G_{\theta_{G}}(I^{L R})$的特征表示和参考图像$I^{HR}$之间的欧几里得距离：<br>$$\begin{aligned}<br>l_{V G G&#x2F;i.}^{S R} {}<em>{j}&#x3D;\frac{1}{W</em>{i,j}H_{i,j}}\sum_{x&#x3D;1}^{W_{i,j}H_{i,j}}(\phi_{i,j}(I^{H R})<em>{x,y}  \<br>-\phi</em>{i,j}(G_{\theta_G}(I^{LR}))<em>{x,y})^2<br>\end{aligned} \tag{5}$$<br>&emsp;&emsp;这里的$W</em>{i,j}$和$H_{i,j}$表示VGG网络中各自特征映射的尺寸大小。</p><h4 id="2-2-2-对抗损失"><a href="#2-2-2-对抗损失" class="headerlink" title="2.2.2 对抗损失"></a>2.2.2 对抗损失</h4><p>&emsp;&emsp;除了前面描述的内容损失外，我们还将生成器网络的GAN组件添加到感知损失中。这样做可以通过试图欺骗鉴别器网络来鼓励我们的网络更加倾向于自然图像的流形上的解决方案。生成损失$l_{Gen}^{SR}$基于鉴别器$D_{\theta_{D}}(G_{\theta_{G}}(I^{L R}))$对所有训练样本的概率来定义，具体如下：<br>$$l_{Gen}^{SR}&#x3D;\sum\limits_{n&#x3D;1}^N-\log D_{\theta_D}(G_{\theta_G}(I^{LR})) \tag{6}$$<br>&emsp;&emsp;这里的$D_{\theta_{D}}(G_{\theta_{G}}(I^{L R}))$表示重建图像$G_{\theta_{G}}(I^{L R})$是自然高分辨率图像的概率。为了获得更好的梯度行为，我们采用最小化-$\log D_{\theta_{D}}(G_{\theta_{G}}(I^{L R}))$的方式，而不是采用$\log[1-D_{\theta_{D}}(G_{\theta_{G}}(I^{L R}))]$的方式[22]。</p><p>$$ContentLoss &#x3D; MSE(VGG(G_{\theta_G}(I^{LR})) -  VGG(I^{HR}))$$</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RealSR</title>
      <link href="/2023/08/14/RealSR/"/>
      <url>/2023/08/14/RealSR/</url>
      
        <content type="html"><![CDATA[<h1 id="RealSR"><a href="#RealSR" class="headerlink" title="RealSR"></a>RealSR</h1><h1 id="Real-World-Super-Resolution-via-Kernel-Estimation-and-Noise-Injection"><a href="#Real-World-Super-Resolution-via-Kernel-Estimation-and-Noise-Injection" class="headerlink" title="Real-World Super-Resolution via Kernel Estimation and Noise Injection"></a>Real-World Super-Resolution via Kernel Estimation and Noise Injection</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;最近的最先进的超分辨率方法在理想数据集上取得了令人印象深刻的表现，无论模糊和噪声如何。然而，这些方法在真实世界的图像超分辨率中总是失败，因为它们大多采用从高质量图像进行简单的双三次降采样来构建低分辨率（LR）和高分辨率（HR）对进行训练，可能会丢失频率相关的细节。为了解决这个问题，我们专注于设计一个新颖的退化框架，用于估计各种模糊核以及真实噪声分布，基于我们的新颖退化框架，我们可以获得与真实世界图像共享相同领域的LR图像。然后，我们提出了一个旨在更好地感知的真实世界超分辨率模型。对合成噪声数据和真实世界图像的大量实验表明，我们的方法优于最先进的方法，噪声更低，视觉质量更好。此外，我们的方法是NTIRE 2020挑战赛真实世界超分辨率的两个赛道的获胜者，显著优于其他竞争者。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>&emsp;&emsp;超分辨率（SR）任务是提高低质量图像的分辨率并增强其清晰度 [2]。近年来，基于深度学习的方法 [9、35、7、34、20、19、23] 在保真度性能方面取得了显著的成果，主要集中在设计网络结构以进一步提高特定数据集的性能。其中大多数使用固定的双三次操作进行降采样以构建训练数据对。同样，在测试阶段，通过双三次内核降采样的输入图像将被输入到设计好的网络中。随后，生成的结果将与Ground Truth（GT）进行比较，以计算PSNR、SSIM和其他指标。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610100658.png" alt="Img"><br>&emsp;&emsp;尽管这些方法在保真度方面有所改进，但它们忽略了一个问题，即使用理想双三次降采样是不合理的。先前的方法通过理想的降采样方法构建数据：<br>$$\mathbf{I}<em>{LR}&#x3D;\mathbf{I}</em>{HR}\downarrow_s , \tag{1}$$<br>&emsp;&emsp;其中，$I_{LR}$和$I_{HR}$分别表示LR和HR图像，$s$表示缩放因子。这使得获取用于训练模型的配对数据变得容易。然而，由于采用已知且固定的降采样核，退化图像可能会丢失高频细节，但可以使低频内容更清晰。基于这样构建的配对数据，SR模型$f(·)$被训练以最小化$n$张图像的平均误差：<br>$$\arg\min\limits_f\Sigma|f(\mathbf{I}^i_{LR})-\mathbf{I}^i_{HR}|,i\in{1,2\cdots n}.\tag{2}$$<br>&emsp;&emsp;如果在相同的降采样数据集上进行测试，生成的结果就如预期的那样。然而，一旦直接在原始图像上进行测试，结果会非常模糊，有很多噪声。主要原因是双三次降采样图像不属于与原始图像相同的领域。由于领域差距，这些方法会产生不愉快的伪影并且在真实世界的图像上失败。例如，EDSR&#x2F;ZSSR在图1中的一个真实图像上产生了不满意的结果。因此，真实世界超分辨率的关键问题是引入准确的退化方法，以确保生成的低分辨率（LR）图像和原始图像具有相同的领域属性。<br>&emsp;&emsp;我们首先分析了不同核对降采样图像的影响 [42、49、11]。在我们的分析之前，我们将原始真实图像定义为源域$\mathcal{X}$，将干净的高分辨率（HR）图像定义为目标域$\mathcal{Y}$。我们发现不同程度的模糊核直接影响了降采样图像的模糊程度。双三次降采样可以被看作是一种理想的降采样方式，因为它尽可能地保留了来自$\mathcal{X}$的信息。然而，这些降采样图像的频率已经改变为另一个域$\mathcal{X}’$。当在${\mathcal{X}’，\mathcal{Y}}$上进行训练时，模型将尝试恢复所有细节，因为在域$\mathcal{X}’$中所有信息都很重要。模型在$I_{LR}$上表现良好，但通常在$I_{src}∈\mathcal{X}$（未处理的真实图像）上失败。另一个问题是降采样图像几乎没有噪声，而$\mathcal{X}$中的真实世界图像通常有很多噪声。仅通过估计模糊核不能准确地建模退化过程。<br>&emsp;&emsp;在本文中，我们提出了一个新颖的真实退化框架用于超分辨率（RealSR），其中包含核估计和噪声注入以保留原始领域属性。一方面，我们首先使用现有的核估计方法 [3] 生成更真实的LR图像。另一方面，我们提出了一种简单而有效的方法，直接从原始图像中收集噪声并将其添加到降采样图像中。此外，我们引入了基于块的鉴别器 [17] 用于RealSR，以避免生成伪影。为验证所提出方法的有效性，我们对合成数据集和真实数据集进行了实验。实验结果表明，与现有最先进方法相比，我们的方法产生了更清晰更干净的结果。最后，我们进行消融实验，分别验证了核估计、噪声注入和基于块的鉴别器对SR生成器的有效性。我们还参加了NTIRE 2020真实世界超分辨率挑战赛，并在两个赛道上远远超过其他竞争者。<br>&emsp;&emsp;总之，我们的总体贡献有三方面:</p><ul><li>我们提出了一个新颖的真实世界超分辨率退化框架RealSR，为超分辨率学习提供了逼真的图像。</li><li>通过估计核和噪声，我们探索了模糊和噪声图像的特定退化。</li><li>我们证明了所提出的RealSR在视觉质量方面达到了最先进的结果。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><p>&emsp;&emsp;<strong>超分辨率。</strong> 最近，许多基于卷积神经网络（CNN）的SR网络[23、30、13、14、22、31、41]在双三次降采样图像上取得了强大的性能。其中代表性的是EDSR[23]，它使用深度残差网络来训练SR模型。张等人[46]提出了一种残差内残差结构，形成非常深的网络，其性能比EDSR更好。戴等人[8]提出了一种二阶通道注意模块，通过使用二阶特征统计量来自适应地重新调整通道特征，以获得更加区分性的表示。哈里斯等人[12]提出了深度反向投影网络，利用迭代的上采样和下采样层，为每个阶段的投影误差提供误差反馈机制。尽管作者在保真度方面取得了良好的性能，但生成的图像视觉效果差，看起来模糊。为了解决这个问题，一些研究人员通过空间特征变换[47、48、38]增强了逼真的纹理。Soh等人提出了一种自然流形判别，用于对模糊或噪声图像进行分类，用于监督生成图像的质量。此外，一些基于生成对抗网络（GAN）的方法[21、44、39]更加关注视觉效果，引入对抗性损失和感知损失。<br>&emsp;&emsp;然而，这些在双三次核生成的数据上训练的SR模型只能在干净的HR数据上表现良好，因为模型在训练过程中从未见过模糊&#x2F;噪声数据。这与实际需求不符，真实的LR图像通常带有噪声和模糊。为了解决这个冲突，徐等人[40、5、4、45]直接从自然场景中采集原始照片对。但是，收集这样的配对数据需要严格的条件和大量的人工成本。在本文中，我们通过分析真实图像中的退化情况，专注于SR网络在真实数据中的训练策略。<br>&emsp;&emsp;<strong>真实世界超分辨率。</strong> 为了克服真实世界超分辨率的这些挑战，最近的工作[42、49]结合了去噪或去模糊的方法。这些方法是在人工构建的模糊和加噪声的数据上进行训练的，进一步增强了SR模型的鲁棒性。然而，这些显式建模方法需要足够的关于模糊&#x2F;噪声的先验知识，因此应用范围有限。<br>&emsp;&emsp;最近，一系列真实世界超分辨率挑战[25、26]吸引了许多参与者。许多新方法被提出来解决这个问题。例如，Fritsche等人[10]提出了DSGAN模型来生成降级图像。Lugmayr等人[24]提出了一种无监督学习方法用于真实世界超分辨率。ZSSR[32]放弃了在大数据上的训练过程，为每个测试图像训练一个小模型，使得特定模型更加关注图像的内部信息。但是付出的代价是推理时间大大增加，难以应用于实际场景。与这些方法不同的是，我们明确地估计了真实图像中的核降级，这对于生成清晰和锐利的结果非常重要。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610102808.png" alt="Img"></p><h2 id="3-The-Proposed-RealSR"><a href="#3-The-Proposed-RealSR" class="headerlink" title="3. The Proposed RealSR"></a>3. The Proposed RealSR</h2><p>&emsp;&emsp;本节中，我们介绍了如图2所示的所提出的退化方法。我们的方法主要分为两个阶段。第一阶段是从真实数据中估计退化并生成逼真的LR图像。第二阶段是基于构建的数据训练SR模型。</p><h3 id="3-1-Realistic-Degradation-for-Super-Resolution"><a href="#3-1-Realistic-Degradation-for-Super-Resolution" class="headerlink" title="3.1. Realistic Degradation for Super-Resolution"></a>3.1. Realistic Degradation for Super-Resolution</h3><p>&emsp;&emsp;这里，我们介绍一种基于核估计和噪声注入的真实图像退化新方法。假设LR图像是通过以下退化方法获得的：<br>$$\mathbf I_{L R}&#x3D;(\mathbf I_{H R}*\mathbf k)\downarrow_{s}+\mathbf n,\tag{3}$$<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610103127.png" alt="Img"></p><h3 id="3-2-Kernel-Estimation-and-Downsampling"><a href="#3-2-Kernel-Estimation-and-Downsampling" class="headerlink" title="3.2. Kernel Estimation and Downsampling"></a>3.2. Kernel Estimation and Downsampling</h3><p>&emsp;&emsp;我们使用核估计算法来明确地从真实图像中估计核。受KernelGAN [3]的启发，我们采用类似的核估计方法，并基于真实图像设置适当的参数。KernelGAN的生成器是一个线性模型，没有任何激活层，因此所有层的参数可以组合成一个固定的核。估计的核需要满足以下约束条件:<br>$$\begin{aligned}<br>\operatorname{arg}\operatorname*{min}<em>{\mathbf{k}}|(\mathbf{I}</em>{s r c}*\mathbf{k})\downarrow_{s}-\mathbf{I}<em>{s r c}\downarrow</em>{s}|<em>{1}+|1-\Sigma\mathbf{k}</em>{i,j}| \<br>+|\Sigma\mathbf{k}<em>{i,j}\cdot\mathbf{m}</em>{i,j}|+|1-D((\mathbf{I}<em>{src}*\mathbf{k})\downarrow_s)|.<br>\end{aligned}\tag{4}$$<br>&emsp;&emsp;其中，$(I</em>{src} * k)↓s$是通过核k降采样得到的LR图像，$I_{src}↓s$是通过理想核降采样得到的图像，因此最小化这个误差可以鼓励降采样图像保留源图像的重要低频信息。此外，上述公式的第二项是将核k限制为总和为1，第三项是对$k$的边界进行惩罚。最后，鉴别器$D(·)$是为了确保源域的一致性。）<br>&emsp;&emsp;<strong>后期处理。</strong> 为了获得更多的HR图像，我们尝试从$\mathcal{X}$中生成无噪声的图像。具体来说，我们采用双三次下采样的方法在源域中对真实图像进行处理，以消除噪声并使图像更加清晰。设$I_{src} ∈ \mathcal{X}$为源图像集中的一张图片，$k_{bic}$为理想的双三次核。然后，使用一个清晰度放大因子$sc$对图像进行下采样。<br>$$\mathbf{I}<em>{HR}&#x3D;(\mathbf{I}</em>{src}*\mathbf{k}<em>{bic})\downarrow</em>{sc}.\tag{5}$$<br>&emsp;&emsp;<strong>应用模糊核来降低图像的清晰度。</strong> 我们将降采样后的图像视为清晰的HR图像。然后，我们通过从退化池中随机选择一个模糊核来对这些HR图像进行退化处理。降采样过程是跨相关操作，后跟步长为$s$的采样，可以表示为：<br>$$\mathbf{I}<em>{D}&#x3D;(\mathbf{I}</em>{H R}*\mathbf{k}<em>{i})\downarrow</em>{s},i\in{1,2\cdots m},\tag{6}$$<br>其中，$I_D$表示降采样后的图像，$k_i$指的是从${k_1，k_2，…，k_m}$中选择的具体模糊核。</p><h3 id="3-3-Noise-Injection"><a href="#3-3-Noise-Injection" class="headerlink" title="3.3. Noise Injection"></a>3.3. Noise Injection</h3><p>&emsp;&emsp;对于带噪声的图像，我们明确地将噪声注入到降采样的图像中，以生成更真实的LR图像。由于在降采样过程中高频信息丢失，所以退化的噪声分布也会同时发生变化。为了使退化的图像具有与源图像类似的噪声分布，我们直接从源数据集X中收集噪声块。我们观察到，具有更丰富内容的块具有更大的方差。基于这个观察结果，并受到[6, 49]的启发，我们设计了一个过滤规则，以收集方差在一定范围内的块。这个规则简单但有效，通过以下方式将噪声和内容分离开来：<br>$$\sigma(\mathbf{n}<em>i)&lt;v,\tag{7}$$<br>其中，$σ(·)$表示计算方差的函数，$v$是方差的最大值。<br>&emsp;&emsp;<strong>降噪技术</strong> 假设收集到了一系列噪声块${n_1，n_2，…，n_l}$并将它们添加到了退化池中。噪声注入过程是通过从噪声池中随机裁剪块来完成的。类似地，我们将这个过程规范化为：<br>$$\mathbf{I}</em>{LR}&#x3D;\mathbf{I}_D+\mathbf{n}<em>i,i\in{1,2\cdots l},\tag{8}$$<br>其中，$n_i$是从噪声池${k_1，k_2，…，k_l}$中裁剪出的噪声块。具体来说，我们采用一种在线噪声注入方法，在训练阶段将内容和噪声结合起来。这使得噪声更加多样化，并规范化SR模型以区分带有噪声的内容。经过模糊核降级和注入噪声后，我们获得了$I</em>{LR} ∈ \mathcal{X}$。</p><h3 id="3-4-Super-Resolution-Model"><a href="#3-4-Super-Resolution-Model" class="headerlink" title="3.4. Super-Resolution Model"></a>3.4. Super-Resolution Model</h3><p>&emsp;&emsp;基于ESRGAN [39]，我们实现了一个SR模型，并在构建的配对数据${I_{LR}，I_{HR}} ∈ {\mathcal{X}，\mathcal{Y}}$上进行了训练。生成器采用RRDB [39]结构，并且生成的图像分辨率将增加4倍。训练过程中采用了多个损失函数，包括像素损失、感知损失[18]和对抗损失。像素损失$L_1$使用L1距离。感知损失$L_per$使用VGG-19 [33]的非激活特征，有助于增强边缘等低频特征的视觉效果。对抗损失$L_adv$用于增强生成图像的纹理细节，使其看起来更加真实。最终的损失函数是这三个损失函数的加权和：<br>$$L_{total}&#x3D;\lambda_1\cdot L_1+\lambda_{per}\cdot L_{per}+\lambda_{adv}\cdot L_{adv},\tag{9}$$<br>其中，$λ_1$，$λ_per$和$λ_adv$分别被经验性地设置为0.01、1和0.005。</p><h3 id="3-5-Patch-Discriminator-in-RealSR"><a href="#3-5-Patch-Discriminator-in-RealSR" class="headerlink" title="3.5. Patch Discriminator in RealSR"></a>3.5. Patch Discriminator in RealSR</h3><p>&emsp;&emsp;此外，我们观察到ESRGAN [39]的鉴别器可能会引入许多伪影。不同于默认的ESRGAN设置，我们使用块鉴别器[17, 50]代替VGG-128 [33]，因为两个方面的便利：1）VGG-128限制了生成图像的大小为128，使得多尺度训练不方便。2）VGG-128包含更深的网络和固定的全连接层，使得鉴别器更关注全局特征而忽略局部特征。相比之下，我们使用一个具有完全卷积结构的块鉴别器，它具有固定的感受野。例如，一个三层网络对应一个70×70的块。也就是说，鉴别器的每个输出值仅与本地固定区域的块相关。块损失将被反馈给生成器，以优化局部细节的梯度。请注意，最终的误差是所有局部误差的平均值，以保证全局一致性。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610113803.png" alt="Img"></p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4. Experiments"></a>4. Experiments</h2><h3 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1. Datasets"></a>4.1. Datasets</h3><p>&emsp;&emsp;<strong>DF2K</strong>  DF2K数据集合并了DIV2K [36]和Flikr2K [1]数据集，共包含3450张图像。这些图像被人工添加了高斯噪声以模拟传感器噪声。验证集包含100张带有相应参考图像的图像，因此可以计算基于参考图像的评估指标。<br>&emsp;&emsp;<strong>DPED</strong> DPED数据集包含5614张由iPhone3相机拍摄的图像。这个数据集中的图像是未经处理的真实图像，更具挑战性，包含噪声、模糊、暗光和其他低质量问题。验证集中的100张图像是从原始真实图像中裁剪出来的。由于没有相应的参考图像，我们只能提供视觉比较。</p><h3 id="4-2-Evaluation-Metrics"><a href="#4-2-Evaluation-Metrics" class="headerlink" title="4.2. Evaluation Metrics"></a>4.2. Evaluation Metrics</h3><p>&emsp;&emsp;对于合成数据的情况，我们计算不同方法生成的结果的PSNR、SSIM和LPIPS [43]。其中，PSNR和SSIM是图像恢复常用的评估指标。这两个指标更关注图像的保真度而不是视觉质量。相比之下，LPIPS更注重图像的视觉特征是否相似。它使用预训练的Alexnet提取图像特征，然后计算两个特征之间的距离。因此，LPIPS越小，生成的图像越接近于真实图像。</p><h3 id="4-3-Evaluation-on-Corrupted-Images"><a href="#4-3-Evaluation-on-Corrupted-Images" class="headerlink" title="4.3. Evaluation on Corrupted Images"></a>4.3. Evaluation on Corrupted Images</h3><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610114314.png" alt="Img"><br>&emsp;&emsp;首先，我们在受损的DF2K数据集上将我们的RealSR与最先进的SR方法进行比较。我们在由100张图像组成的验证集上评估性能。在这些方法生成结果后，我们根据参考图像计算PSNR、SSIM和LPIPS。由于LPIPS更能反映视觉质量，我们主要关注这个指标。比较的方法包括EDSR [23]、ESRGAN [39]、ZSSR [32]和K-ZSSR。我们使用作者发布的预训练模型评估EDSR和ESRGAN方法。由于ZSSR不需要训练过程，我们只需在验证图像上运行其测试代码即可。具体而言，K-ZSSR是KernelGAN [3]和ZSSR的组合。KernelGAN估计的核被用于ZSSR训练过程中的图像块降采样，而ZSSR则采用默认的双三次插值进行退化。<br>&emsp;&emsp;<strong>DF2K数据集上的定量结果</strong> 如表1所示，我们的RealSR表现出最佳的LPIPS性能，表明我们的结果在视觉特征方面更接近于真实图像。需要注意的是，我们的方法在PSNR方面比EDSR低，这是因为我们使用了更关注视觉质量的感知损失。通常，PSNR和LPIPS指标并不呈正相关关系，在某一范围内甚至呈相反的关系。<br>&emsp;&emsp;DF2K数据集上的定性结果如图3所示，我们可以看到同一图像上不同方法的局部细节，其中我们的RealSR产生的噪声明显更少。一方面，与EDSR和ZSSR相比，我们的结果更清晰，具有更丰富的纹理细节。另一方面，与ESRGAN和K-ZSSR相比，我们的结果几乎没有伪影，在于准确估计了真实噪声分布的降级过程。特别是，K-ZSSR使用的核比双三次插值更模糊，因此用于训练的图像几乎没有噪声，在对噪声图像进行处理时会产生许多伪影。在测试期间，SR模型将噪声错误地视为输入图像的内容。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610114442.png" alt="Img"></p><h3 id="4-4-Evaluation-on-Real-World-Images"><a href="#4-4-Evaluation-on-Real-World-Images" class="headerlink" title="4.4. Evaluation on Real-World Images"></a>4.4. Evaluation on Real-World Images</h3><p>&emsp;&emsp;我们提出的方法最令人关注的问题是真实世界的超分辨率，因此我们在DPED数据集上评估我们的RealSR，其中照片受到模糊、噪声等降解问题的影响。就像在真实图像的SR训练中遇到的问题一样，在验证阶段没有可以参考的参考图像。因此，我们只展示视觉比较的结果。为了使细节更清晰，我们放大了局部区域。<br>&emsp;&emsp;DPED数据集上的定性结果如图4所示。EDSR、ESRGAN和ZSSR方法未能正确区分树枝和天空中的噪声，导致结果模糊。在我们的结果中，树干和树枝更清晰，物体和背景之间的分界线更加清晰。对于K-ZSSR，由于噪声处理错误，结果产生了不必要的纹理细节。如果我们放大图像，这个结果是不可接受的，不能被视为高分辨率图像。在处理一些实心背景时，我们的方法的优势更加明显。从第三张图像可以看出，屋檐下的噪声已经被消除，只留下重要的低频特征。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610114608.png" alt="Img"><br>&emsp;&emsp;与现有方法相比，我们的RealSR产生的噪声和伪影更少，表明由噪声注入估计的噪声更接近真实噪声。与EDSR、ESRGAN和ZSSR相比，我们的RealSR结果更清晰、没有歧义。这是因为它们的方法都是在没有从真实图像中估计模糊核的情况下，使用双三次插值数据进行训练。此外，我们使用更加注重图像视觉特征的感知损失。与使用像素损失的EDSR相比，我们的结果具有更清晰的细节。此外，训练新的ZSSR或K-ZSSR模型的成本要比推理高得多，而我们的方法在推理期间只需要前向运行时间。</p><h3 id="4-5-NTIRE-2020-Challenge"><a href="#4-5-NTIRE-2020-Challenge" class="headerlink" title="4.5. NTIRE 2020 Challenge"></a>4.5. NTIRE 2020 Challenge</h3><p>&emsp;&emsp;我们的RealSR是NTIRE 2020挑战赛中真实世界超分辨率[26]两个赛道的优胜者。其中赛道1是通过图像处理伪影生成的合成受损数据，赛道2是智能手机图像的真实数据。每个赛道提供的数据包括两个域。一个是包含噪声和模糊的源域数据集，另一个是定义的干净HR目标数据集。任务是将LR图像的分辨率放大4倍，同时保持生成的SR图像的清晰度和锐度与给定的目标数据集一致。由于没有给定的训练数据对，参赛者需要使用这两组图像构建训练数据。我们应用了提出的方法，并在表2和表3中展示了在两个赛道上取得的最佳结果。需要注意的是，最终决策是基于人类研究，即赛道1的平均意见分数（MOS）和赛道2的平均意见排名（MOR）[26]。我们的方法大大优于其他方法，并生成具有优越的锐度和清晰度的SR图像。</p><h3 id="4-6-Ablation-Study"><a href="#4-6-Ablation-Study" class="headerlink" title="4.6. Ablation Study"></a>4.6. Ablation Study</h3><p>&emsp;&emsp;为了进一步验证估计核、在降级过程中注入噪声以及SR训练中使用的块鉴别器的必要性，我们在DPED数据集上进行了消融实验。我们首先介绍每个实验的设置。</p><ul><li>Bicubic: 在这个设置下，我们采用双三次插值核对高分辨率图像进行下采样，然后直接使用这些成对数据来训练SR模型。没有核估计和噪声注入，这个设置保持其他参数为默认值，可以理解为在真实数据集上微调ESRGAN以验证其鲁棒性。</li><li>Noise: 在双三次插值的基础上添加噪声注入。因为没有使用核估计方法，可以通过与提出的完整方法相比较来验证核估计的有效性。</li><li>Kernel: 只使用核估计方法，但没有明确添加噪声，因此可以用于观察噪声注入对结果的影响。</li><li>VGG-128: 如第3.5节所述，此设置使用默认的VGG-128判别器。</li><li>Patch: 此设置使用更轻的块鉴别器，与前面四个设置进行比较，以验证我们的结论。</li></ul><p>接下来，我们展示三个比较分析，以验证三个提出的组件的有效性。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610115028.png" alt="Img"><br>&emsp;&emsp;<strong>核估计的效果</strong> 从图5中可以看出，与“Noise”相比，“Patch”生成的结果更清晰。这证明了核估计在SR训练中的重要性，有助于SR模型产生更锐利的边缘。<br>&emsp;&emsp;<strong>噪声注入的效果</strong> 在这个比较实验中，我们将噪声注入设置为一个选项，以验证噪声注入是否必要。从图5中可以看出，没有明确的噪声注入，“Kernel”的结果有很多伪影，非常类似于在干净数据上训练的ESRGAN结果。注入的噪声与原始噪声分布一致，从而确保SR模型在测试过程中对噪声具有鲁棒性。<br>&emsp;&emsp;<strong>块鉴别器的效果</strong> 在真实数据上，我们使用块鉴别器替换VGG-128。将“Patch”与“VGG-128”进行比较，我们发现具有过大感受野的VGG-128鉴别器会导致不真实的纹理，部分与原始图像相矛盾。相比之下，块鉴别器还原了重要的边缘特征，并避免了不愉快的伪影，从而生成更真实的细节。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSR.md/img-20230610115035.png" alt="Img"></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>&emsp;&emsp;本文提出了一种基于核估计和噪声注入的新型降级框架RealSR。通过使用不同组合的降级（例如模糊和噪声），我们获取了与真实图像具有共同域的LR图像。利用这些域一致的数据，我们使用块鉴别器训练了一个真实图像超分辨率GAN，可以产生更好的感知HR结果。在合成噪声数据和真实世界图像上的实验证明，我们的RealSR优于现有的最先进方法，结果具有更低的噪声和更好的视觉质量。此外，我们的RealSR还是NTIRE 2020真实世界超分辨率挑战赛两个赛道的优胜者，其在人类感知方面大大优于其他方法。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>RealSRGAN</title>
      <link href="/2023/08/14/RealSRGAN/"/>
      <url>/2023/08/14/RealSRGAN/</url>
      
        <content type="html"><![CDATA[<h1 id="RealSRGAN-CVPR2020"><a href="#RealSRGAN-CVPR2020" class="headerlink" title="RealSRGAN         (CVPR2020)"></a>RealSRGAN         (CVPR2020)</h1><h1 id="Real-World-Super-Resolution-using-Generative-Adversarial-Networks"><a href="#Real-World-Super-Resolution-using-Generative-Adversarial-Networks" class="headerlink" title="Real-World Super-Resolution using Generative Adversarial Networks"></a>Real-World Super-Resolution using Generative Adversarial Networks</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;鲁棒的真实世界超分辨率(SR)旨在从相应的低分辨率(LR)图像中生成感知导向的高分辨率(HR)图像，而不需要访问配对的LR-HR地面实况。本文研究如何推进真实世界SR的技术水平。我们的方法涉及部署一系列生成对抗网络(GANs)用于鲁棒的真实世界SR。该集合使用不同的GANs训练不同的对抗目标。由于缺乏关于地面实况模糊和噪声模型的知识，我们设计了一个通用的训练集，其中LR图像由一组HR图像通过各种退化模型生成。我们通过超分辨率处理由未知图像处理伪影引起的LR图像，实现了良好的感知质量。对于在移动设备上捕获的真实世界SR，GANs通过弱监督的移动SR训练集进行训练，该训练集由DPED数据集构建，该数据集提供了以相同比例注册的移动设备-单反相机图像对。我们的GANs集合利用图像亮度的线索，并进行调整以在低照度下生成更好的HR图像。在NTIRE 2020真实世界超分辨率数据集上的实验表明，我们提出的SR方法实现了良好的感知质量。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>&emsp;&emsp;图像超分辨率(SR)通过试图恢复缺失的信息，从给定的低分辨率(LR)图像生成高分辨率(HR)图像。SR方法已经在许多计算机视觉应用中得到应用，例如监视、人脸和虹膜识别以及医学图像处理。最近，深度卷积神经网络(CNNs)已被部署来解决图像超分辨率问题，因为它们展示了显著的精度提高。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619091258.png" alt="Img"><br>&emsp;&emsp;由于缺乏真实世界LR-HR图像块，在大多数先前的方法中，图像通过双三次降采样创建LR-HR训练对。这导致LR图像干净且无噪声。不幸的是，在实际应用中，图像直接来自相机，在其中将始终存在额外的噪声或未知退化[18]。因此，仅训练用于重建使用双三次降采样人为降采样的图像的最先进CNN方法，当应用于真实世界图像时可能会导致严重的伪影。为了解决这个问题，一些研究人员使用不同焦距的数字单镜反光(DSLR)相机拍摄图像[3]，并通过某些配准算法进一步对齐它们。然而，DSLR成像系统仍然与常用于真实世界的移动成像系统不同。此外，捕获在不同比例之间的图像的对齐相对困难。<br>&emsp;&emsp;本文从以下几个方面研究了真实世界超分辨率问题。首先，我们基于图像处理伪影的合理假设，采用多种降级方法对高质量HR图像进行多次降级，创建了一个通用的训练集，包括不同的降采样方法、不同的模糊核和不同的噪声等。我们展示了对于没有任何确切降级模型知识的图像，先前在配对SR数据集上表现良好的生成对抗网络(GAN)能够在训练我们的新数据集后表现良好，并且具有良好的泛化能力。其次，我们提出了一种弱监督的方法来训练GAN，用于超分辨率处理真实世界移动图像。在没有任何关于移动HR-LR降级模型的知识的情况下，我们通过从DPED数据集[11]提供的相同比例的注册移动-DSLR图像生成配对的LR-HR图像，创建了一个移动SR数据集。我们将移动图像作为LR，并将我们通过通用训练集训练的SR模型应用于配对的DSLR图像，以创建具有良好感知质量的超分辨率HR图像。在对这些LR-HR对进行微调后，当在移动图像上进行测试时，我们观察到明显的性能提高，如图1所示。在NTIRE 2020真实世界SR挑战赛第2轨道图像上进行测试时，与基于DPED数据集训练的最先进真实世界SR方法ESRGAN-FS[8]相比，我们的SR方法实现了明显更好的感知质量。第三，我们研究了基于不同GAN网络的融合，以提高所得SR图像的整体感知质量。<br>&emsp;&emsp;本文的主要贡献如下:</p><ul><li>我们的通用SR模型在通过多种降级方式生成的SR数据集上训练，对于由图像处理伪影引起的未知降级的图像具有良好的泛化能力。</li><li>我们设计了一个基于注册的移动-DSLR图像对于相同比例的移动SR数据集，其中DSLR图像使用我们的通用SR模型进行超分辨率处理。在该数据集上微调我们的SR模型可以提高移动图像的感知质量。</li><li>我们基于GAN的融合能够提高估计的HR图像的感知质量并减少伪影。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h3 id="2-1-PSNR-oriented-super-resolution"><a href="#2-1-PSNR-oriented-super-resolution" class="headerlink" title="2.1. PSNR-oriented super-resolution"></a>2.1. PSNR-oriented super-resolution</h3><p>&emsp;&emsp;在过去的几年中，许多研究致力于解决图像超分辨率问题。其中大多数仍然基于人工合成的双三次降采样退化，并以PSNR为基准进行评估。一些早期的SR算法采用滤波方法，如双线性、双三次和Lanczos滤波[7]。这些滤波算法可能会生成平滑的输出，而不恢复任何高频信息。它们计算效率高，但准确性受限，因为它们过于简化了SR问题。其他方法假定LR空间和HR空间之间存在某种映射关系。这种映射可以通过基于稀疏编码的图像表示[30][28]从大量LR-HR对中学习，其中一个稀疏系数向量在LR空间和HR空间之间共享。最近，CNN已被广泛用于图像超分辨率。在[6]中，Dong等人设计了一个3层CNN，学习双三次上采样的LR空间和相应HR空间之间的映射关系。为了进一步提高准确性，提出了更复杂的网络。Kim等人[12]将层数增加到20，并使用小滤波器和可调节梯度剪切的高学习率。Kim等人[13]建议使用具有跳跃连接的深度递归网络，其中多个卷积层共享相同的权重。Tai等人[26]进一步将递归结构集成到残差网络中。Dahl等人[5]将ResNet与像素递归超分辨率相结合，显示了在人脸和床上的SR上有前景的结果。Lim等人[16]通过删除传统残差网络中不必要的模块并扩大模型规模，设计了一个增强的深度超分辨率网络(EDSR)。Tai等人[27]提出了一种非常深的持久性记忆网络(MemNet)，引入了一个记忆块，由递归单元和门控单元组成，通过自适应学习过程明确挖掘持久性记忆。Haris等人[10]提出了深度反投影网络，利用迭代的上采样和下采样层，在每个阶段提供投影误差的误差反馈机制。Zhang等人[34]提出了非常深的残差通道注意力网络(RCANs)，具有残差在残差(RIR)结构和通道注意力机制，以自适应地重新缩放通道特征。</p><h3 id="2-2-Perceptual-oriented-super-resolution"><a href="#2-2-Perceptual-oriented-super-resolution" class="headerlink" title="2.2. Perceptual-oriented super-resolution"></a>2.2. Perceptual-oriented super-resolution</h3><p>&emsp;&emsp;PSNR并不一致于人类视觉，这意味着具有更好PSNR的SR网络可能导致感知质量较差[2]。为解决这个问题，Ledig等人[15]采用了一个非常深的残差网络，并进一步提出了超分辨率生成对抗网络(SRGAN)，以获得纹理类似于自然纹理的HR图像。王等人[29]通过引入无批量归一化的残差-残差密集块（RRDB）作为基本的网络构建单元，并使用相对论GAN让鉴别器预测相对真实性来改进SRGAN。但在现实世界的超分辨率中，由于未知的降级和缺乏训练数据，假设双三次降级的SR方法仍然效果不佳。最近，已经引入了几种方法来进行实际低分辨率图像的超分辨率。张等人[31]在训练和测试时除了输入LR图像外，还提供了与模糊和噪声相关的额外信息，以提高SR模型在不同降级上的性能。然而，它明确假定了降级的知识。为了将这种多重降级方法扩展到输入图像的降级未知的盲SR，顾等人[9]使用了一个预测网络来估计模糊核，以及一个校正网络来减少主SR网络输出的伪影。周等人[35]试图通过利用现有的模糊核估计算法[20]来估计真实LR图像中的相机模糊。然后使用估计的模糊核来生成合成的LR-HR图像对，以训练SR网络。Lugmayr等人[17]从一个未配对的LR-HR图像对数据集开始，训练CycleGAN网络将双三次降采样的HR图像转换为代表实际图像特征定义的LR图像域。他们使用使用这种方法生成的LR-HR图像对来训练基于GAN的SR网络。在[8]中，通过将双三次降采样域与源LR域之间的转换建模为一个GAN网络，将双三次降采样图像中的低频和高频分离，并使用高频分量仅训练GAN的鉴别器来提高GAN的训练。Bell等人[1]提出了一个盲SR算法，使用无监督方法来估计输入LR图像的SR核。然后可以将估计的SR核插入现有的SR算法中，以实现盲SR的高质量结果。</p><h2 id="3-Proposed-Method-for-Real-World-SR"><a href="#3-Proposed-Method-for-Real-World-SR" class="headerlink" title="3. Proposed Method for Real-World SR"></a>3. Proposed Method for Real-World SR</h2><p>&emsp;&emsp;我们的实际超分辨率方法包括三个步骤。第一步是生成实际超分辨率数据集。第二步是训练不同的生成对抗网络（GAN）用于生成超分辨率图像。更具体地说，我们遵循Enhance Super-resolution Generative Adversarial Networks (ESRGAN) [29]框架，使用Residual Channel Attention Network (RCAN) [34]作为生成器，而不是Residual-in-Residual Dense Block (RRDB)。在测试期间，生成器（RCAN）直接用于估计给定LR图像的HR图像。为了进一步提高感知质量，我们使用ESRGAN中的不同鉴别器和超参数训练了两个RCAN。这使我们得到了两个具有互补特性的SR网络。第三步是为训练的SR-GAN的结果设计一种集成策略。我们的最终SR预测是这两个RCAN的像素级集成，如图2所示。</p><h3 id="3-1-Dataset-construction-for-real-world-SR"><a href="#3-1-Dataset-construction-for-real-world-SR" class="headerlink" title="3.1. Dataset construction for real-world SR"></a>3.1. Dataset construction for real-world SR</h3><p>&emsp;&emsp;我们生成了两个不同的数据集，第一个数据集是用于通用的实际超分辨率，对于LR图像的来源没有任何假设。第二个数据集是用于实际超分辨率，当已知LR图像是由移动电话相机拍摄时。</p><h4 id="3-1-1-Synthetic-dataset-generation-for-robust-real-world-SR"><a href="#3-1-1-Synthetic-dataset-generation-for-robust-real-world-SR" class="headerlink" title="3.1.1 Synthetic dataset generation for robust real-world SR"></a>3.1.1 Synthetic dataset generation for robust real-world SR</h4><p>&emsp;&emsp;为了训练一个具有强鲁棒性的SR网络，而不需要先前关于HR-LR降级模型的知识，我们根据图像处理伪影的多个降级构建了一个SR数据集。更具体地说，我们按照通用降级模型从HR图像y生成LR图像x，该模型定义为：<br>$$x&#x3D;\mathcal{N}\left(D(y*k)\right),\tag{1}$$<br>&emsp;&emsp;其中，$D$是下采样操作，$k$是模糊核，$∗$表示卷积，$N(t)$对输入$t$应用噪声，其中噪声模型不一定是加性的。<br>&emsp;&emsp;<strong>下采样</strong> 我们考虑了多种方法，包括最近邻、双线性、双三次和Lanczos。在生成LR补丁时，下采样方法是随机选择的。<br>&emsp;&emsp;<strong>模糊核</strong> 不同于图像去模糊，SR的模糊核通常很简单。我们使用最常用的各向同性高斯模糊核，由标准差参数化。在我们的实现中，我们在[0.2,3]的范围内随机采样高斯核的标准差，并将核大小固定为15×15。<br>&emsp;&emsp;<strong>噪声</strong> 大多数实际LR图像由于某些图像处理伪影而带有噪声。在[22]中表明，一些实际噪声由高斯、泊松或泊松高斯成分组成。因此，我们在生成LR图像时随机选择高斯、泊松和泊松高斯噪声。参数基于实际图像处理伪影的合理假设，其中高斯噪声的标准差从范围[0,25]中随机选择，泊松噪声的峰值从范围[50,150]中均匀采样。在生成泊松高斯噪声时，我们遵循[22]中的过程。更具体地说，我们将泊松高斯噪声生成为：<br>$$N(t)&#x3D;\alpha p+n,\tag{2}$$<br>其中，$n$是独立同分布的高斯噪声，$p∼P（t）$是具有均值$t$的泊松随机变量。我们使用类似于[50,150]的泊松峰值范围，但将高斯标准差范围减小到[0,5]。常数$α$设置为1。<br>&emsp;&emsp;上述方法生成的LR图像示例如图3所示，展示了构建的SR训练集中降级的多样性。</p><h4 id="3-1-2-Synthetic-dataset-generation-for-mobile-real-world-SR"><a href="#3-1-2-Synthetic-dataset-generation-for-mobile-real-world-SR" class="headerlink" title="3.1.2 Synthetic dataset generation for mobile real-world SR"></a>3.1.2 Synthetic dataset generation for mobile real-world SR</h4><p>&emsp;&emsp;在实际超分辨率中，匹配目标成像设备领域的LR-HR图像高质量数据集对于SR网络的性能至关重要。已经有一些工作集中于创建实际超分辨率的数据集。在[3]中，数据是使用尼康和佳能两种DSLR相机模型捕获的。作者使用不同的焦距拍摄图像，得出HR图像和相应的具有不同缩放因子的LR图像。然后将得到的LR-HR图像进行配准，创建最终的配对SR数据集[3]。然而，这些DSLR图像在实际超分辨率场景中常用的移动图像上的泛化效果不好。在[4]中，SR数据集分别由手机和DSLR相机拍摄。但由于来自不同尺度和不同领域的图像配准的困难，这些移动图像与DSLR图像不对齐。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619093639.png" alt="Img"><br>&emsp;&emsp;为了缓解现有方法的这些缺点，我们提出了一种创建高质量移动SR数据集的有效方法。其灵感来源于从同一比例拍摄的不同领域的图像进行配准比从不同比例进行配准更容易。更具体地说，我们提议使用在同一比例下配准的图像对创建新的移动SR数据集，而无需知道确切的HR-LR降级模型。为此，我们使用DSLR照片增强数据集（DPED）[11]，该数据集为图像增强提供了在同一比例下的注册移动-DSLR补丁对。我们通过对DSLR补丁进行超分辨率处理来创建移动SR数据集，以创建HR图像。我们提议数据集中的LR图像是相应的移动补丁。我们观察到，即使对DSLR补丁应用简单的双三次上采样算法来进行超分辨率，也可以生成用于训练移动SR网络的高质量HR补丁。我们进一步使用在3.1.1节中描述的通用数据集训练的SR模型来超分辨率这些DSLR补丁。与基于双三次上采样的数据生成相比，在训练SR网络后，这给我们带来了视觉质量的明显提高。更多细节可以在4.3.2节中找到。<br>&emsp;&emsp;我们展示了使用所提出的移动SR数据集微调GAN网络将产生高质量的输出，当应用于超分辨率实际移动LR图像时。我们的方法实际上遵循一种弱监督方式，因此SR输出的感知质量比没有使用成对数据的先前技术[8]要好得多。我们提出的技术还使得超分辨率移动图像进行数字变焦的实际应用更加可行。</p><h3 id="3-2-Super-resolution-GANs"><a href="#3-2-Super-resolution-GANs" class="headerlink" title="3.2. Super-resolution GANs"></a>3.2. Super-resolution GANs</h3><h4 id="3-2-1-RCAN"><a href="#3-2-1-RCAN" class="headerlink" title="3.2.1 RCAN"></a>3.2.1 RCAN</h4><p>&emsp;&emsp;在[34]中提出了残差通道注意力网络（RCAN）。RCAN基于残差内残差（RIR）结构，它由具有长跳连接的多个残差组成。每个残差组包含一些具有短跳连接的残差块（ResBlock）。在每个ResBlock中，利用通道注意力机制通过考虑通道间的相互依赖关系来自适应地重新缩放通道特征。我们在训练GAN网络时使用RCAN作为生成器。</p><h4 id="3-2-2-ESRGAN"><a href="#3-2-2-ESRGAN" class="headerlink" title="3.2.2 ESRGAN"></a>3.2.2 ESRGAN</h4><p>&emsp;&emsp;众所周知，像素级PSNR导向的SR方法通常会产生过度平滑的结果，并且无法恰当地恢复高频细节[14]。SRGAN [14]利用GAN网络模拟自然图像空间的优势，并使用感知和对抗损失来指导SR网络，使其倾向于生成与自然图像流形相符的输出图像。之后，与SRGAN中感知驱动、基于GAN的方法相关的几个修改被引入[29, 33, 25]。我们遵循ESRGAN [29]框架，因为它使用相对论鉴别器，能够产生更锐利的边缘和更逼真的纹理细节。<br>&emsp;&emsp;我们使用ESRGAN和RCAN生成器来训练我们的SR网络。在训练过程中，我们的生成器损失函数$L^R_G$包括$L_1$图像损失、感知损失$L_p$和对抗损失$L^R_a$，与[29]类似，如公式3所述。<br>$$L_G^R&#x3D;L_p+\lambda L_a^R+\eta L_1,\tag{3}$$<br>其中，$L_1&#x3D;\mathbb{E}<em>x[|G(x)-y|<em>1]$计算了RCAN生成器$G(.)$生成的超分辨率图像$G(x)$与高分辨率真实图像$y$之间的$L_1$距离。$\mathbb{E}<em>x[.]$表示对小批量中的所有图像进行平均操作。感知损失$L_p$使用预训练的19层VGG网络计算$G(x)$和$y$之间的特征图距离。我们将从RCAN生成器网络$G$中输出的图像表示为$x_f &#x3D; G(x)$，即假图像，相应的真实图像为$x_r$。对抗损失$L^R_a$基于相对论GAN鉴别器[29]，定义如下：<br>$$\begin{gathered}<br>L</em>{a}^{R}&#x3D;-\mathbb{E}</em>{x</em>{r}}[\log(1-D_{R}(x_{r},x_{f})] \<br>-\mathbb{E}_{x_f}[\log(D_R(x_f,x_r)],<br>\end{gathered}\tag{4}$$<br>其中，鉴别器$D_R$判断真实图像$x_r$是否比假图像$x_f$更逼真，而不是决定输入图像是绝对真实还是假的。<br>&emsp;&emsp;公式3中的超参数$λ$和$η$决定了不同损失组件在最终损失函数中的贡献。可以增加参数$η$以减少估计的定量误差，而增加对抗损失权重将导致结果的感知质量提高。此外，我们还使用RCAN生成器训练了另一个GAN，但使用基于标准GAN [14]的不同生成器损失函数$L^S_G$。<br>$$L_G^S&#x3D;L_p+\lambda L_a^S+\eta L_1,\tag{5}$$<br>其中，$L^S_a$是基于标准GAN的对抗损失。我们观察到，由公式3和公式5中的损失函数训练的SR估计具有一些互补的特征。我们通过一种简单而有效的融合策略利用这些不同的特征来提高图像的整体视觉质量，如下一节所述。</p><h3 id="3-3-Ensemble-fusion-of-SR-GANs"><a href="#3-3-Ensemble-fusion-of-SR-GANs" class="headerlink" title="3.3. Ensemble fusion of SR-GANs"></a>3.3. Ensemble fusion of SR-GANs</h3><p>&emsp;&emsp;我们观察到，通过公式3中的相对论GAN损失训练的RCAN生成的SR输出在高频区域显示出很好的感知质量。相比之下，通过公式5中的标准GAN损失训练的RCAN生成的SR输出在一些低照度图像的平滑区域生成较少的伪影。受现有像素级融合工作[24, 21, 23]的启发，我们提出了一种使用两个RCAN生成器的SR估计的融合方法。我们选择基于图像中所有像素的中位亮度的选择性平均技术，以提高低照度图像的视觉质量。我们将使用相对论GAN损失函数训练的RCAN模型的HR输出表示为$y^R_SR$，将使用标准GAN损失函数训练的RCAN模型的HR输出表示为$y^S_SR$。融合的输出图像$y^{fused}<em>{SR}$通过以下方式得到：<br>$$y</em>{SR}^{fused}&#x3D;\left{\begin{matrix}\alpha y_{SR}^R+\beta y_{SR}^S&amp;if&amp;Y_{med}&lt;\gamma\ y_{SR}^R&amp;otherwise\end{matrix}\right.\tag{6}$$<br>其中，$Y_{med}$是yR_SR的YCbCr色彩空间表示中Y（亮度）分量的所有像素强度值的中位数。我们的融合框架与[29]中的基于GAN的图像插值的不同之处在于，我们融合了基于不同对抗损失训练的两个GAN模型，具有不同的互补效果，以确保用于融合的两个图像的感知质量接近。这确保了我们可以在不牺牲整体感知质量的情况下减少某些低照度图像区域中的伪影。相比之下，[29]中的图像插值使用了面向PSNR和基于GAN的SR估计，可能会降低融合图像的整体感知质量。</p><h2 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4. Experimental Results"></a>4. Experimental Results</h2><h3 id="4-1-Training-on-NTIRE-2020-datasets"><a href="#4-1-Training-on-NTIRE-2020-datasets" class="headerlink" title="4.1. Training on NTIRE 2020 datasets"></a>4.1. Training on NTIRE 2020 datasets</h3><p>&emsp;&emsp;我们使用NTIRE 2020真实世界超分辨率[19]（RealSR）挑战赛的数据集来评估我们的方法。NTIRE 2020 RealSR挑战旨在刺激对真实世界超分辨率的研究，其中降级是未知的，并且给出了不成对的数据。在这种设置中，不存在可以直接用于训练的基准参考图像。相反，模型需要仅从一组源域图像中学习，这些图像来自特定相机传感器的源。NTIRE 2020 RealSR挑战包含两个轨道，研究不同类型的源域。这两个轨道都提供来自源域和目标域的不成对图像。这两个轨道的目标域是同一组高质量的清晰图像。在轨道1中，源域包含由某些去噪算法产生的带有伪影的图像。相比之下，在轨道2中，源域包含由智能手机图像增强操作产生的带有伪影的图像。这两个轨道的降级是不同的。这两个轨道的缩放因子均为x4。<br>&emsp;&emsp;我们将我们的SR方法应用于NTIRE RealSR数据集。对于轨道1，我们使用ESRGAN在第3.1.1节中描述的通用SR训练集上训练我们的RCAN。当生成我们的通用SR数据集时，我们使用由轨道1训练数据提供的800个目标域图像，以及来自Flickr2K数据集的额外2650个HR图像，生成带有随机退化的LR图像。退化参数是随机选择的，包括下采样方法、模糊核和噪声。当使用相对论GAN训练RCAN时，公式3中损失函数的权重设置为$λ&#x3D;0.005，η&#x3D;0.01$。当使用标准GAN训练RCAN时，公式5中损失函数的权重设置为$λ&#x3D;0.005，η&#x3D;0.005$。最终的SR输出是两个RCAN的集成，如第3.3节所述，其中融合参数设置为$α&#x3D;0.6，β&#x3D;0.4，γ&#x3D;64$。这些融合参数通过在轨道1验证图像上在平滑区域中减少伪影和在纹理区域中保持锐度之间取得良好平衡来进行调整。<br>&emsp;&emsp;对于轨道2，我们按照第3.1.2节中的说明生成我们的移动SR数据集。我们使用DPED训练集[11]提供的160K个带有“iphone”标签的移动补丁作为LR图像，并在相应的对齐DSLR补丁上应用我们的轨道1 SR模型生成x4超分辨率的HR补丁。我们在这个移动SR数据集上微调在轨道1中使用的RCAN，作为我们的轨道2 SR模型，使用相同的损失函数权重。最终的输出仍然是微调后的RCAN的集成，具有相同的融合参数。</p><h3 id="4-2-Results"><a href="#4-2-Results" class="headerlink" title="4.2. Results"></a>4.2. Results</h3><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619102129.png" alt="Img"><br>&emsp;&emsp;对于轨道1，我们使用常用的PSNR和SSIM指标以及感知度量LPIPS [32]，定量评估我们提出的算法在验证数据上的性能。表1总结了不同SR方法在轨道1验证数据上的数值比较。可以看出，我们在通用SR数据集上训练的网络相对于在双三次降采样下训练的网络具有明显更好的准确性。我们的SR解决方案达到了最低的LPIPS，这意味着我们具有最好的感知质量，如图4所示。当在具有未知退化的轨道1图像上进行测试时，基于双三次采样的标准ESRGAN [29]仍然存在许多噪声。与在相同通用SR训练集上训练PSNR导向的RCAN相比，我们实现了更好的感知质量，尽管PSNR和SSIM较低。这与感知导向的GAN更适合于真实世界超分辨率的事实一致。<br>&emsp;&emsp;对于轨道2，由于我们无法获取基准参考图像，因此我们进行定性分析来验证我们方法的有效性。图5比较了我们提出的算法与一些现有的SR方法在应用于轨道2的测试图像时的效果。我们发现，相对于基于双三次降采样训练的标准ESRGAN [29]以及同样训练于DPED数据集上的最先进的真实世界SR算法ESRGAN-FS [8]，我们的算法能够产生更高质量的图像。ESRGAN-FS [8]将DPED移动图像视为HR，并使用下采样GAN生成LR图像。然后使用频率分离的ESRGAN（ESRGAN-FS）来训练这些LR图像。相比之下，我们使用具有相同尺度的移动-DSLR图像对，而没有准确的HR-LR退化知识。因此，我们的解决方案是一种弱监督方法。性能明显优于ESRGAN-FS算法，后者采用无监督的数据生成方式。这些结果验证了使用我们通过对齐DSLR图像进行超分辨率生成的移动SR数据集的有效性。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619102146.png" alt="Img"></p><h3 id="4-3-Ablation-study"><a href="#4-3-Ablation-study" class="headerlink" title="4.3. Ablation study"></a>4.3. Ablation study</h3><h4 id="4-3-1-Using-fusion-on-the-SR-outputs"><a href="#4-3-1-Using-fusion-on-the-SR-outputs" class="headerlink" title="4.3.1 Using fusion on the SR outputs"></a>4.3.1 Using fusion on the SR outputs</h4><p>&emsp;&emsp;在本节中，我们对我们的融合方法进行了消融研究。图6展示了在轨道1测试图像上应用或不应用融合的一些SR输出（如果未应用融合，则仅可视化通过相对论GAN训练的RCAN的SR输出）。可以看出，我们能够通过使用所提出的融合算法来减少低照度图像中的不良伪影，同时避免失去锐度。这证明了我们提出的融合算法对于增强图像的整体视觉质量的有效性。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619102255.png" alt="Img"></p><h4 id="4-3-2-Different-ways-to-generate-mobile-SR-dataset"><a href="#4-3-2-Different-ways-to-generate-mobile-SR-dataset" class="headerlink" title="4.3.2 Different ways to generate mobile SR dataset"></a>4.3.2 Different ways to generate mobile SR dataset</h4><p>&emsp;&emsp;基于注册的移动-DSLR图像对创建我们的移动SR数据集有多种方法，如表2所示。我们可以保持相同尺度的移动图像并超分辨率DSLR图像（T1、T2、T3），或将移动图像下采样并保持相同尺度的DSLR图像。T1、T2和T3之间的差异在于如何超分辨率HR图像，可以通过简单的双三次插值（T1）、应用感知导向的SR模型，如我们的轨道1 RCAN（T2），或者PSNR导向的SR模型（T3，与表1中的PSNR导向的RCAN相同）来实现。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619102337.png" alt="Img"><br>&emsp;&emsp;在图7中，我们比较了应用T1、T2、T3和T4在DPED数据集上训练的不同SR网络生成的轨道2测试集的SR输出。可以看出，T2给出了最好的感知质量。原因是T2训练中使用的HR图像具有比T1、T3和T4更好的感知质量。T3看起来比T1稍微好一些，因为PSNR导向的SR可能会生成比简单的双三次上采样更好的HR补丁。T4仍然显示一些伪影，因为T4中的LR图像实际上被模糊了两次，一次是由于下采样退化，另一次是由于移动相机。相比之下，NTIRE RealSR轨道2的测试图像只被移动相机模糊了一次。T4的输出伪影来自于这种模型不匹配问题。<br><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/RealSRGAN.md/img-20230619103706.png" alt="Img"></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h2><p>&emsp;&emsp;本文讨论了针对真实世界超分辨率有用的三个问题。首先，我们使用多种HR-LR降采样模型构建了一个通用的SR数据集。我们还展示了基于GAN的通用SR模型可以成功地使用我们的通用SR数据集进行鲁棒的真实世界SR训练。其次，我们提出了一种新颖的弱监督SR方案，以从由真实移动相机拍摄的图像中生成更高分辨率的图像。使用注册的移动-DSLR图像创建移动SR数据集，其中移动图像用作LR，超分辨率的DSLR图像用作HR。可以通过在移动SR数据集上微调我们的通用SR模型来改善移动图像的感知质量。第三，我们证明了不同GAN的集合可以减少低照度图像的伪影，这有助于整体感知质量。在NTIRE 2020 RealSR挑战数据集上的实验结果表明了我们方法的有效性。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>USRNET</title>
      <link href="/2023/08/14/USRNet/"/>
      <url>/2023/08/14/USRNet/</url>
      
        <content type="html"><![CDATA[<h1 id="USRNet"><a href="#USRNet" class="headerlink" title="USRNet"></a>USRNet</h1><p>本工作的主要贡献如下：<br>1）提出了一种端到端可训练的展开超分辨率网络（USRNet）。USRNet是第一个尝试通过单个端到端训练的模型处理具有不同缩放因子、模糊核和噪声级别的经典退化模型的方法。<br>2）USRNet将基于模型的方法的灵活性和基于学习的方法的优点结合起来，提供了一种弥合基于模型和基于学习方法之间差距的途径。<br>3）USRNet在解决方案上本质上施加了退化约束（即估计的HR图像应符合退化过程）和先验约束（即估计的HR图像应具有自然特征）。<br>4）USRNet在具有不同退化设置的LR图像上表现良好，显示出在实际应用中具有巨大潜力的特点。</p><p>超参数模块充当一个“滑杆”，用于控制数据模块和先验模块的输出。例如，随着$\alpha_k$的增加，解$z_k$会逐渐接近$x_{k-1}$。根据$\alpha_k$和$\beta_k$的定义，$\alpha_k$由$\sigma$和$\mu_k$确定，而$\beta_k$取决于$\lambda$和$\mu_k$。虽然可以学习一个固定的$\lambda$和$\mu_k$，但我们认为，如果$\lambda$和$\mu_k$与影响不适定程度的两个关键元素，即缩放因子$s$和噪声水平$\sigma$有关，那么可以获得性能提升。我们使用一个单独的模块来预测$\alpha$和$\beta$，其中$\alpha&#x3D;[\alpha_1,\alpha_2,…,\alpha_K]$和$\beta&#x3D;[\beta_1,\beta_2,…,\beta_K]$。</p><p>超参数模块由三个全连接层组成，其中前两个激活函数为ReLU，最后一个激活函数为Softplus [19]。每层的隐藏节点数为64。考虑到$\alpha_k$和$\beta_k$应该是正数，且公式（7）应该避免除以极小的$\alpha_k$，因此输出的Softplus层后面跟随着额外的1e-6加法。在第4.4节中，我们将展示缩放因子和噪声水平如何影响超参数。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>TRANSFOMER</title>
      <link href="/2023/08/14/Transfomer/"/>
      <url>/2023/08/14/Transfomer/</url>
      
        <content type="html"><![CDATA[<h1 id="Transfomer"><a href="#Transfomer" class="headerlink" title="Transfomer"></a>Transfomer</h1><p>Seq2Seq：满足输入序列生成输出序列的模式。</p><p>Encoder Decoder模型的特点：</p><ul><li>Encoder将可变长度的输入序列编码成一个固定长度的向量；</li><li>Decoder将固定长度的向量解码成一个可变长度的输出序列；</li><li>Encoder-Decoder阶段的编码与解码的方式可以是CNN、RNN、LSTM、GRU等；</li></ul><p>主流的序列转录模型:包括一个 encoder 和 一个 decoder的 RNN 或者 CNN 架构。<br>表现好的序列转录模型：用了 attention<br>表现好的sequence transduction 模型在 encoder 和 docoder 之间使用了 attention。<br>本文提出 基于attention 的Transformer</p><p>Transformer 变相金刚 的贡献：简单 simple (褒义词)，跟之前的（表现好的）循环 or 卷积架构不一样。 </p><p>RNN特点：给定一个序列，对序列的第$t$个元素，它会计算一个输出$h_t$,也叫$t$的隐藏状态，$h_t$是由前一个元素的隐藏状态$h_{t-1}$和当前第$t$个元素本身所决定的。<br>RNN缺点：</p><ul><li>时许化计算，当计算第$h_t$时，必须保证第$t-1$个元素的$h_{t-1}$已经计算出来，计算难以并行，计算性能差。</li><li>由于历史信息是一步一步向后传递，若序列比较长的话，早期的历史信息可能丢掉，如果不想丢掉，就需要做一个比较大的$h_t$,这样内存开销很大。</li></ul><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p><img src="/../assets/%E8%AE%BA%E6%96%87/FILES/Transfomer.md/img-20230428095443.png" alt="Img"><br>为什么不用BatchNorm:由于序列长度不一，每次计算均值和方差差异较大。<br>LayerNorm: 每个样本自己来算均值和方差，相对来说稳定一些。</p>]]></content>
      
      
      <categories>
          
          <category> 论文 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>动态内存管理</title>
      <link href="/2023/08/14/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2023/08/14/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="动态内存管理"><a href="#动态内存管理" class="headerlink" title="动态内存管理"></a>动态内存管理</h1><h2 id="栈内存和堆内存"><a href="#栈内存和堆内存" class="headerlink" title="栈内存和堆内存"></a>栈内存和堆内存</h2><ul><li><p>栈内存的特点：更好的局部性，对象自动销毁</p></li><li><p>堆内存的特点：运行期动态扩展，需要显式释放<br><img src="/../assets/Code/FILES/%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.md/img-20230727200050.png" alt="Img"></p></li><li><p>在C++中通常使用new与delete来构造、销毁对象</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.out &amp;&amp; $tmpFile.out</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span>* <span class="title">fun</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> *res = <span class="keyword">new</span> <span class="built_in">int</span>(<span class="number">2</span>);</span><br><span class="line">    <span class="keyword">return</span> res; <span class="comment">//存活周期更长</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span>* <span class="title">fun_1</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> res = <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">return</span> &amp;res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> x;</span><br><span class="line">    x = <span class="number">2</span>;      <span class="comment">//栈内存</span></span><br><span class="line">    cout&lt;&lt; x &lt;&lt; endl;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* y = <span class="keyword">new</span> <span class="built_in">int</span>(<span class="number">3</span>);    <span class="comment">//堆内存</span></span><br><span class="line">    cout&lt;&lt; *y &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">delete</span> y;   <span class="comment">//显示释放</span></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* z = <span class="built_in">fun</span>();</span><br><span class="line">    cout&lt;&lt; *z &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">delete</span> z;   <span class="comment">//显示释放</span></span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* x_1 = <span class="built_in">fun_1</span>();</span><br><span class="line">    cout&lt;&lt;*x_1&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>对象的构造分成两步：分配内存与在所分配的内存上构造对象；对象的销毁与之类似</p></li><li><p>new的几种常见形式</p><ul><li><p>构造单一对象&#x2F;对象数组</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 单一对象</span></span><br><span class="line">    <span class="type">int</span>* y = <span class="keyword">new</span> <span class="type">int</span>;</span><br><span class="line">    <span class="comment">// 对象数组</span></span><br><span class="line">    <span class="type">int</span>* z = <span class="keyword">new</span> <span class="type">int</span>[<span class="number">5</span>]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">    cout&lt;&lt; z[<span class="number">2</span>] &lt;&lt; endl;</span><br><span class="line">    <span class="keyword">delete</span>[] z; <span class="comment">//删除数组</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>nothrow new </p> <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;new&gt;</span></span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"> <span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"> </span>&#123;</span><br><span class="line">     <span class="type">int</span>* z = <span class="built_in">new</span> (nothrow) <span class="type">int</span>[<span class="number">5</span>]&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">     <span class="keyword">if</span>(y == <span class="literal">nullptr</span>)</span><br><span class="line">         cout&lt;&lt; z[<span class="number">2</span>] &lt;&lt; endl;</span><br><span class="line">     <span class="keyword">delete</span>[] z; <span class="comment">//删除数组</span></span><br><span class="line"></span><br><span class="line">     <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure></li><li><p>placement new</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span>* <span class="title">fun</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">char</span> ch[<span class="built_in">sizeof</span>(<span class="type">int</span>)];   <span class="comment">//栈内存</span></span><br><span class="line">    <span class="type">int</span>* y = <span class="built_in">new</span>(ch) <span class="built_in">int</span>(<span class="number">4</span>);    </span><br><span class="line">    <span class="keyword">return</span> y;   <span class="comment">//危险行为，栈内存在函数执行完会被回收</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">char</span> ch[<span class="built_in">sizeof</span>(<span class="type">int</span>)];   <span class="comment">//栈内存</span></span><br><span class="line">    <span class="type">int</span>* y = <span class="built_in">new</span>(ch) <span class="built_in">int</span>(<span class="number">4</span>);    <span class="comment">//使用已开辟的内存空间构造对象</span></span><br><span class="line">    cout&lt;&lt;*y&lt;&lt;endl;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>new auto</p>  <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span>* y = <span class="keyword">new</span> <span class="built_in">auto</span>(<span class="number">3</span>);</span><br><span class="line">    cout&lt;&lt;*y&lt;&lt;endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>new与对象对齐</p><figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> <span class="title function_">alignas</span><span class="params">(<span class="number">256</span>)</span> Str&#123;&#125;; <span class="comment">//按照256字节对齐，数据项仅仅能存储在地址是数据项大小的整数倍的内存位置上</span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    Str* ptr = new Str();</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; ptr&lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>delete 的常见用法</p><ul><li>销毁单一对象&#x2F;对象数组</li><li>placement delete</li></ul></li><li><p>使用new与delete的注意事项</p><ul><li>根据分配的是单一对象还是数组，采用相应的方式销毁</li><li>delete nullptr</li><li>不能delete一个非new返回的内存</li><li>同一块内存不能delete多次</li></ul></li><li><p>调整系统自身的new &#x2F; delete行为</p><ul><li>不要轻易使用</li></ul></li></ul><h2 id="智能指针"><a href="#智能指针" class="headerlink" title="智能指针"></a>智能指针</h2><ul><li>使用new与delete的问题：内存所有权不清晰，容易产生不销毁，多销毁的情况<figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span>* <span class="title function_">fun</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span>* res = new <span class="type">int</span>(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span>* y = fun();  <span class="comment">//内存所有权不清晰</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>C++的解决方案：智能指针<ul><li>auto_ptr（C++17删除）  </li><li>shared_ptr &#x2F; uniuqe_ptr &#x2F; weak_ptr</li></ul></li></ul><h3 id="shared-ptr——基于引用计数的共享内存解决方案"><a href="#shared-ptr——基于引用计数的共享内存解决方案" class="headerlink" title="shared_ptr——基于引用计数的共享内存解决方案"></a>shared_ptr——基于引用计数的共享内存解决方案</h3><ul><li>基本用法  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span> <span class="comment">//所需头文件</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// int* x(new int(3));</span></span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">x</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>))</span>;  <span class="comment">//1</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;x.use_count()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; y = x;          <span class="comment">//2,程序结束时先销毁y,引用计数减一</span></span><br><span class="line">    <span class="comment">//cout&lt;&lt;*x&lt;&lt;endl;</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;y.use_count()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; z = x;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;z.use_count()&lt;&lt;<span class="built_in">endl</span>; <span class="comment">//语句体结束，被删除，引用计数减1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;y.use_count()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">fun</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; res = new <span class="type">int</span>(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">auto</span> y = fun();  <span class="comment">//不存在内存泄露</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>reset &#x2F; get方法  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">fun</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">res</span><span class="params">(new <span class="type">int</span>(<span class="number">100</span>))</span>;</span><br><span class="line">    <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">fun2</span><span class="params">(<span class="type">int</span>*)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;<span class="string">&quot;fun2&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">auto</span> y = fun();  <span class="comment">//不存在内存泄露</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;*y&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;*(y.get())&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//fun2(y);类型不匹配无法调用</span></span><br><span class="line">    fun2(y.get());  <span class="comment">//智能指针转普通指针</span></span><br><span class="line"></span><br><span class="line">    y.reset(new <span class="type">int</span>(<span class="number">3</span>)); <span class="comment">//将原始内存释放，重新指向新内存</span></span><br><span class="line">    y.reset((<span class="type">int</span>*)nullptr); <span class="comment">//等价于 shared_ptr&lt;int&gt; y;或 y.reset();</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>指定内存回收逻辑  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">fun</span><span class="params">(<span class="type">int</span> *ptr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;fun is called&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    delete ptr;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">x</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>),fun)</span>;<span class="comment">//指定内存回收逻辑</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>std::make_shared  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">ptr</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>))</span>;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; ptr2 = make_shared&lt;<span class="type">int</span>&gt;(<span class="number">3</span>);</span><br><span class="line">    <span class="comment">// auto  ptr2 = make_shared&lt;int&gt;(3);</span></span><br><span class="line">     </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li>支持数组（C++17支持shared_ptr&lt;T[]&gt;；C++20支持make_shared分配数组）  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>[]&gt; <span class="title function_">ptr</span><span class="params">(new <span class="type">int</span>[<span class="number">5</span>])</span>;</span><br><span class="line">    <span class="comment">// auto ptr = make_shared&lt;int[]&gt;(5); //since C++ 20</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>注意：shared_ptr管理的对象不要调用delete销毁   <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">x</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>))</span>;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">y</span><span class="params">(x.get())</span>; <span class="comment">//二次销毁，会崩溃</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="unique-ptr——独占内存的解决方案"><a href="#unique-ptr——独占内存的解决方案" class="headerlink" title="unique_ptr——独占内存的解决方案"></a>unique_ptr——独占内存的解决方案</h3><ul><li>基本用法  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//--run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">unique_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">x</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>))</span>;</span><br><span class="line">    <span class="comment">//unique_ptr&lt;int&gt; y=x; 报错</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;x.get()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">unique_ptr</span>&lt;<span class="type">int</span>&gt; y = move(x);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;x.get()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;y.get()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>unique_ptr不支持复制，但可以移动   <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//--run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">unique_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">x</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>))</span>;</span><br><span class="line">    <span class="comment">//unique_ptr&lt;int&gt; y=x; 报错</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;x.get()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">unique_ptr</span>&lt;<span class="type">int</span>&gt; y = move(x);</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;x.get()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;y.get()&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>为unique_ptr指定内存回收逻辑  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">fun</span><span class="params">(<span class="type">int</span>* ptr)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;fun is called\n&quot;</span>;</span><br><span class="line">    delete ptr;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;<span class="type">int</span>&gt; <span class="title function_">x</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>),fun)</span>;</span><br><span class="line">    <span class="built_in">unique_ptr</span>&lt;<span class="type">int</span>,<span class="title function_">decltype</span><span class="params">(&amp;fun)</span>&gt; <span class="title function_">z</span><span class="params">(new <span class="type">int</span>(<span class="number">3</span>),fun)</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><h3 id="weak-ptr——防止循环引用而引入的智能指针"><a href="#weak-ptr——防止循环引用而引入的智能指针" class="headerlink" title="weak_ptr——防止循环引用而引入的智能指针"></a>weak_ptr——防止循环引用而引入的智能指针</h3><ul><li>基于shared_ptr构造     </li><li>lock方法 :返回对应的shared_ptr  <figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#inlcude<span class="string">&lt;new&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Str</span></span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">    <span class="comment">//shared_ptr&lt;Str&gt; m_nei;</span></span><br><span class="line">    weak_ptr&lt;Str&gt; m_nei; <span class="comment">//为解决循环引用提出，不会增加引用计数</span></span><br><span class="line">    ~Str()</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;~Str is called&quot;</span>&lt;&lt;<span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;;</span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;Str&gt; <span class="title function_">x</span><span class="params">(new Str&#123;&#125;)</span>;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&lt;Str&gt; <span class="title function_">y</span><span class="params">(new Str&#123;&#125;)</span>;</span><br><span class="line">    x-&gt;m_nei = y;</span><br><span class="line">    y-&gt;m_nei = x; <span class="comment">//循环引用，析构函数不会被调用</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">auto</span> ptr = x-&gt;m_nei.lock();ptr)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;Can access pointer\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cout</span>&lt;&lt;<span class="string">&quot;Cannot access pointer\n&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><h3 id="动态内存的相关问题"><a href="#动态内存的相关问题" class="headerlink" title="动态内存的相关问题"></a>动态内存的相关问题</h3><ul><li><p>sizeof不会返回动态分配的内存大小</p><figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;new&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span>* ptr = new <span class="type">int</span>[<span class="number">3</span>]; <span class="comment">// new返回的是申请的内存空间的首地址</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; <span class="keyword">sizeof</span>(ptr) &lt;&lt;<span class="built_in">endl</span>; <span class="comment">//返回8 </span></span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="type">int</span>&gt; x;  <span class="comment">//vector 动态分配内存</span></span><br><span class="line">    <span class="built_in">cout</span>&lt;&lt; <span class="keyword">sizeof</span>(x) &lt;&lt;<span class="built_in">endl</span>;    <span class="comment">//返回24，push_back后也返回24</span></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用分配器（allocator）来分配内存</p><figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;new&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    allocator&lt;<span class="type">int</span>&gt; al;</span><br><span class="line">    <span class="type">int</span>* ptr = al.allocate(<span class="number">3</span>);<span class="comment">//内存分配，未初始化，不涉及构造函数的调用</span></span><br><span class="line">    al.deallocate(ptr,<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用malloc &#x2F; free 来管理内存</p><figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// --run-- g++ $tmpFile.cpp -o $tmpFile.o &amp;&amp; $tmpFile.o</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;new&gt;</span></span></span><br><span class="line"></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//malloc传入的参数是分配了多少字节</span></span><br><span class="line">    <span class="type">int</span> &amp;p1 = <span class="built_in">malloc</span>(<span class="number">4</span>*<span class="keyword">sizeof</span>(<span class="type">int</span>));</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>使用aligned_alloc来分配对齐内存</p></li><li><p>动态内存与异常安全</p><figure class="highlight c"><figcaption><span>++</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;memory&gt;</span></span></span><br><span class="line">using namespace <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">int</span>* ptr = new <span class="type">int</span>(<span class="number">3</span>);</span><br><span class="line">        <span class="comment">//.... </span></span><br><span class="line">    <span class="comment">//即使之前的内存操作产生了异常，最后也要将内存释放，防止产生垃圾内存</span></span><br><span class="line">    delete ptr;</span><br><span class="line">    <span class="comment">//改用智能指针</span></span><br><span class="line">    </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>C++对于垃圾回收的支持</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hexo文章教程</title>
      <link href="/2023/08/14/hexo%E6%96%87%E7%AB%A0%E6%95%99%E7%A8%8B/"/>
      <url>/2023/08/14/hexo%E6%96%87%E7%AB%A0%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Hexo文章教程"><a href="#Hexo文章教程" class="headerlink" title="Hexo文章教程"></a>Hexo文章教程</h2><h3 id="Front-matter"><a href="#Front-matter" class="headerlink" title="Front-matter"></a>Front-matter</h3><p><code>Front-matter </code>是 markdown 文件最上方以<code>---</code>分隔的区域，用于指定个别档案的变数。</p><ul><li><code>Page Front-matter</code> 用於頁面配置</li><li><code>Post Front-matter</code> 用於文章頁配置<br>Page Front-matter：<figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title:</span><br><span class="line">date:</span><br><span class="line">updated:</span><br><span class="line">type:</span><br><span class="line">comments:</span><br><span class="line">description:</span><br><span class="line">keywords:</span><br><span class="line">top<span class="emphasis">_img:</span></span><br><span class="line"><span class="emphasis">mathjax:</span></span><br><span class="line"><span class="emphasis">katex:</span></span><br><span class="line"><span class="emphasis">aside:</span></span><br><span class="line"><span class="emphasis">aplayer:</span></span><br><span class="line"><span class="emphasis">highlight_</span>shrink:</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hexo </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>

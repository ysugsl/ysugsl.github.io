<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>CMOS | Sober</title><meta name="author" content="ShuangLong Gong"><meta name="copyright" content="ShuangLong Gong"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-ResolutionAbstract&amp;emsp;&amp;emsp;现有的大多数盲目图像超分辨率（SR）方法都假设模糊核是空间不变的。然而，由于物体运动、失焦等原因，现实应用中涉及的模糊通常是空间变化的，这">
<meta property="og:type" content="article">
<meta property="og:title" content="CMOS">
<meta property="og:url" content="http://example.com/2023/08/14/CMOS/index.html">
<meta property="og:site_name" content="Sober">
<meta property="og:description" content="Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-ResolutionAbstract&amp;emsp;&amp;emsp;现有的大多数盲目图像超分辨率（SR）方法都假设模糊核是空间不变的。然而，由于物体运动、失焦等原因，现实应用中涉及的模糊通常是空间变化的，这">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/assets/head.jpg">
<meta property="article:published_time" content="2023-08-13T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-13T16:00:00.000Z">
<meta property="article:author" content="ShuangLong Gong">
<meta property="article:tag" content="Deep Learning; C ++">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/assets/head.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://example.com/2023/08/14/CMOS/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'CMOS',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-14 00:00:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-footer-beautify@1.0.0/lib/runtime.min.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/assets/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Sober"><span class="site-name">Sober</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归档</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友联</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">CMOS</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-13T16:00:00.000Z" title="发表于 2023-08-14 00:00:00">2023-08-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-13T16:00:00.000Z" title="更新于 2023-08-14 00:00:00">2023-08-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>23分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="CMOS"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="Better-“CMOS”-Produces-Clearer-Images-Learning-Space-Variant-Blur-Estimation-for-Blind-Image-Super-Resolution"><a href="#Better-“CMOS”-Produces-Clearer-Images-Learning-Space-Variant-Blur-Estimation-for-Blind-Image-Super-Resolution" class="headerlink" title="Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-Resolution"></a>Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-Resolution</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>&emsp;&emsp;现有的大多数盲目图像超分辨率（SR）方法都假设模糊核是空间不变的。然而，由于物体运动、失焦等原因，现实应用中涉及的模糊通常是空间变化的，这会导致先进的SR方法性能严重下降。为了解决这个问题，我们首先引入了两个新的数据集，即NYUv2-BSR和Cityscapes-BSR，以支持进一步研究具有空间变异模糊的盲目SR。基于这些数据集，我们设计了一个新颖的跨模态融合网络（CMOS），可以同时估计模糊和语义，从而提高SR结果。它涉及一个特征组合交互注意（GIA）模块，使得两种模态的交互更加有效并避免不一致性。由于其结构的普适性，GIA还可以用于其他特征的交互。在上述数据集和真实世界图像上，与最先进的方法进行的定性和定量实验表明了我们方法的优越性，例如，在NYUv2-BSR上相对于MANet1获得了+1.91↑ &#x2F; +0.0048↑的PSNR &#x2F; SSIM。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><p>&emsp;&emsp;盲目图像超分辨率旨在从具有未知退化的低分辨率（LR）图像重建高分辨率（HR）图像，因其在实际应用中的重要性而受到广泛关注[2,5,6,12,15,22-24,29]。通常使用两种退化模型，即双三次下采样[35]和传统退化[26,32]，来从HR图像生成LR图像。后者可以被建模为：<br>$$y&#x3D;(x\bigotimes k)\downarrow_s+n.\tag{1}$$<br>&emsp;&emsp;该方法假设LR图像y是通过先将HR图像x与模糊核k卷积，然后进行缩小操作（缩小因子为s）和加噪声n的操作得到的。此外，一些工作[38, 48]提出了更复杂和逼真的退化模型，也假设模糊是空间不变的。然而，在实际应用中，由于失焦和物体运动等因素，模糊通常会在空间上发生变化，因此不匹配会严重降低现有SR方法的性能。图1给出了一个例子，当LR图像受到空间变异模糊的影响时。由于KernelGAN [1]和DCLS [28]仅为一幅图像估计一个模糊核，因此存在许多不匹配。在图1的第一行中，这两种方法估计的核比补丁的实际核更尖锐，导致SR结果过度平滑，高频纹理显著模糊。在第二行中，估计的核比正确的核更平滑，SR结果显示由于过度增强高频边缘而引起的环状伪影。这种现象说明模糊不匹配会严重影响SR结果，导致不自然的输出。在本文中，我们专注于空间变异模糊估计，以确保估计的核对图像中的每个像素都是正确的。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708101405.png" alt="Img"><br>&emsp;&emsp;最近的一些工作[15, 23, 43]已经考虑了空间变异模糊。其中，MANet [23]是最具代表性的模型，它假设在一个小的补丁内，模糊是空间不变的。基于此，MANet使用适度的感受野来保持退化的局部性。然而，仍存在两个关键问题。1）由于没有包含空间变异模糊的可用数据集，MANet是在空间不变的图像上训练的，导致训练和测试阶段的模糊偏差。2）即使限制感受野的大小，估计结果在不同核心的边界仍然很差，导致空间变异模糊的均值预测。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708101427.png" alt="Img"><br>&emsp;&emsp;为了解决上述挑战，我们首先引入一种新的退化方法，并提出了两个相应的数据集，即NYUv2-BSR和Cityscapes-BSR，以支持SR领域中空间变异模糊的相关研究。作为初步探索，本文以失焦模糊为例进行研究，并根据物体的深度使用[19]中提出的方法生成模糊。此外，我们还在数据集中添加了一些空间不变的模糊，以便在它们上训练的模型可以应对空间变异和不变的情况。<br>&emsp;&emsp;此外，为了改善不同模糊区域边界处的性能，我们提出了一种名为跨模态融合网络（CMOS）的新型模型。我们的直觉是，锐利的语义边缘通常与失焦模糊边界对齐，可以帮助区分不同的模糊程度。这引起了一个关键问题，即如何有效地将语义引入到过程中。具体来说，我们首先同时预测模糊和语义，而不是使用语义作为额外的输入，这不仅避免了在测试阶段使用额外的信息，还使非盲目SR方法可以利用这两种模态恢复更细致的纹理。其次，为了增强边缘处的准确性，我们进行了语义和模糊特征之间的交互，以进行补充信息学习，这受到了多任务学习[36,42]的启发。然而，在某些情况下，这两种模态是不一致的。如图2所示，墙壁和上面的图片在语义图中完全不同，有清晰的边界。但它们的深度几乎相同，因此基于深度的模糊程度也非常相似。在这种情况下，两种模态不仅无法使用共同的特征，而且还可能互相产生负面影响。此外，由于我们在数据集中添加了一些带有均匀模糊图的空间不变模糊图像，这也会极大地增加不一致性。<br>&emsp;&emsp;受这些观察的启发，我们提出了一个特征分组交互注意力（GIA）模块，以帮助两种模态之间的交互。GIA具有两个平行流：一个沿空间维度操作，另一个沿通道维度操作。两个流都使用分组交互来处理输入特征并进行调整。此外，GIA具有基于流场[21]的上采样层，以支持不同分辨率的输入。其通用结构使其可以用于不仅仅是两种模态之间的交互。<br>&emsp;&emsp;本文的主要贡献如下：</p>
<ul>
<li>为了支持SR领域中的空间变异模糊研究，我们引入了一种新的失焦模糊退化模型，并提出了两个新的数据集，即NYUv2-BSR和Cityscapes-BSR。</li>
<li>我们设计了一种名为CMOS的新型模型，用于估计空间变异模糊，利用额外的语义信息来提高模糊预测的准确性。所提出的GIA模块用于使两种模态有效地交互。请注意，GIA是通用的，可以用于任何两个特征之间。</li>
<li>与现有的非盲SR模型相结合，CMOS可以估计空间变异和空间不变模糊，并在两种情况下实现SOTA SR性能。</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2. Related Work"></a>2. Related Work</h2><h2 id="2-1-Degradation-Model"><a href="#2-1-Degradation-Model" class="headerlink" title="2.1. Degradation Model"></a>2.1. Degradation Model</h2><p>&emsp;&emsp;如果假设的退化与现实中的不同，SR方法会导致性能不佳。许多工作[4、45、49]使用传统模型（公式1）生成它们的训练数据。与双三次下采样[40、50]相比，虽然传统模型考虑了更多因素，但它仍然过于简单，无法模拟真实的退化。因此，Real-ESRGAN [38]通过反复应用传统模型提出了一个灵活的高阶退化模型，而BSRGAN [48]调整了传统模型的退化顺序，并使用随机混淆的模糊、下采样和噪声。梁等人[23]更进一步地模拟了空间变异模糊，通过将图像分成补丁并应用不同的核来实现。不幸的是，它无法很好地模拟真实情况。因此，为支持相关研究，我们将空间变异的失焦模糊引入到SR中，并提出了两个相应的数据集，即NYUv2-BSR和Cityscapes-BSR。</p>
<h2 id="2-2-Kernel-Estimation"><a href="#2-2-Kernel-Estimation" class="headerlink" title="2.2. Kernel Estimation"></a>2.2. Kernel Estimation</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230707164736.png" alt="Img"><br>&emsp;&emsp;盲SR的主流方法之一是首先估计退化，然后将其用作非盲SR的先验信息。KernelGAN [1]提出从图像块的内部分布中学习核，而IKC [6]使用迭代校正方案学习核的PCA特征。罗等人[28]将模糊估计转移到LR空间，并学习核权重而不是核本身。然而，这些方法仅估计唯一的核，因此在空间变异情况下性能将显著降低。因此，KOALAnet [15]提出为每个像素学习特定的核，而MANet [23]设计了一个具有适应退化局部性的中等感受野的网络。然而，它们仍然存在一些限制，例如中等的感受野可能会限制模型的容量。相比之下，我们的CMOS可以通过语义信息有效而准确地预测空间变异模糊。</p>
<h2 id="2-3-Non-blind-SR"><a href="#2-3-Non-blind-SR" class="headerlink" title="2.3. Non-blind SR"></a>2.3. Non-blind SR</h2><p>&emsp;&emsp;非盲SR旨在恢复具有已知退化的图像。早期的非盲SR方法[13、14、18、25]基于双三次下采样，很难推广到具有更复杂退化的图像。为了解决这个问题，SRMD [49]首先提出将模糊和噪声拉伸到LR图像的大小，并将拼接的图像和退化映射作为输入来恢复HR对应物。随着SRMD的发展，SFTMD [6]使用SFT层[39]来组合拉伸退化映射，而不仅仅是简单的拼接，而UDVD [44]采用每个像素的动态卷积来更有效地处理跨图像的变异退化。此外，还研究了多种退化的零样本方法[11、32、34]。值得注意的是，我们的CMOS可以轻松地与大多数非盲SR方法结合使用，以实现出色的盲SR性能。</p>
<h2 id="3-The-Proposed-Datasets"><a href="#3-The-Proposed-Datasets" class="headerlink" title="3. The Proposed Datasets"></a>3. The Proposed Datasets</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708100254.png" alt="Img"><br>&emsp;&emsp;为了支持空间变异模糊的研究，我们提出了两个新颖的数据集，NYUv2-BSR和CityscapesBSR，其中BSR代表盲图像SR。据我们所知，我们是第一个将失焦（一种最常见的空间变异模糊）引入盲图像SR的研究者。失焦是由深度差异引起的。不在对焦平面上的每个点都对应于图像平面上的一圈混淆（COC）。该模糊可以通过与COC直径相关的标准差σ的各向同性高斯核来模拟[17]，这可以使用薄透镜模型[30]计算COC直径来实现。我们采用[19]中提出的方法对图像进行模糊处理，并使用每个像素的σ构建地面实况模糊图。<br>&emsp;&emsp;如上所述，我们需要深度-彩色图像对来生成具有失焦模糊的图像。因此，我们选择NYUv2 [33]和Cityscapes [3]作为原始数据集。NYUv2是一个室内数据集。它包含1449对RGB和深度图像，其中795对用于训练，其余654对用于测试。Cityscapes是一个室外数据集，其中精细注释的部分包括训练、验证和测试集，分别包含2975、500和1525幅图像。由于Cityscapes中的深度图包含无效测量值，这些值不利于生成失焦图像，因此我们使用CREStereo [20]，一种基于深度学习的立体匹配方法，生成视差图并基于相机参数计算相应的深度图。图3显示了NYUv2和Cityscapes的原始RGB图像，以及生成的失焦图像和相应的模糊图。<br>&emsp;&emsp;就各向同性高斯核的参数而言，NYUv2和Cityscapes的核宽度范围分别设置为[0.0，5.0]和[0.0，15.0]。核大小固定为21×21和61×61，下采样比例因子设置为4。此外，1&#x2F;4的图像被空间不变的核模糊，使得在这些数据集上训练的模型不受空间变异情况的限制。表1显示了详细信息。此外，为确保实验的充分性和公正性，我们为每个数据集创建了五个测试组，每个组都有不同的1&#x2F;4图像使用空间不变核进行模糊处理。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708100212.png" alt="Img"></p>
<h2 id="4-Method"><a href="#4-Method" class="headerlink" title="4. Method"></a>4. Method</h2><p>&emsp;&emsp;如前所述，清晰的语义边缘可以增加边界附近空间变异模糊估计的准确性。受此启发，我们提出了一种交叉模态融合网络（CMOS），通过它们的相互监督来同时预测模糊和语义地图。</p>
<h2 id="4-1-Overview"><a href="#4-1-Overview" class="headerlink" title="4.1. Overview"></a>4.1. Overview</h2><p>&emsp;&emsp;受[36]启发，CMOS是一个多尺度网络，由三个主要阶段组成，如图4所示。在第一阶段，使用一个能够生成多尺度特征的全卷积编码器提取深层特征${F_0,F_1,…,F_n}$。接下来，在每个尺度$i$上，我们应用两个任务特定的头$head_i^b$和$head_i^s$来预测初始模糊和语义特征$F^i_{blur}$和$F^i_{seg}$。然后，我们使用一个提出的$GIA^i_m$模块来实现两种模态之间的有效信息交互，以相互监督的方式获得更准确的特征$\hat{\boldsymbol{F}}<em>{blur}^i$和$\hat{\boldsymbol{F}}</em>{seg}^i$，其公式化为：<br>$$\boldsymbol{F}<em>{blur}^i&#x3D;\operatorname{head}<em>b^i(\boldsymbol{F}<em>b^i),\tag{2}$$<br>$$\boldsymbol{F}</em>{seg}^i&#x3D;\text{head}<em>s^i(\boldsymbol{F}<em>s^i),\tag{3}$$<br>$$\hat{\boldsymbol{F}}</em>{blur}^i,\hat{\boldsymbol{F}}</em>{seg}^i&#x3D;\mathrm{GIA}</em>{m}^i(\boldsymbol{F}</em>{blur}^i,\boldsymbol{F}<em>{seg}^i),\tag{4}$$<br>&emsp;&emsp;其中，$F^i_b$和$F^i_s$表示任务特定头的输入。为了更好地利用多尺度信息，我们使用$GIA^i_b$和$GIA^i_s$来融合相邻的低尺度特征，因此头部的输入可以写成：<br>$$\boldsymbol{F}<em>b^0&#x3D;\boldsymbol{F}<em>s^0&#x3D;\boldsymbol{F}^0,\tag{5}$$<br>$$\boldsymbol{F}</em>{b}^{i}&#x3D;\mathrm{Sum}(\mathrm{GIA}</em>{b}^{i}(\boldsymbol{F}^{i},\hat{\boldsymbol{F}}</em>{blur}^{i-1})),\tag{6}$$<br>$$\boldsymbol{F}<em>s^i&#x3D;\mathrm{Sum}(\mathrm{GIA}<em>s^i(\boldsymbol{F}^i,\hat{\boldsymbol{F}}</em>{seg}^{i-1})),\tag{7}$$<br>&emsp;&emsp;其中，$Sum(·)$表示将模块的输出相加。在最高分辨率$n$处，任务特定特征被馈送到两个卷积层中，以生成辅助模糊和语义地图进行额外监督，这有助于进一步提高最终预测的准确性。<br>&emsp;&emsp;最后一个阶段包括$n + 1$个$GIA^i_l$模块，以获取每个尺度的最终特征$F^i_B$和$F^i_S$，如下所示：<br>$$F_B^i,F_S^i&#x3D;\mathrm{GIA}<em>l^i(\boldsymbol{F}</em>{blur}^i,\boldsymbol{F}</em>{seg}^i).\tag{8}$$<br>&emsp;&emsp;这些特征随后被拼接并进行卷积，以获得模糊和语义地图的预测结果。通过这种方式，我们可以为每个尺度构建一种更简短的监督路径，并进一步促进模糊和语义之间的交互。此外，由于模糊是在高分辨率空间中完成的，因此我们使用双线性插值将输出上采样，倍数为s。</p>
<h2 id="4-2-Grouping-Interactive-Attention-Module"><a href="#4-2-Grouping-Interactive-Attention-Module" class="headerlink" title="4.2. Grouping Interactive Attention Module"></a>4.2. Grouping Interactive Attention Module</h2><p>&emsp;&emsp;GIA的设计旨在帮助模糊和语义更有效地交互，并避免不一致性。此外，由于其具有通用结构，它还可以用于其他特征。GIA涉及两个并行流，在空间和通道维度上操作，它可以通过使用基于流的上采样模块[21]处理不同分辨率的输入。<br>&emsp;&emsp;<strong>空间分组特征交互。</strong> 输入特征在大多数补丁上可能相似，但在某些补丁上可能不同，如图2所示，挂在墙上的图片使得模糊和语义地图之间存在差异。因此，我们提出在通用的空间注意力[7，46，47]机制中调整空间加权图，以利用相似信息并避免不一致性。在图4(b)的上半部分，首先对每个输入进行了处理，我们提出调整通用空间注意机制中的空间权重图，以利用相似信息并避免不一致性。<br>&emsp;&emsp;在图4(b)的上半部分，每个输入首先通过卷积层并被分成由$F^j_w$表示的窗口。这些窗口随后经过另一个卷积层进一步处理，然后被馈送到特征交互模块（第4.2节的最后一部分）。空间调整权重图$M^j_a ∈ R^{1×H×W}$可以通过交互后的1×1卷积层获得。此外，每个输入都有自己的空间权重图$M^j_o ∈ R^{1×H×W}$，直接从窗口中提取，使用另一个1×1卷积层。因此，对应于两个输入的输出$F^j$可以表示为：<br>$$\boldsymbol{F}^{j}&#x3D;\mathrm{Mul}(\boldsymbol{F}<em>{w}^{j},\mathrm{Add}(\boldsymbol{M}</em>{o}^{j},\alpha\boldsymbol{M}<em>{a}^{j})),j&#x3D;1,2,\tag{9}$$<br>其中α是一个可学习的参数。最后，窗口被恢复成特征，并通过一个3×3卷积层平滑可能的接缝，得到最终的输出。<br>&emsp;&emsp;<strong>通道分组特征交互。</strong> 由于空间特征交互集中于局部细节，我们进一步引入通道分组特征交互来校准受[8]启发的全局信息。首先，我们通过应用全局平均池化和一个MLP层将输入$F^j</em>{in}$转换为通道注意向量$A^j_o ∈ R^C$。然后，将这些向量馈送到特征交互模块中，通过另一个MLP层获得两个调整注意向量$A^j_a ∈ R^C$，将这两个特征进行整合。类似于空间特征交互，最终的输出可以通过以下方式获得：<br>$$\boldsymbol{F}^{\boldsymbol{j}}&#x3D;\mathrm{Mul}(\boldsymbol{F}<em>{in}^{j},\mathrm{Add}(\boldsymbol{A}</em>{o}^{j},\beta\boldsymbol{A}<em>{a}^{j})),j&#x3D;1,2\tag{10}$$<br>其中β是一个可学习的参数。由于全局信息对于模糊[31]和语义估计[27]都非常重要，通道维度的特征交互也是必不可少的。<br>&emsp;&emsp;<strong>特征组交互。</strong> 该模块旨在以组的方式交互空间或通道特征。对于空间交互，输入大小为$C × H × W$。我们将每个像素的特征视为一组，并将分组特征的大小设置为$N × D$，其中$N &#x3D; HW，D &#x3D; C$。对于通道交互，输入大小为$C$。它将被分成长度为$D$的$N$组，其中$C &#x3D; ND$。通过这种方式，空间和通道输入都可以表示为$G_i ∈ R^{N×D}$，其中i表示两个不同的输入。然后，我们使用内积进行特征交互，并获得交互特征$F</em>{fuse} ∈ R^{N×N}$，<br>$$F_{fuse}&#x3D;G_1{G_2}^T.\tag{11}$$<br>&emsp;&emsp;之后，对于空间交互，可以通过将$F_{fuse}$重塑为$H×W×N$获得其中一个输出，另一个输出可以通过将$F_{fuse}$重塑为$N×H×W$获得。对于通道交互，两个最终输出相同，可以通过简单地将$F_{fuse}$展平来获得。</p>
<h2 id="4-3-Loss-Function"><a href="#4-3-Loss-Function" class="headerlink" title="4.3. Loss Function"></a>4.3. Loss Function</h2><p>&emsp;&emsp;我们使用平均绝对误差（MAE）来衡量模糊估计的损失，并使用交叉熵（CE）损失来衡量语义分割的损失。如图4(a)所示，辅助损失$L_1$和损失$L_3$都是MAE，而辅助损失$L_2$和损失$L_4$都是CE，具体如下：<br>$$\mathcal{L}<em>1&#x3D;\mathcal{L}<em>3&#x3D;\frac{1}{H\times W}\sum</em>{i&#x3D;1}^{H}\sum</em>{j&#x3D;1}^{W}\lVert B_{i,j}-\hat{B}<em>{i,j}\rVert_1\tag{12}$$<br>$$\mathcal{L}<em>2&#x3D;\mathcal{L}<em>4&#x3D;-\frac1{H\times W}\sum</em>{i&#x3D;1}^H\sum</em>{j&#x3D;1}^W\sum</em>{c&#x3D;1}^C\boldsymbol{S}<em>{i,j}^c\log(\hat{\boldsymbol{S}}</em>{i,j}^c)\tag{13}$$<br>&emsp;&emsp;其中$\hat{B}<em>{i,j}$和${B}</em>{i,j}$分别表示在位置$（i，j）$处的估计模糊图和相应的真实模糊图。类似地，$\hat{\boldsymbol{S}}<em>{ij}^{c}$和$S</em>{ij}^{c}$表示$c$类别在位置$（i，j）$处的估计语义图和真实语义图。$C$是对象类别的数量，$H，W$是地图的高度和宽度。我们没有采用特定的损失权重策略，而是简单地将损失相加。<br>$$\mathcal{L}&#x3D;\mathcal{L}_1+\mathcal{L}_2+\mathcal{L}_3+\mathcal{L}_4\tag{14}$$</p>
<h2 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h2><h2 id="5-1-Experimental-Setup"><a href="#5-1-Experimental-Setup" class="headerlink" title="5.1. Experimental Setup"></a>5.1. Experimental Setup</h2><p>&emsp;&emsp;<strong>CMOS的设置。</strong> 我们选择HRNet [37]作为我们的主干网络，并将前两个卷积的步幅改为1。这相当于输入LR图像的4个尺度（1、1&#x2F;4、1&#x2F;8、1&#x2F;16）。任务特定的头部被实现为两个基本残差块[9]。对于语义分割，我们在NYUv2-BSR中使用官方的40个类别，在Cityscapes-BSR中使用19个类别。所有实验均使用预训练的ImageNet权重进行。<br>&emsp;&emsp;<strong>非盲超分辨率的设置。</strong> 对于非盲超分辨率，我们使用[23]中提出的RRDB-SFT模型。为了将模糊和语义信息输入模型，我们使用了GIA模块。最后，我们使用CMOS估计的模糊和语义地图来微调RRDB-SFT模型。超分辨率图像和高分辨率图像之间的损失也是MAE。<br>&emsp;&emsp;<strong>实现细节。</strong> 我们选择了640×480的图像大小，用于NYUv2-BSR和Cityscapes-BSR。我们通过使用${1，1.2，1.5}$中的随机比例进行缩放来增强训练数据，并将模糊值除以比例。我们还以0.5的概率翻转训练样本。我们使用$β_1 &#x3D; 0.9$和$β_2 &#x3D; 0.99$的Adam优化器[16]训练模型，批量大小为8，训练700个epochs。学习率初始化为0.0001，采用带有10个热身epochs的余弦学习率调度。使用PyTorch实现，在RTX 3090 GPU上训练CMOS大约需要28小时。<br>&emsp;&emsp;<strong>评估指标。</strong> 对于模糊估计，我们使用PSNR和SSIM [41]。对于语义分割，我们使用mIoU。对于由RRDB-SFT生成的最终SR图像，其中模糊和语义地图是由CMOS估计的，我们在YCbCr空间的Y通道上比较PSNR&#x2F;SSIM。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708102231.png" alt="Img"><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708102242.png" alt="Img"><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708102251.png" alt="Img"></p>
<h2 id="5-2-Comparison-with-the-State-of-the-Arts"><a href="#5-2-Comparison-with-the-State-of-the-Arts" class="headerlink" title="5.2. Comparison with the State-of-the-Arts"></a>5.2. Comparison with the State-of-the-Arts</h2><p>&emsp;&emsp;我们将CMOS与现有的盲超分辨率模型进行比较：KernelGAN [1]、KOALAnet [15]、DCLS [28]、DAN [10]和MANet [23]以及上限模型（在给定真实模糊和语义地图的情况下的RRDB-SFT）。我们使用它们的官方实现和设置在NYUv2-BSR和Cityscapes-BSR上重新训练了所有比较方法。KernelGAN是一种无监督方法，仅在测试时对LR图像进行训练。DCLS和DAN是针对空间不变模糊的端到端方法，而KOALAnet和MANet是针对空间变异模糊的两阶段方法。由于我们使用了MANet中提出的非盲超分辨率模型（即RRDB-SFT），因此我们采用相同的设置以确保公平性。<br>&emsp;&emsp;<strong>定量比较。</strong> 如表2和表3所示，CMOS在两个提出的数据集中的不同测试组中均表现最佳。值得注意的是，仅估计图像的一个模糊核的方法（即Kernel GAN，DCLS和DAN）在真实核空间变异时都会出现严重的性能下降。虽然KOALAnet为不同的图像像素估计不同的核，但它不包括任何用于空间变异特性的特殊处理，并且也产生不良的结果。MANet考虑到模糊的局部性质，因此表现相对较好。相比之下，所提出的模型CMOS有效地利用语义信息帮助空间变异模糊估计和非盲超分辨率，大幅优于MANet。<br>&emsp;&emsp;<strong>定性比较。</strong> 我们在图5中展示了几个代表性的视觉样本。可以观察到，我们的CMOS在去除模糊和避免伪影方面均优于以前的方法。其他方法可能会产生振铃伪影（尤其是KernelGAN），或者无法恢复纹理细节，导致补丁仍然模糊。</p>
<h2 id="5-3-Ablation-Study"><a href="#5-3-Ablation-Study" class="headerlink" title="5.3. Ablation Study"></a>5.3. Ablation Study</h2><p>&emsp;&emsp;本节中的所有实验均使用NYUv2-BSR进行训练，指标（即PSNR、SSIM和mIoU）是指跨5个测试集（第3节）的平均值。<br>&emsp;&emsp;<strong>使用空间变异模糊进行训练的重要性。</strong> 根据[23]，由于中等的感受野，即使在空间不变的模糊图像上训练，MANet也可以处理空间变异的情况。但我们认为有必要使用包含空间变异模糊的图像进行训练。为了证明这一点，我们训练了两个MANet模型：一个在提出的NYUv2-BSR数据集上，另一个在从NYUv2数据集生成的空间不变模糊图像上。比较结果如表4所示。显然，使用空间变异模糊图像进行训练可以显著提高超分辨率图像的PSNR和SSIM，分别提高了10.62 dB和0.2266。这表明，在训练和测试阶段保持图像模糊类型的一致性非常重要。<br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708103622.png" alt="Img"><br><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/../assets/%E8%AE%BA%E6%96%87/FILES/Unsupervised%20Degradation%20Representation%20Learning%20for%20Blind%20Super-Resolution.md/img-20230708103636.png" alt="Img"><br>&emsp;&emsp;<strong>GIA模块的有效性。</strong> 我们取出GIA模块的组成部分，即基于流的上采样（F）、通道交互（C）和空间交互（S），以验证其有效性。我们分别记录最佳PSNR和mIoU模型。如表5所示，仅使用基于流的上采样略微改善了结果，当与通道交互结合时，性能可以显著提高。此外，利用所有三个组件，即完整的GIA模块，可以产生更大的改进。<br>&emsp;&emsp;<strong>语义信息在SR中的有效性。</strong> 为了说明语义信息有助于SR，我们对它进行了削弱，仅将模糊地图输入到RRDB-SFT中。值得注意的是，我们在这里使用了真实的模糊和语义地图。如表6所示，添加语义地图可以改善最终SR结果的PSNR（+0.34 dB↑）和SSIM（+0.0022↑）。我们认为，语义信息可能使网络利用它所学习的相关对象的纹理特征，并且清晰的语义边缘也可能有助于SR。<br>&emsp;&emsp;<strong>多任务学习（MTL）的有效性。</strong> 为了证明MTL的有效性，首先，我们对模糊和语义地图进行分别预测，并将它们与CMOS的结果进行比较。其次，我们比较仅利用预测的模糊地图与同时利用预测的模糊和语义地图所实现的SR结果。如表7所示，联合估计提高了语义分割的结果（mIoU +1.66↑），尽管模糊估计的性能略有下降。但总体上，MTL可以将最终SR结果的PSNR&#x2F;SSIM提高+1.28↑&#x2F;+0.002↑，证明语义对整个SR过程有用。<br>&emsp;&emsp;<strong>辅助监督的重要性。</strong> 我们在CMOS中去除辅助监督，以查看它是否对我们的框架必要。如表8所示，在多尺度结构中没有辅助监督，虽然SSIM略有增加，但PSNR和mIoU分别下降了0.38↓和0.24%↓。因此，辅助监督可以在整体上提高CMOS的性能。</p>
<h2 id="5-4-Experiments-on-Real-Wrold-SR"><a href="#5-4-Experiments-on-Real-Wrold-SR" class="headerlink" title="5.4. Experiments on Real-Wrold SR"></a>5.4. Experiments on Real-Wrold SR</h2><p>&emsp;&emsp;由于实际图像没有真实的地面真值，因此我们只比较不同方法的视觉结果。如图6所示，与我们数据集中的结果类似，KernelGAN仍会产生振铃伪影，特别是在室外场景中。DAN、DCLS和KOALAnet都会产生模糊结果，而MANet表现略好。相比之下，CMOS可以产生逼真自然的纹理，结果最清晰。</p>
<h2 id="6-Conclusion"><a href="#6-Conclusion" class="headerlink" title="6. Conclusion"></a>6. Conclusion</h2><p>&emsp;&emsp;本文将焦外模糊引入SR，并提出了两个新的数据集：NYUv2-BSR和Cityscapes-BSR。此外，我们进一步提出了一种新的模型CMOS，可以同时估计模糊和语义地图。通过将语义信息纳入模型，我们可以恢复更细腻的SR结果。GIA模块用于在空间和通道维度上实现有效的特征交互。在提出的数据集和实际图像上的广泛实验表明，我们的模型在与现有的非盲模型集成时可以实现SOTA性能。致谢：本工作得到中国国家自然科学基金（61836015）的支持。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="http://example.com">ShuangLong Gong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://example.com/2023/08/14/CMOS/">http://example.com/2023/08/14/CMOS/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">Sober</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="/assets/head.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/08/14/CDCN/" title="CDCN"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">CDCN</div></div></a></div><div class="next-post pull-right"><a href="/2023/08/14/DAN-V1/" title="DAN_V1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">DAN_V1</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/assets/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">ShuangLong Gong</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ysugsl"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ysugsl" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:sober0306@foxmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Welcome to my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Better-%E2%80%9CCMOS%E2%80%9D-Produces-Clearer-Images-Learning-Space-Variant-Blur-Estimation-for-Blind-Image-Super-Resolution"><span class="toc-text">Better “CMOS” Produces Clearer Images : Learning Space-Variant Blur Estimation for Blind Image Super-Resolution</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1. Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-Work"><span class="toc-text">2. Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Degradation-Model"><span class="toc-text">2.1. Degradation Model</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Kernel-Estimation"><span class="toc-text">2.2. Kernel Estimation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Non-blind-SR"><span class="toc-text">2.3. Non-blind SR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-The-Proposed-Datasets"><span class="toc-text">3. The Proposed Datasets</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Method"><span class="toc-text">4. Method</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-Overview"><span class="toc-text">4.1. Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-Grouping-Interactive-Attention-Module"><span class="toc-text">4.2. Grouping Interactive Attention Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Loss-Function"><span class="toc-text">4.3. Loss Function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Experiments"><span class="toc-text">5. Experiments</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-Experimental-Setup"><span class="toc-text">5.1. Experimental Setup</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-Comparison-with-the-State-of-the-Arts"><span class="toc-text">5.2. Comparison with the State-of-the-Arts</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-Ablation-Study"><span class="toc-text">5.3. Ablation Study</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-Experiments-on-Real-Wrold-SR"><span class="toc-text">5.4. Experiments on Real-Wrold SR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Conclusion"><span class="toc-text">6. Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/14/1-%E5%B8%B8%E7%94%A8%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E5%91%BD%E4%BB%A4/" title="1.常用文件管理命令">1.常用文件管理命令</a><time datetime="2023-08-13T16:00:00.000Z" title="发表于 2023-08-14 00:00:00">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/14/2-tmux-vim/" title="2.tumx &amp; vim">2.tumx &amp; vim</a><time datetime="2023-08-13T16:00:00.000Z" title="发表于 2023-08-14 00:00:00">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/14/3-Shell%E8%AF%AD%E6%B3%95/" title="3.Shell语法">3.Shell语法</a><time datetime="2023-08-13T16:00:00.000Z" title="发表于 2023-08-14 00:00:00">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/14/4-ssh/" title="4.ssh">4.ssh</a><time datetime="2023-08-13T16:00:00.000Z" title="发表于 2023-08-14 00:00:00">2023-08-14</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/08/14/5-Git/" title="5. Git">5. Git</a><time datetime="2023-08-13T16:00:00.000Z" title="发表于 2023-08-14 00:00:00">2023-08-14</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By ShuangLong Gong</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="30" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><!-- hexo injector body_end start --><script data-pjax>
  function butterfly_footer_beautify_injector_config(){
    var parent_div_git = document.getElementById('footer-wrap');
    var item_html = '<p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" data-title="博客框架为Hexo_v6.2.0" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" data-title="主题版本Butterfly_v4.3.1" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" data-title="本站项目由Github托管" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&amp;logo=GitHub" alt=""/></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px" data-title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" title=""><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris" alt=""/></a></p>';
    console.log('已挂载butterfly_footer_beautify')
    parent_div_git.insertAdjacentHTML("beforeend",item_html)
    }
  var elist = 'null'.split(',');
  var cpage = location.pathname;
  var epage = 'all';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_footer_beautify_injector_config();
  }
  else if (epage === cpage){
    butterfly_footer_beautify_injector_config();
  }
  </script><!-- hexo injector body_end end --></body></html>